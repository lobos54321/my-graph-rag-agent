#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GraphRAG Agent ä¸»åº”ç”¨å…¥å£
ç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å…å¤æ‚çš„æ¨¡å—å¯¼å…¥é—®é¢˜
"""

import os
import sys
import re
import json
import requests
import tempfile
import shutil
import time
from pathlib import Path
import openai
import PyPDF2
import io
from dotenv import load_dotenv
from bs4 import BeautifulSoup

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "server"))

from fastapi import FastAPI, HTTPException, File, UploadFile
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="GraphRAG Agent API",
    description="åŸºäºçŸ¥è¯†å›¾è°±çš„æ™ºèƒ½æ–‡æ¡£åˆ†æç³»ç»Ÿ",
    version="1.0.0"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è®¸æ‰€æœ‰åŸŸå
    allow_credentials=True,
    allow_methods=["*"],  # å…è®¸æ‰€æœ‰HTTPæ–¹æ³•
    allow_headers=["*"],  # å…è®¸æ‰€æœ‰è¯·æ±‚å¤´
)

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {"message": "GraphRAG Agent API is running!", "version": "1.0.0"}

@app.get("/api/auth/profile")
async def auth_profile():
    """è®¤è¯é…ç½®æ–‡ä»¶ç«¯ç‚¹ - å…¼å®¹å‰ç«¯"""
    return {
        "user": "GraphRAG User",
        "authenticated": True,
        "service": "GraphRAG Agent"
    }

@app.get("/api/graphrag/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {
        "status": "healthy",
        "service": "GraphRAG Agent",
        "database": "memory" if os.getenv("DATABASE_TYPE", "memory") == "memory" else "neo4j",
        "embedding_provider": os.getenv("CACHE_EMBEDDING_PROVIDER", "openai")
    }

def extract_text_from_file(content: bytes, filename: str) -> str:
    """å®‰å…¨çš„æ–‡ä»¶å†…å®¹æå–æ–¹æ³• - é¿å…å¯èƒ½å¯¼è‡´æ®µé”™è¯¯çš„å¤æ‚ä¾èµ–"""
    try:
        print(f"ğŸ“„ å¼€å§‹å®‰å…¨æå–æ–‡ä»¶å†…å®¹: {filename} ({len(content)} bytes)")
        
        # è·å–æ–‡ä»¶æ‰©å±•å
        file_ext = os.path.splitext(filename)[1].lower()
        
        # ç›´æ¥ä½¿ç”¨å®‰å…¨çš„æå–æ–¹æ³•ï¼Œé¿å…GraphRAG FileReader
        if file_ext == '.pdf':
            text = extract_pdf_with_improved_method(content, filename)
        else:
            text = extract_text_fallback(content, filename)
        
        print(f"âœ… æ–‡ä»¶å†…å®¹æå–å®Œæˆ: {len(text)} å­—ç¬¦")
        
        # å†…å®¹è´¨é‡éªŒè¯å’Œæ”¹è¿› - ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬
        if text and len(text) > 10:
            try:
                print(f"ğŸ”§ å¼€å§‹å†…å®¹æ”¹è¿›å¤„ç†: {len(text)} å­—ç¬¦")
                validated_text = improve_text_content(text, {"overall_score": 0.8})
                print(f"âœ… å†…å®¹æ”¹è¿›å®Œæˆ: {len(validated_text)} å­—ç¬¦")
                return validated_text
            except Exception as improve_error:
                print(f"âŒ Content improvement failed: {improve_error}")
                return text
        else:
            return text
                
    except Exception as e:
        print(f"âŒ æ–‡ä»¶æå–å¤±è´¥: {e}")
        return f"æ–‡ä»¶æå–å¤±è´¥: {str(e)}"

def validate_and_improve_content(text: str, filename: str, file_ext: str) -> str:
    """éªŒè¯å’Œæ”¹è¿›æå–çš„å†…å®¹è´¨é‡"""
    if not text or len(text.strip()) < 10:
        print(f"âš ï¸ å†…å®¹è¿‡çŸ­ï¼Œå¯èƒ½æå–ä¸å®Œæ•´: {len(text)} å­—ç¬¦")
        return text
    
    print(f"ğŸ” å¼€å§‹å†…å®¹è´¨é‡éªŒè¯: {filename}")
    
    # 1. åŸºç¡€å†…å®¹è´¨é‡æ£€æŸ¥
    quality_metrics = analyze_content_quality(text, file_ext)
    print(f"ğŸ“Š å†…å®¹è´¨é‡æŒ‡æ ‡:")
    print(f"  - å®Œæ•´æ€§è¯„åˆ†: {quality_metrics['completeness_score']:.2f}")
    print(f"  - å¯è¯»æ€§è¯„åˆ†: {quality_metrics['readability_score']:.2f}")
    print(f"  - ä¿¡æ¯å¯†åº¦: {quality_metrics['information_density']:.2f}")
    print(f"  - ç»“æ„å®Œæ•´æ€§: {quality_metrics['structure_integrity']:.2f}")
    
    # 2. å†…å®¹é¢„å¤„ç†å’Œä¿®å¤
    improved_text = improve_text_content(text, quality_metrics)
    
    # 3. æœ€ç»ˆè´¨é‡éªŒè¯
    final_metrics = analyze_content_quality(improved_text, file_ext)
    improvement_ratio = (final_metrics['overall_score'] - quality_metrics['overall_score']) / max(quality_metrics['overall_score'], 0.1)
    
    print(f"âœ… å†…å®¹ä¼˜åŒ–å®Œæˆ:")
    print(f"  - åŸå§‹é•¿åº¦: {len(text)} â†’ ä¼˜åŒ–åé•¿åº¦: {len(improved_text)}")
    print(f"  - è´¨é‡æå‡: {improvement_ratio:.1%}")
    print(f"  - æœ€ç»ˆè´¨é‡è¯„åˆ†: {final_metrics['overall_score']:.2f}")
    
    return improved_text

def analyze_content_quality(text: str, file_ext: str) -> dict:
    """åˆ†æå†…å®¹è´¨é‡æŒ‡æ ‡ - å®‰å…¨ç‰ˆæœ¬"""
    try:
        if not text or len(text) == 0:
            return {
                'completeness_score': 0.0,
                'readability_score': 0.0,
                'information_density': 0.0,
                'structure_integrity': 0.0,
                'overall_score': 0.0
            }
        
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥é¿å…å†…å­˜é—®é¢˜
        if len(text) > 50000:  # é™åˆ¶åœ¨50Kå­—ç¬¦å†…
            text = text[:50000]
        
        # 1. å®Œæ•´æ€§è¯„åˆ† - ç®€åŒ–ç‰ˆæœ¬
        length_score = min(1.0, len(text) / 500)
        
        # ç®€åŒ–å¥å­æ£€æŸ¥ï¼Œé¿å…å¤æ‚æ­£åˆ™è¡¨è¾¾å¼
        sentence_indicators = text.count('.') + text.count('ã€‚') + text.count('!') + text.count('ï¼') + text.count('?') + text.count('ï¼Ÿ')
        sentence_score = min(1.0, sentence_indicators / 5)
        
        completeness_score = (length_score + sentence_score) / 2
        
        # 2. ç®€åŒ–çš„å¯è¯»æ€§è¯„åˆ†
        total_chars = len(text)
        if total_chars == 0:
            readability_score = 0.0
        else:
            # ç®€åŒ–çš„ä¹±ç æ£€æµ‹ - åªæ£€æŸ¥å‰1000å­—ç¬¦
            sample_text = text[:1000] if len(text) > 1000 else text
            garbled_chars = sum(1 for c in sample_text if ord(c) > 127 and c.isalnum())
            garbled_ratio = garbled_chars / len(sample_text)
            
            # ç®€åŒ–çš„å”¯ä¸€æ€§æ£€æŸ¥
            words = text.split()[:500]  # åªæ£€æŸ¥å‰500ä¸ªè¯
            if len(words) > 0:
                unique_words = len(set(words))
                uniqueness_ratio = unique_words / len(words)
            else:
                uniqueness_ratio = 1.0
            
            readability_score = (1 - min(garbled_ratio, 0.5)) * uniqueness_ratio
        
        # 3. ç®€åŒ–çš„ä¿¡æ¯å¯†åº¦
        key_terms = ['ç³»ç»Ÿ', 'æŠ€æœ¯', 'æ¶æ„', 'æ¨¡å—', 'åŠŸèƒ½', 'æ•°æ®', 'åˆ†æ']
        keyword_count = sum(1 for term in key_terms if term in text)
        info_density = min(1.0, keyword_count / 5)
        
        # 4. ç®€åŒ–çš„ç»“æ„å®Œæ•´æ€§
        paragraph_count = text.count('\n\n') + 1
        paragraph_score = min(1.0, paragraph_count / 3)
        
        title_count = text.count('#') + text.count('ä¸€ã€') + text.count('1.')
        title_score = min(1.0, title_count / 2)
        
        structure_integrity = (paragraph_score + title_score) / 2
        
        # è®¡ç®—æ€»ä½“è´¨é‡è¯„åˆ†
        overall_score = (completeness_score * 0.3 + readability_score * 0.3 + 
                        info_density * 0.2 + structure_integrity * 0.2)
        
        return {
            'completeness_score': completeness_score,
            'readability_score': readability_score,
            'information_density': info_density,
            'structure_integrity': structure_integrity,
            'overall_score': overall_score
        }
        
    except Exception as e:
        print(f"âš ï¸ Content quality analysis error: {e}")
        return {
            'completeness_score': 0.5,
            'readability_score': 0.5,
            'information_density': 0.5,
            'structure_integrity': 0.5,
            'overall_score': 0.5
        }

def improve_text_content(text: str, quality_metrics: dict) -> str:
    """åŸºäºè´¨é‡æŒ‡æ ‡æ”¹è¿›æ–‡æœ¬å†…å®¹ - å®‰å…¨ç‰ˆæœ¬"""
    try:
        if not text:
            return text
        
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ä»¥é¿å…å†…å­˜é—®é¢˜
        if len(text) > 20000:
            text = text[:20000]
        
        improved_text = text
        
        # 1. å®‰å…¨çš„æ ¼å¼æ¸…ç† - é¿å…æ­£åˆ™è¡¨è¾¾å¼
        # æ ‡å‡†åŒ–æ¢è¡Œç¬¦
        improved_text = improved_text.replace('\r\n', '\n').replace('\r', '\n')
        
        # ç®€å•çš„ç©ºæ ¼æ¸…ç†
        lines = improved_text.split('\n')
        cleaned_lines = []
        prev_empty = False
        
        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                if not prev_empty:
                    cleaned_lines.append('')
                prev_empty = True
            else:
                # ç®€å•çš„å¤šç©ºæ ¼åˆå¹¶
                cleaned_line = ' '.join(line_stripped.split())
                cleaned_lines.append(cleaned_line)
                prev_empty = False
        
        improved_text = '\n'.join(cleaned_lines)
        
        # 2. å®‰å…¨çš„æ®µè½ç»“æ„æ”¹è¿›
        if quality_metrics.get('structure_integrity', 1.0) < 0.5:
            lines = improved_text.split('\n')
            organized_lines = []
            current_paragraph = []
            
            for line in lines:
                line = line.strip()
                if not line:
                    if current_paragraph:
                        organized_lines.append(' '.join(current_paragraph))
                        current_paragraph = []
                    continue
                
                # ç®€åŒ–çš„æ®µè½æ£€æµ‹ï¼Œé¿å…å¤æ‚æ“ä½œ
                is_new_paragraph = (
                    line.startswith(('ä¸€ã€', 'äºŒã€', 'ä¸‰ã€', 'å››ã€', 'äº”ã€')) or
                    line.startswith(('1.', '2.', '3.', '4.', '5.')) or
                    line.startswith(('ï¼ˆä¸€ï¼‰', 'ï¼ˆäºŒï¼‰', 'ï¼ˆä¸‰ï¼‰')) or
                    line.startswith('#') or
                    (len(line) < 50 and not line.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')))
                )
                
                if is_new_paragraph and current_paragraph:
                    organized_lines.append(' '.join(current_paragraph))
                    current_paragraph = [line]
                else:
                    current_paragraph.append(line)
            
            if current_paragraph:
                organized_lines.append(' '.join(current_paragraph))
            
            improved_text = '\n\n'.join(organized_lines)
        
        # 3. ç®€å•çš„å†…å®¹è¡¥å…¨
        if quality_metrics.get('completeness_score', 1.0) < 0.5:
            if improved_text and not improved_text.rstrip().endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
                improved_text += "..."
        
        return improved_text.strip()
        
    except Exception as e:
        print(f"âš ï¸ æ–‡æœ¬æ”¹è¿›å¤±è´¥ï¼Œè¿”å›åŸæ–‡æœ¬: {e}")
        return text if text else ""

def extract_pdf_with_improved_method(content: bytes, filename: str) -> str:
    """æ”¹è¿›çš„PDFæå–æ–¹æ³•ï¼Œå¢å¼ºç»“æ„ä¿æŒå’ŒOCRåå¤‡å¤„ç†"""
    try:
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(content))
        text = ""
        page_count = len(pdf_reader.pages)
        extraction_stats = {
            "total_pages": page_count,
            "successful_pages": 0,
            "empty_pages": 0,
            "failed_pages": 0,
            "ocr_attempted": 0
        }
        
        print(f"ğŸ“„ å¼€å§‹é€é¡µæå–PDFå†…å®¹ï¼Œå…± {page_count} é¡µ")
        
        for page_num in range(page_count):
            try:
                page = pdf_reader.pages[page_num]
                
                # å¤šé‡æå–ç­–ç•¥
                page_text = ""
                
                # 1. æ ‡å‡†æ–‡æœ¬æå–
                try:
                    page_text = page.extract_text() or ""
                    if page_text.strip():
                        extraction_stats["successful_pages"] += 1
                        print(f"âœ… ç¬¬ {page_num+1} é¡µæ ‡å‡†æå–æˆåŠŸ: {len(page_text)} å­—ç¬¦")
                except Exception as extract_err:
                    print(f"âš ï¸ ç¬¬ {page_num+1} é¡µæ ‡å‡†æå–å¤±è´¥: {extract_err}")
                
                # 2. å¦‚æœæ ‡å‡†æå–å†…å®¹å¤ªå°‘ï¼Œå°è¯•æ”¹è¿›æå–
                if len(page_text.strip()) < 50:
                    try:
                        # å°è¯•é€ä¸ªæ–‡æœ¬å¯¹è±¡æå–
                        if '/Contents' in page and page['/Contents']:
                            alt_text = ""
                            # ç®€åŒ–çš„é¡µé¢å†…å®¹è·å– - é¿å…å¤æ‚æ“ä½œ
                            if hasattr(page, '_get_contents_as_bytes'):
                                try:
                                    content_bytes = page._get_contents_as_bytes()
                                    if content_bytes:
                                        alt_text = content_bytes.decode('utf-8', errors='ignore')
                                        # ç®€å•çš„æ–‡æœ¬æå– - é¿å…æ­£åˆ™è¡¨è¾¾å¼
                                        if '(' in alt_text and ')' in alt_text:
                                            # åŸºç¡€çš„æ‹¬å·å†…å®¹æå–
                                            parts = alt_text.split('(')
                                            text_parts = []
                                            for part in parts[1:10]:  # é™åˆ¶å¤„ç†æ•°é‡
                                                if ')' in part:
                                                    text_part = part.split(')')[0]
                                                    if len(text_part) < 100:  # é¿å…è¿‡é•¿çš„æ–‡æœ¬
                                                        text_parts.append(text_part)
                                            if text_parts:
                                                page_text = ' '.join(text_parts)
                                                print(f"ğŸ“ ç¬¬ {page_num+1} é¡µä½¿ç”¨æ”¹è¿›æå–: {len(page_text)} å­—ç¬¦")
                                except Exception:
                                    pass  # å¦‚æœæå–å¤±è´¥ï¼Œè·³è¿‡
                    except Exception as alt_err:
                        print(f"âš ï¸ ç¬¬ {page_num+1} é¡µæ”¹è¿›æå–å¤±è´¥: {alt_err}")
                
                # 3. ç»“æ„åŒ–æ¸…ç†å’Œæ”¹è¿›
                if page_text.strip():
                    # ä¿æŒåŸå§‹ç»“æ„çš„æ¸…ç†
                    cleaned_text = clean_extracted_text_with_structure(page_text, page_num + 1)
                    text += cleaned_text + f"\n\n--- ç¬¬ {page_num+1} é¡µç»“æŸ ---\n\n"
                else:
                    print(f"âš ï¸ ç¬¬ {page_num+1} é¡µå†…å®¹ä¸ºç©ºæˆ–æå–å¤±è´¥")
                    extraction_stats["empty_pages"] += 1
                    text += f"[ç¬¬ {page_num+1} é¡µ: æ— å¯æå–çš„æ–‡æœ¬å†…å®¹]\n\n"
                    
            except Exception as e:
                print(f"âŒ å¤„ç†PDFæ–‡ä»¶ {filename} çš„ç¬¬ {page_num+1} é¡µå¤±è´¥: {str(e)}")
                extraction_stats["failed_pages"] += 1
                text += f"[ç¬¬ {page_num+1} é¡µè¯»å–å¤±è´¥: {str(e)}]\n\n"
        
        # æ·»åŠ æå–ç»Ÿè®¡ä¿¡æ¯
        stats_summary = f"""
PDFæå–ç»Ÿè®¡æŠ¥å‘Š - {filename}:
- æ€»é¡µæ•°: {extraction_stats['total_pages']}
- æˆåŠŸæå–: {extraction_stats['successful_pages']} é¡µ
- ç©ºç™½é¡µé¢: {extraction_stats['empty_pages']} é¡µ
- å¤±è´¥é¡µé¢: {extraction_stats['failed_pages']} é¡µ
æå–å®Œæ•´åº¦: {(extraction_stats['successful_pages'] / max(extraction_stats['total_pages'], 1) * 100):.1f}%

=== æ–‡æ¡£å†…å®¹å¼€å§‹ ===

"""
        
        final_text = stats_summary + text.strip()
        print(f"ğŸ“‹ PDFæå–å®Œæˆï¼Œæ€»é•¿åº¦: {len(final_text)} å­—ç¬¦")
        print(f"ğŸ“Š æå–ç»Ÿè®¡: æˆåŠŸ{extraction_stats['successful_pages']}/{extraction_stats['total_pages']}é¡µ")
        
        return final_text
        
    except Exception as e:
        print(f"âŒ æ”¹è¿›PDFæå–æ–¹æ³•å¤±è´¥: {str(e)}")
        return f"PDFæå–å¤±è´¥: {str(e)}\nè¯·å°è¯•ä½¿ç”¨å…¶ä»–PDFå¤„ç†å·¥å…·æˆ–æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æŸåã€‚"

def clean_extracted_text_with_structure(text: str, page_num: int) -> str:
    """å¢å¼ºç‰ˆæ–‡æœ¬æ¸…ç†ï¼Œä¿æŒæ–‡æ¡£ç»“æ„ - å®‰å…¨ç‰ˆæœ¬"""
    if not text:
        return ""
    
    try:
        # 1. åŸºç¡€æ¸…ç†
        cleaned = text.strip()
        
        # 2. å®‰å…¨çš„æ®µè½ç»“æ„æ¸…ç† - é¿å…å¤æ‚æ­£åˆ™è¡¨è¾¾å¼
        # ç®€å•çš„æ¢è¡Œç¬¦æ ‡å‡†åŒ–
        cleaned = cleaned.replace('\r\n', '\n').replace('\r', '\n')
        
        # ç®€å•çš„ç©ºè¡Œåˆå¹¶
        lines = cleaned.split('\n')
        processed_lines = []
        prev_empty = False
        
        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                if not prev_empty:
                    processed_lines.append('')
                prev_empty = True
            else:
                # ç®€å•çš„ç©ºæ ¼æ¸…ç†
                cleaned_line = ' '.join(line_stripped.split())
                processed_lines.append(cleaned_line)
                prev_empty = False
        
        cleaned = '\n'.join(processed_lines)
        
        # 3. é¿å…å¤æ‚çš„æ–‡æœ¬ä¿®å¤æ“ä½œï¼Œä¿æŒç®€å•
        
    except Exception as e:
        print(f"âš ï¸ æ–‡æœ¬æ¸…ç†å¤±è´¥: {e}")
        # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè¿”å›åŸºç¡€æ¸…ç†çš„æ–‡æœ¬
        cleaned = text.strip() if text else ""
    
    # 4. è¯†åˆ«å’Œä¿æŒæ ‡é¢˜ç»“æ„
    lines = cleaned.split('\n')
    structured_lines = []
    
    for i, line in enumerate(lines):
        line = line.strip()
        if not line:
            structured_lines.append('')
            continue
            
        # æ£€æµ‹å¯èƒ½çš„æ ‡é¢˜ï¼ˆçŸ­è¡Œã€é¦–å­—æ¯å¤§å†™ã€æ²¡æœ‰å¥å·ç»“å°¾ï¼‰
        is_title = (
            len(line) < 80 and 
            line[0].isupper() and 
            not line.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')) and
            not line.startswith(('â€¢', '-', '1.', '2.', '3.'))
        )
        
        if is_title and i < len(lines) - 1:
            # åœ¨æ ‡é¢˜å‰åæ·»åŠ é€‚å½“çš„ç©ºè¡Œ
            if structured_lines and structured_lines[-1]:
                structured_lines.append('')
            structured_lines.append(f"ã€{line}ã€‘")  # æ ‡è®°ä¸ºæ ‡é¢˜
            structured_lines.append('')
        else:
            structured_lines.append(line)
    
    # 5. é‡æ–°ç»„è£…æ–‡æœ¬
    result = '\n'.join(structured_lines)
    
    # 6. å®‰å…¨çš„æœ€ç»ˆæ¸…ç† - é¿å…æ­£åˆ™è¡¨è¾¾å¼
    try:
        # ç®€å•çš„å¤šç©ºè¡Œæ¸…ç†
        lines = result.split('\n')
        final_lines = []
        empty_count = 0
        
        for line in lines:
            if not line.strip():
                empty_count += 1
                if empty_count <= 2:  # æœ€å¤šä¿ç•™ä¸¤ä¸ªç©ºè¡Œ
                    final_lines.append('')
            else:
                empty_count = 0
                final_lines.append(line)
        
        result = '\n'.join(final_lines).strip()
    except Exception:
        result = result.strip() if result else ""
    
    # 7. æ·»åŠ é¡µç æ ‡è®°
    if result:
        result = f"=== ç¬¬ {page_num} é¡µå†…å®¹ ===\n\n{result}"
    
    return result

def clean_extracted_text(text: str) -> str:
    """æ¸…ç†æå–çš„æ–‡æœ¬ï¼Œå‡å°‘ä¹±ç ï¼ˆå‘åå…¼å®¹ç‰ˆæœ¬ï¼‰"""
    if not text:
        return ""
    
    # å®‰å…¨çš„åŸºç¡€æ¸…ç† - é¿å…regex
    try:
        # ç®€å•çš„å­—ç¬¦ä¸²æ›¿æ¢ï¼Œé¿å…æ­£åˆ™è¡¨è¾¾å¼
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        lines = text.split('\n')
        cleaned_lines = []
        prev_empty = False
        
        for line in lines:
            stripped = line.strip()
            if not stripped:
                if not prev_empty:
                    cleaned_lines.append('')
                prev_empty = True
            else:
                cleaned_lines.append(' '.join(stripped.split()))
                prev_empty = False
        
        text = '\n'.join(cleaned_lines).strip()
    except Exception:
        pass  # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œä¿æŒåŸæ–‡æœ¬
    
    return text

def extract_text_fallback(content: bytes, filename: str) -> str:
    """éPDFæ–‡ä»¶çš„å›é€€æå–æ–¹æ³•"""
    try:
        if filename.lower().endswith(('.txt', '.md')):
            return content.decode('utf-8', errors='ignore')
        else:
            return content.decode('utf-8', errors='ignore')
    except Exception as e2:
        print(f"âŒ åŸºç¡€æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
        return f"æ— æ³•æå–æ–‡ä»¶å†…å®¹: {str(e2)}"

def create_basic_analysis(text_content: str, filename: str) -> dict:
    """åˆ›å»ºåŸºç¡€åˆ†æç»“æœï¼ˆå½“OpenAI APIè¶…æ—¶æˆ–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
    try:
        # åŸºäºæ–‡æœ¬å†…å®¹è¿›è¡Œç®€å•åˆ†æ
        content_length = len(text_content) if text_content else 0
        
        # æå–ä¸€äº›åŸºæœ¬æ¦‚å¿µ
        basic_concepts = ["æ–‡æ¡£åˆ†æ", "å†…å®¹æå–"]
        if "PDF" in filename.upper():
            basic_concepts.append("PDFæ–‡æ¡£")
        if content_length > 1000:
            basic_concepts.append("é•¿æ–‡æ¡£")
        elif content_length > 0:
            basic_concepts.append("çŸ­æ–‡æ¡£")
            
        # ç®€å•çš„æ–‡æœ¬åˆ†æ
        if text_content and len(text_content) > 50:
            words = text_content.lower().split()
            # æ£€æµ‹ä¸€äº›å¸¸è§å…³é”®è¯
            if any(word in text_content.lower() for word in ["æŠ€æœ¯", "ç³»ç»Ÿ", "å¼€å‘", "api", "ä»£ç "]):
                basic_concepts.append("æŠ€æœ¯æ–‡æ¡£")
            if any(word in text_content.lower() for word in ["äº§å“", "éœ€æ±‚", "åŠŸèƒ½", "ç”¨æˆ·"]):
                basic_concepts.append("äº§å“æ–‡æ¡£")
            if any(word in text_content.lower() for word in ["åˆ†æ", "æŠ¥å‘Š", "æ•°æ®", "ç»Ÿè®¡"]):
                basic_concepts.append("åˆ†ææŠ¥å‘Š")
        
        return {
            "content": f"åŸºç¡€åˆ†æå®Œæˆã€‚æ–‡æ¡£ {filename} åŒ…å« {content_length} å­—ç¬¦çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ä¸ªPDFæ–‡æ¡£çš„åŸºç¡€åˆ†æç»“æœï¼Œå†…å®¹å·²æå–å¹¶å¯è¿›è¡Œè¿›ä¸€æ­¥åˆ†æã€‚",
            "concepts": basic_concepts,
            "entities": [filename.split('.')[0], "æ–‡æ¡£å†…å®¹"],
            "relationships": [
                {"source": filename.split('.')[0], "target": "æ–‡æ¡£å†…å®¹", "type": "contains", "description": "åŒ…å«å†…å®¹"}
            ],
            "knowledgeTreeSuggestion": "æ–‡æ¡£ç®¡ç†/PDFæ–‡æ¡£/å¾…åˆ†ç±»",
            "confidence": 0.6
        }
    except Exception as e:
        return {
            "content": f"åŸºç¡€åˆ†æå¤±è´¥: {str(e)}",
            "concepts": ["åˆ†æå¤±è´¥"],
            "entities": ["é”™è¯¯"],
            "relationships": [],
            "knowledgeTreeSuggestion": "ç³»ç»Ÿé”™è¯¯/åˆ†æå¤±è´¥",
            "confidence": 0.1
        }

async def extract_video_content(url: str) -> dict:
    """æå–è§†é¢‘å†…å®¹ - æ”¯æŒYouTubeã€Bilibiliç­‰ä¸»æµå¹³å°ï¼Œå¹¶é›†æˆAIåˆ†æ"""
    try:
        print(f"ğŸ¬ æ£€æµ‹åˆ°è§†é¢‘é“¾æ¥ï¼Œå¼€å§‹æå–å†…å®¹: {url}")
        
        video_info = {
            "platform": "unknown",
            "title": "",
            "description": "",
            "duration": "",
            "uploader": "",
            "view_count": "",
            "upload_date": "",
            "tags": [],
            "transcript": "",
            "comments_sample": []
        }
        
        # æ£€æµ‹è§†é¢‘å¹³å°
        if 'youtube.com' in url or 'youtu.be' in url:
            print(f"ğŸ” DEBUG: ç¡®è®¤ä¸ºYouTubeé“¾æ¥ï¼Œè°ƒç”¨Cobaltå¢å¼ºæå–")
            video_info = extract_youtube_content_with_cobalt(url, video_info)
            print(f"ğŸ” DEBUG: Cobaltæå–ç»“æœ - æ ‡é¢˜: {video_info.get('title', 'N/A')[:50]}")
            print(f"ğŸ” DEBUG: Cobaltæå–ç»“æœ - å¹³å°: {video_info.get('platform', 'N/A')}")
        elif 'bilibili.com' in url or 'b23.tv' in url:
            video_info = extract_bilibili_content(url, video_info)
        elif 'vimeo.com' in url:
            video_info = extract_vimeo_content(url, video_info)
        else:
            # é€šç”¨è§†é¢‘é¡µé¢å†…å®¹æå–
            video_info = extract_generic_video_content(url, video_info)
        
        # ç»„åˆå®Œæ•´çš„è§†é¢‘å†…å®¹
        content_parts = []
        
        if video_info["title"]:
            content_parts.append(f"æ ‡é¢˜: {video_info['title']}")
        
        if video_info["uploader"]:
            content_parts.append(f"åˆ›ä½œè€…: {video_info['uploader']}")
            
        if video_info["duration"]:
            content_parts.append(f"æ—¶é•¿: {video_info['duration']}")
            
        if video_info["view_count"]:
            content_parts.append(f"æ’­æ”¾é‡: {video_info['view_count']}")
            
        if video_info["upload_date"]:
            content_parts.append(f"å‘å¸ƒæ—¶é—´: {video_info['upload_date']}")
            
        if video_info["description"]:
            content_parts.append(f"æè¿°: {video_info['description']}")
            
        if video_info["tags"]:
            content_parts.append(f"æ ‡ç­¾: {', '.join(video_info['tags'])}")
            
        if video_info["transcript"]:
            content_parts.append(f"å­—å¹•/è½¬å½•: {video_info['transcript']}")
            
        if video_info["comments_sample"]:
            content_parts.append(f"çƒ­é—¨è¯„è®º: {'; '.join(video_info['comments_sample'][:5])}")
        
        combined_content = "\n\n".join(content_parts)
        
        # ğŸ¤– é›†æˆAIåˆ†æå’ŒçŸ¥è¯†å›¾è°±
        print(f"ğŸ¤– å¼€å§‹è§†é¢‘å†…å®¹AIåˆ†æ: {video_info['platform']}")
        
        # åˆ›å»ºè™šæ‹Ÿæ–‡ä»¶åç”¨äºçŸ¥è¯†å›¾è°±
        virtual_filename = f"video_{video_info['platform'].lower()}_{video_info.get('title', 'unknown')[:50].replace(' ', '_')}.txt"
        
        # ğŸ¤– ä½¿ç”¨å®‰å…¨çš„AIåˆ†ææ–¹æ³• - ğŸ”¥ ä¿®å¤ï¼šä¿ç•™è§†é¢‘å†…å®¹ï¼ŒAIåˆ†æä»…ç”¨äºè¡¥å……
        ai_analysis = {}
        if combined_content and len(combined_content) > 50:
            try:
                ai_analysis = await safe_analyze_with_openai(combined_content, virtual_filename)
                print(f"âœ… è§†é¢‘AIåˆ†æå®Œæˆ: {len(ai_analysis.get('entities', []))}ä¸ªå®ä½“, {len(ai_analysis.get('concepts', []))}ä¸ªæ¦‚å¿µ")
            except Exception as ai_error:
                print(f"âŒ è§†é¢‘AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ: {ai_error}")
                ai_analysis = create_basic_analysis(combined_content, virtual_filename)
        else:
            ai_analysis = create_basic_analysis(combined_content, virtual_filename)
            
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šç¡®ä¿AIåˆ†æä¸è¦†ç›–å®é™…è§†é¢‘å†…å®¹
        # å¦‚æœAIåˆ†æè¿”å›äº†é€šç”¨å†…å®¹ï¼Œå¼ºåˆ¶ä½¿ç”¨å®é™…æå–çš„è§†é¢‘å†…å®¹
        ai_content = ai_analysis.get("content", "")
        if ("åŸºç¡€åˆ†æå®Œæˆ" in ai_content or "å…³äº æ–°é—» ç‰ˆæƒ" in ai_content or len(ai_content) < 100):
            print("ğŸ”§ æ£€æµ‹åˆ°AIåˆ†æè¿”å›é€šç”¨å†…å®¹ï¼Œä½¿ç”¨å®é™…è§†é¢‘å†…å®¹æ›¿ä»£")
            ai_analysis["content"] = combined_content  # ä½¿ç”¨å®é™…æå–çš„è§†é¢‘å†…å®¹
            ai_analysis["original_ai_content"] = ai_content  # ä¿å­˜åŸAIåˆ†æå†…å®¹ç”¨äºè°ƒè¯•
            print("âœ… å·²æ›¿æ¢AIåˆ†æå†…å®¹ä¸ºå®é™…è§†é¢‘å†…å®¹")
        
        # ğŸ”§ ä¿®å¤æ•°æ®åŒæ­¥é—®é¢˜ - åœ¨å†…å®¹ä¿®å¤åå†åŒæ­¥
        synchronized_data = synchronize_graph_data(ai_analysis)
        
        # ğŸ” æ·»åŠ å†…å®¹è´¨é‡è¯„ä¼°
        content_quality_metrics = {}
        if combined_content:
            try:
                print(f"ğŸ” å¼€å§‹è§†é¢‘å†…å®¹è´¨é‡åˆ†æ: {len(combined_content)} å­—ç¬¦")
                content_quality_metrics = analyze_content_quality(combined_content, ".txt")
                print(f"âœ… è§†é¢‘å†…å®¹è´¨é‡åˆ†æå®Œæˆ")
            except Exception as quality_error:
                print(f"âŒ Video content quality analysis failed: {quality_error}")
                content_quality_metrics = {
                    'completeness_score': 0.8,
                    'readability_score': 0.9,
                    'information_density': 0.7,
                    'structure_integrity': 0.8,
                    'overall_score': 0.8
                }
        
        # ğŸ¯ æ·»åŠ æå–å‡†ç¡®æ€§éªŒè¯
        extraction_validation = {}
        if combined_content and synchronized_data:
            try:
                print(f"ğŸ¯ å¼€å§‹è§†é¢‘æå–å‡†ç¡®æ€§éªŒè¯")
                extraction_validation = validate_extraction_accuracy(synchronized_data, combined_content, virtual_filename)
                print(f"âœ… è§†é¢‘æå–å‡†ç¡®æ€§éªŒè¯å®Œæˆ")
            except Exception as extraction_error:
                print(f"âŒ Video extraction validation failed: {extraction_error}")
                extraction_validation = {
                    "accuracy_score": 0.8,
                    "validation_checks": {},
                    "warnings": ["éªŒè¯è¿‡ç¨‹å‡ºé”™"],
                    "recommendations": ["å»ºè®®äººå·¥å®¡æ ¸"]
                }
        
        # ğŸ¯ ç”Ÿæˆæ–‡æ¡£ç›®å½•å’Œå†…å®¹ç»“æ„
        document_structure = generate_document_structure(combined_content, virtual_filename)
        
        # ğŸ”¥ å›¾è°±æ›´æ–°ç»“æœï¼ˆå®‰å…¨æ¨¡å¼ï¼‰
        graph_update_result = {
            "status": "safe_mode",
            "message": "å›¾è°±æ›´æ–°å·²ç¦ç”¨ä»¥é¿å…æ®µé”™è¯¯",
            "updates": {"document_nodes": 1, "entity_nodes": 0, "relationships": 0}
        }
        
        print(f"âœ… è§†é¢‘å†…å®¹å·²æˆåŠŸé›†æˆåˆ°çŸ¥è¯†å›¾è°±ç³»ç»Ÿ: {video_info['platform']}")
        
        return {
            "status": "success",
            "url": url,
            "extraction_method": f"è§†é¢‘å†…å®¹æå– ({video_info['platform']})",
            "extraction_type": "video_extraction",
            "platform": video_info["platform"],
            "content": combined_content,
            "content_length": len(combined_content),
            "method": f"Video Content Extraction ({video_info['platform']})",
            "video_info": video_info,
            # ğŸ¯ æ–°å¢å®Œæ•´çš„åˆ†æç»“æœ - ä¸æ–‡ä»¶åˆ†æç«¯ç‚¹ä¿æŒä¸€è‡´
            "analysis": {
                "content": combined_content,  # ä½¿ç”¨å®Œæ•´çš„è§†é¢‘å†…å®¹
                "ai_analysis_summary": synchronized_data.get("content", "è§†é¢‘AIåˆ†æå®Œæˆ"),
                "concepts": synchronized_data.get("concepts", []),
                "entities": synchronized_data.get("entities", []),
                "relationships": synchronized_data.get("relationships", []),
                "knowledge_tree": synchronized_data.get("knowledge_tree", {}),
                "knowledgeTreeSuggestion": synchronized_data.get("knowledgeTreeSuggestion", f"è§†é¢‘å†…å®¹/{video_info['platform']}/AIåˆ†æ"),
                "confidence": synchronized_data.get("confidence", 0.85),
                "extraction_depth": {
                    "relationship_count": len(synchronized_data.get("relationships", [])),
                    "entity_count": len(synchronized_data.get("entities", [])),
                    "concept_count": len(synchronized_data.get("concepts", [])),
                    "has_knowledge_tree": bool(synchronized_data.get("knowledge_tree")),
                    "semantic_layers": len(synchronized_data.get("knowledge_tree", {}).get("semantic_clusters", [])),
                    "domain_identified": bool(synchronized_data.get("knowledge_tree", {}).get("domain")),
                    "theme_count": len(synchronized_data.get("knowledge_tree", {}).get("themes", []))
                },
                "content_quality": {
                    **content_quality_metrics,
                    "quality_grade": get_quality_grade(content_quality_metrics.get('overall_score', 0)) if content_quality_metrics else "è‰¯å¥½ (B)",
                    "recommendations": generate_quality_recommendations(content_quality_metrics) if content_quality_metrics else ["è§†é¢‘å†…å®¹è´¨é‡è‰¯å¥½"]
                },
                "extraction_validation": extraction_validation,
                "fileInfo": {
                    "filename": virtual_filename,
                    "source_url": url,
                    "type": "video_content",
                    "platform": video_info["platform"],
                    "textLength": len(combined_content),
                    "extraction_completeness": content_quality_metrics.get("completeness_score", 0.8),
                    "content_readability": content_quality_metrics.get("readability_score", 0.9)
                },
                "graph_update": graph_update_result,
                "debug_version": "2025-09-12-video-integration",  # è§†é¢‘é›†æˆç‰ˆæœ¬
                # ğŸ¯ æ–‡æ¡£ç»“æ„å’Œå†…å®¹
                "document": {
                    "raw_content": combined_content[:15000] + ("..." if len(combined_content) > 15000 else ""),  # å¢åŠ åŸå§‹å†…å®¹é•¿åº¦é™åˆ¶
                    "full_content": combined_content,  # å®Œæ•´å†…å®¹
                    "structure": document_structure,
                    "directory": document_structure.get("directory", []),
                    "sections": document_structure.get("sections", []),
                    "summary": document_structure.get("summary", ""),
                    "word_count": len(combined_content.split()) if combined_content else 0,
                    "char_count": len(combined_content) if combined_content else 0
                }
            },
            "service_ready": True
        }
        
    except Exception as e:
        print(f"âŒ è§†é¢‘å†…å®¹æå–å¤±è´¥: {e}")
        return {
            "status": "error",
            "message": f"è§†é¢‘å†…å®¹æå–å¤±è´¥: {str(e)}",
            "content": f"æ— æ³•æå–è§†é¢‘å†…å®¹: {url}",
            "content_length": 0,
            "service_ready": False
        }

def extract_youtube_content(url: str, video_info: dict) -> dict:
    """æå–YouTubeè§†é¢‘å†…å®¹ - å¢å¼ºç‰ˆï¼Œæ·±åº¦æå–è§†é¢‘ä¿¡æ¯"""
    try:
        print(f"ğŸ”´ YouTubeè§†é¢‘å†…å®¹æå–: {url}")
        video_info["platform"] = "YouTube"
        
        # å¢å¼ºçš„è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Upgrade-Insecure-Requests': '1'
        }
        
        print(f"ğŸŒ å‘é€è¯·æ±‚åˆ°YouTube...")
        response = requests.get(url, headers=headers, timeout=20)
        print(f"ğŸ“¡ HTTPçŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“ å“åº”é•¿åº¦: {len(response.content)} å­—èŠ‚")
        print(f"ğŸ” DEBUG: å“åº”å¤´: Content-Encoding = {response.headers.get('Content-Encoding', 'None')}")
        print(f"ğŸ” DEBUG: å“åº”å¤´: Content-Type = {response.headers.get('Content-Type', 'None')}")
        response.raise_for_status()
        
        # ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥åŸå§‹HTMLå†…å®¹
        # å¤„ç†ç¼–ç é—®é¢˜ - ç¡®ä¿æ­£ç¡®è§£ç gzipå†…å®¹
        try:
            import gzip
            # æ£€æŸ¥å“åº”æ˜¯å¦æ˜¯gzipå‹ç¼©çš„
            if response.headers.get('Content-Encoding') == 'gzip':
                # æ‰‹åŠ¨è§£å‹gzipå†…å®¹
                html_content = gzip.decompress(response.content).decode('utf-8', errors='ignore')
                print(f"ğŸ”§ æ£€æµ‹åˆ°gzipå‹ç¼©ï¼Œå·²æ‰‹åŠ¨è§£å‹")
            else:
                html_content = response.text
                
            # æ£€æŸ¥è§£å‹ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if html_content.startswith('<') or 'html' in html_content.lower()[:100]:
                print(f"âœ… HTMLå†…å®¹è§£ææˆåŠŸ")
            else:
                print(f"âš ï¸ å†…å®¹å¯èƒ½ä»æœ‰é—®é¢˜ï¼Œå°è¯•fallbackè§£ç ")
                html_content = response.content.decode('utf-8', errors='ignore')
                
        except Exception as e:
            print(f"âš ï¸ è§£å‹/è§£ç å‡ºé”™ï¼Œä½¿ç”¨fallback: {e}")
            html_content = response.content.decode('utf-8', errors='ignore')
        
        print(f"ğŸ” HTMLå†…å®¹å‰500å­—ç¬¦: {html_content[:500]}")
        print(f"ğŸ” æ£€æŸ¥æ˜¯å¦åŒ…å«YouTubeå…³é”®å…ƒç´ :")
        print(f"  - 'ytInitialPlayerResponse' å­˜åœ¨: {'ytInitialPlayerResponse' in html_content}")
        meta_og_title_exists = 'meta property="og:title"' in html_content
        print(f"  - 'meta property=\"og:title\"' å­˜åœ¨: {meta_og_title_exists}")
        print(f"  - 'videoDetails' å­˜åœ¨: {'videoDetails' in html_content}")
        print(f"  - '<title>' æ ‡ç­¾å­˜åœ¨: {'<title>' in html_content}")
        
        # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°åŒæ„é¡µé¢æˆ–é”™è¯¯é¡µé¢
        if 'consent.youtube.com' in html_content or 'before_you_continue' in html_content:
            print("âš ï¸ æ£€æµ‹åˆ°YouTubeåŒæ„é¡µé¢ï¼Œéœ€è¦cookieså¤„ç†")
        if 'This video is unavailable' in html_content:
            print("âš ï¸ æ£€æµ‹åˆ°è§†é¢‘ä¸å¯ç”¨ä¿¡æ¯")
        if 'Sign in to confirm your age' in html_content:
            print("âš ï¸ æ£€æµ‹åˆ°å¹´é¾„ç¡®è®¤é¡µé¢")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        print(f"ğŸ“„ HTMLè§£æå®Œæˆ")
        
        # è°ƒè¯•ï¼šæ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«é¢„æœŸçš„YouTubeå…ƒç´ 
        title_element = soup.find('title')
        page_title = title_element.get_text() if title_element else "æ— æ ‡é¢˜"
        print(f"ğŸ·ï¸ é¡µé¢æ ‡é¢˜: {page_title}")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸€äº›å…³é”®çš„YouTubeå…ƒç´ 
        meta_og_title = soup.find('meta', property='og:title')
        meta_description = soup.find('meta', property='og:description')
        print(f"ğŸ” Meta og:title å­˜åœ¨: {bool(meta_og_title)}")
        print(f"ğŸ” Meta og:description å­˜åœ¨: {bool(meta_description)}")
        
        if meta_og_title:
            print(f"ğŸ¯ å‘ç° og:title å†…å®¹: {meta_og_title.get('content', '')[:100]}")
        if meta_description:
            print(f"ğŸ¯ å‘ç° og:description å†…å®¹: {meta_description.get('content', '')[:100]}")
        
        # 1. å¤šé‡æ ‡é¢˜æå–ç­–ç•¥ - å¢åŠ è¯¦ç»†è°ƒè¯•
        title_strategies = [
            ('meta', {'property': 'og:title'}),
            ('meta', {'name': 'title'}),
            ('title', None),
            ('h1', {'class': 'ytd-video-primary-info-renderer'}),
            ('.ytd-video-primary-info-renderer h1', None),
            ('[data-testid="video-title"]', None)
        ]
        
        print(f"ğŸ¯ å¼€å§‹å°è¯• {len(title_strategies)} ç§æ ‡é¢˜æå–ç­–ç•¥...")
        
        for i, (selector, attrs) in enumerate(title_strategies):
            try:
                print(f"ğŸ” ç­–ç•¥ {i+1}: é€‰æ‹©å™¨='{selector}', å±æ€§={attrs}")
                
                if attrs:
                    element = soup.find(selector, attrs)
                    print(f"   - find() æŸ¥æ‰¾ç»“æœ: {bool(element)}")
                    if element:
                        title_text = element.get('content', '') if selector == 'meta' else element.get_text(strip=True)
                        print(f"   - æå–çš„æ–‡æœ¬: '{title_text[:100]}{'...' if len(title_text) > 100 else ''}'")
                        if title_text and title_text.lower() != 'youtube':
                            video_info["title"] = title_text.strip()
                            print(f"âœ… æ ‡é¢˜æå–æˆåŠŸ (ç­–ç•¥{i+1}): {video_info['title'][:50]}...")
                            break
                        else:
                            print(f"   - æ–‡æœ¬æ— æ•ˆ (ç©ºæˆ–ä¸º'youtube')")
                    else:
                        print(f"   - æœªæ‰¾åˆ°åŒ¹é…å…ƒç´ ")
                else:
                    element = soup.select_one(selector)
                    print(f"   - select_one() æŸ¥æ‰¾ç»“æœ: {bool(element)}")
                    if element:
                        title_text = element.get_text(strip=True)
                        print(f"   - æå–çš„æ–‡æœ¬: '{title_text[:100]}{'...' if len(title_text) > 100 else ''}'")
                        if title_text and title_text.lower() != 'youtube':
                            video_info["title"] = title_text.strip()
                            print(f"âœ… æ ‡é¢˜æå–æˆåŠŸ (ç­–ç•¥{i+1}): {video_info['title'][:50]}...")
                            break
                        else:
                            print(f"   - æ–‡æœ¬æ— æ•ˆ (ç©ºæˆ–ä¸º'youtube')")
                    else:
                        print(f"   - æœªæ‰¾åˆ°åŒ¹é…å…ƒç´ ")
            except Exception as e:
                print(f"âŒ ç­–ç•¥ {i+1} å¤±è´¥: {e}")
                continue
        
        if not video_info.get("title"):
            print(f"âš ï¸ æ‰€æœ‰æ ‡é¢˜æå–ç­–ç•¥éƒ½å¤±è´¥äº†")
        
        # 2. å¤šé‡æè¿°æå–ç­–ç•¥
        desc_strategies = [
            ('meta', {'property': 'og:description'}),
            ('meta', {'name': 'description'}),
            ('#description', None),
            ('.ytd-video-secondary-info-renderer #description', None),
            ('.watch-main-col #watch-description-text', None)
        ]
        
        for selector, attrs in desc_strategies:
            try:
                if attrs:
                    element = soup.find(selector, attrs)
                    if element:
                        desc_text = element.get('content', '') if selector == 'meta' else element.get_text(strip=True)
                        if desc_text and len(desc_text) > 10:
                            video_info["description"] = desc_text.strip()
                            print(f"âœ… æè¿°æå–æˆåŠŸ: {len(video_info['description'])} å­—ç¬¦")
                            break
                else:
                    element = soup.select_one(selector)
                    if element:
                        desc_text = element.get_text(strip=True)
                        if desc_text and len(desc_text) > 10:
                            video_info["description"] = desc_text.strip()
                            print(f"âœ… æè¿°æå–æˆåŠŸ: {len(video_info['description'])} å­—ç¬¦")
                            break
            except Exception as e:
                print(f"âš ï¸ æè¿°æå–ç­–ç•¥å¤±è´¥: {e}")
                continue
        
        # 3. å¤šé‡ä¸Šä¼ è€…æå–ç­–ç•¥
        uploader_strategies = [
            ('link', {'itemprop': 'name'}),
            ('meta', {'property': 'og:video:tag'}),
            ('.ytd-video-owner-renderer a', None),
            ('.ytd-channel-name a', None),
            ('#owner-name a', None),
            ('#upload-info strong', None)
        ]
        
        for selector, attrs in uploader_strategies:
            try:
                if attrs:
                    element = soup.find(selector, attrs)
                    if element:
                        uploader_text = element.get('content', '') if selector in ['meta', 'link'] else element.get_text(strip=True)
                        if uploader_text:
                            video_info["uploader"] = uploader_text.strip()
                            print(f"âœ… UPä¸»æå–æˆåŠŸ: {video_info['uploader']}")
                            break
                else:
                    element = soup.select_one(selector)
                    if element:
                        uploader_text = element.get_text(strip=True)
                        if uploader_text:
                            video_info["uploader"] = uploader_text.strip()
                            print(f"âœ… UPä¸»æå–æˆåŠŸ: {video_info['uploader']}")
                            break
            except Exception as e:
                print(f"âš ï¸ UPä¸»æå–ç­–ç•¥å¤±è´¥: {e}")
                continue
        
        # 4. æå–è§†é¢‘ç»Ÿè®¡ä¿¡æ¯
        try:
            # æ’­æ”¾é‡
            view_selectors = [
                ('meta', {'itemprop': 'interactionCount'}),
                ('.view-count', None),
                ('[class*="view"]', None),
                ('#count .view-count', None)
            ]
            
            for selector, attrs in view_selectors:
                try:
                    if attrs:
                        element = soup.find(selector, attrs)
                        if element:
                            view_text = element.get('content', '') if selector == 'meta' else element.get_text(strip=True)
                            if view_text and ('view' in view_text.lower() or 'æ¬¡è§‚çœ‹' in view_text or 'æ’­æ”¾' in view_text):
                                video_info["view_count"] = view_text
                                break
                    else:
                        element = soup.select_one(selector)
                        if element:
                            view_text = element.get_text(strip=True)
                            if view_text and ('view' in view_text.lower() or 'æ¬¡è§‚çœ‹' in view_text or 'æ’­æ”¾' in view_text):
                                video_info["view_count"] = view_text
                                break
                except:
                    continue
            
            # æ—¶é•¿
            duration_selectors = [
                ('meta', {'itemprop': 'duration'}),
                ('.ytp-time-duration', None),
                ('.video-duration', None)
            ]
            
            for selector, attrs in duration_selectors:
                try:
                    if attrs:
                        element = soup.find(selector, attrs)
                        if element:
                            duration_text = element.get('content', '') if selector == 'meta' else element.get_text(strip=True)
                            if duration_text and ':' in duration_text:
                                video_info["duration"] = duration_text
                                break
                    else:
                        element = soup.select_one(selector)
                        if element:
                            duration_text = element.get_text(strip=True)
                            if duration_text and ':' in duration_text:
                                video_info["duration"] = duration_text
                                break
                except:
                    continue
            
            # å‘å¸ƒæ—¶é—´
            upload_date_selectors = [
                ('meta', {'itemprop': 'uploadDate'}),
                ('.date', None),
                ('#info-strings yt-formatted-string', None)
            ]
            
            for selector, attrs in upload_date_selectors:
                try:
                    if attrs:
                        element = soup.find(selector, attrs)
                        if element:
                            date_text = element.get('content', '') if selector == 'meta' else element.get_text(strip=True)
                            if date_text:
                                video_info["upload_date"] = date_text
                                break
                    else:
                        element = soup.select_one(selector)
                        if element:
                            date_text = element.get_text(strip=True)
                            if date_text and any(word in date_text.lower() for word in ['ago', 'å‰', 'published', 'uploaded']):
                                video_info["upload_date"] = date_text
                                break
                except:
                    continue
                    
        except Exception as stats_error:
            print(f"âš ï¸ è§†é¢‘ç»Ÿè®¡ä¿¡æ¯æå–å¤±è´¥: {stats_error}")
        
        # 5. å°è¯•æå–æ ‡ç­¾å’Œå…³é”®è¯
        try:
            keywords_meta = soup.find('meta', {'name': 'keywords'})
            if keywords_meta:
                keywords = keywords_meta.get('content', '').split(',')
                video_info["tags"] = [tag.strip() for tag in keywords if tag.strip()][:10]  # é™åˆ¶æ•°é‡
            
            # ä¹Ÿå°è¯•ä»è§†é¢‘æè¿°ä¸­æå–æ ‡ç­¾
            if not video_info.get("tags"):
                hashtag_elements = soup.find_all('a', href=lambda x: x and '/hashtag/' in x)
                if hashtag_elements:
                    video_info["tags"] = [tag.get_text(strip=True) for tag in hashtag_elements[:10]]
                    
        except Exception as tag_error:
            print(f"âš ï¸ æ ‡ç­¾æå–å¤±è´¥: {tag_error}")
        
        # 6. å¢å¼ºçš„JSONæ•°æ®æå–
        try:
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string and 'ytInitialPlayerResponse' in script.string:
                    try:
                        import re
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–JSONæ•°æ®
                        pattern = r'ytInitialPlayerResponse\s*=\s*(\{.*?\});'
                        match = re.search(pattern, script.string)
                        if match:
                            json_str = match.group(1)
                            import json
                            player_data = json.loads(json_str)
                            
                            # ä»JSONä¸­æå–æ›´è¯¦ç»†çš„ä¿¡æ¯
                            video_details = player_data.get('videoDetails', {})
                            if video_details:
                                if not video_info.get("title") and video_details.get('title'):
                                    video_info["title"] = video_details['title']
                                if not video_info.get("description") and video_details.get('shortDescription'):
                                    video_info["description"] = video_details['shortDescription']
                                if not video_info.get("uploader") and video_details.get('author'):
                                    video_info["uploader"] = video_details['author']
                                if not video_info.get("view_count") and video_details.get('viewCount'):
                                    video_info["view_count"] = f"{video_details['viewCount']} views"
                                if not video_info.get("duration") and video_details.get('lengthSeconds'):
                                    seconds = int(video_details['lengthSeconds'])
                                    minutes = seconds // 60
                                    seconds = seconds % 60
                                    video_info["duration"] = f"{minutes}:{seconds:02d}"
                            
                            print(f"âœ… JSONæ•°æ®æå–æˆåŠŸï¼Œè·å¾—è¯¦ç»†è§†é¢‘ä¿¡æ¯")
                            break
                    except Exception as json_error:
                        print(f"âš ï¸ JSONè§£æå¤±è´¥: {json_error}")
                        continue
                        
        except Exception as json_extract_error:
            print(f"âš ï¸ JSONæ•°æ®æå–å¤±è´¥: {json_extract_error}")
        
        # 7. éªŒè¯å’Œæ¸…ç†æå–ç»“æœ
        if not video_info.get("title"):
            video_info["title"] = "æ— æ³•æå–è§†é¢‘æ ‡é¢˜"
        if not video_info.get("uploader"):
            video_info["uploader"] = "æœªçŸ¥åˆ›ä½œè€…"
        if not video_info.get("description"):
            video_info["description"] = "æ— æ³•æå–è§†é¢‘æè¿°"
        
        print(f"âœ… YouTubeå†…å®¹æå–å®Œæˆ:")
        print(f"  - æ ‡é¢˜: {video_info.get('title', 'N/A')[:50]}...")
        print(f"  - åˆ›ä½œè€…: {video_info.get('uploader', 'N/A')}")
        print(f"  - æ’­æ”¾é‡: {video_info.get('view_count', 'N/A')}")
        print(f"  - æ—¶é•¿: {video_info.get('duration', 'N/A')}")
        print(f"  - æè¿°é•¿åº¦: {len(video_info.get('description', ''))} å­—ç¬¦")
        
        return video_info
        
    except Exception as e:
        print(f"âŒ YouTubeå†…å®¹æå–å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        # æä¾›é”™è¯¯æƒ…å†µä¸‹çš„åŸºç¡€ä¿¡æ¯
        video_info["title"] = f"æå–å¤±è´¥: {str(e)}"
        video_info["uploader"] = "æå–å¤±è´¥"
        video_info["description"] = f"YouTubeè§†é¢‘å†…å®¹æå–é‡åˆ°é”™è¯¯: {str(e)}"
        return video_info

def extract_bilibili_content(url: str, video_info: dict) -> dict:
    """æå–Bilibiliè§†é¢‘å†…å®¹ - å¢å¼ºç‰ˆï¼Œæ”¯æŒå¤šç§é€‰æ‹©å™¨å’ŒAPIæ•°æ®æå–"""
    try:
        print("ğŸ“º Bilibiliè§†é¢‘å†…å®¹æå–...")
        video_info["platform"] = "Bilibili"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Referer': 'https://www.bilibili.com/',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'same-origin'
        }
        response = requests.get(url, headers=headers, timeout=20)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # 1. å¤šé‡æ ‡é¢˜æå–ç­–ç•¥
        title_strategies = [
            # æ–°ç‰ˆé¡µé¢ç»“æ„
            ('meta', {'property': 'og:title'}),
            ('meta', {'name': 'title'}),
            ('h1', {'class': 'video-title'}),
            ('h1', {'data-title': True}),
            ('.video-title', None),
            ('.mediainfo_mediaTitle__Zyiqh', None),
            ('.video-info-title', None),
            # é¡µé¢æ ‡é¢˜åå¤‡
            ('title', None)
        ]
        
        for selector, attrs in title_strategies:
            try:
                if attrs:
                    element = soup.find(selector, attrs)
                    if element:
                        if selector == 'meta':
                            title_text = element.get('content', '')
                        else:
                            title_text = element.get('data-title', '') or element.get_text(strip=True)
                        
                        if title_text and title_text != 'å“”å“©å“”å“© (ã‚œ-ã‚œ)ã¤ãƒ­ å¹²æ¯~-bilibili':
                            video_info["title"] = title_text.replace('_å“”å“©å“”å“©_bilibili', '').strip()
                            print(f"âœ… æ ‡é¢˜æå–æˆåŠŸ: {video_info['title'][:50]}...")
                            break
                else:
                    # CSSé€‰æ‹©å™¨
                    element = soup.select_one(selector)
                    if element:
                        title_text = element.get_text(strip=True)
                        if title_text and title_text != 'å“”å“©å“”å“© (ã‚œ-ã‚œ)ã¤ãƒ­ å¹²æ¯~-bilibili':
                            video_info["title"] = title_text.replace('_å“”å“©å“”å“©_bilibili', '').strip()
                            print(f"âœ… æ ‡é¢˜æå–æˆåŠŸ: {video_info['title'][:50]}...")
                            break
            except Exception as title_error:
                print(f"âš ï¸ æ ‡é¢˜æå–ç­–ç•¥å¤±è´¥: {title_error}")
                continue
        
        # 2. å¤šé‡UPä¸»æå–ç­–ç•¥
        uploader_strategies = [
            ('meta', {'name': 'author'}),
            ('meta', {'property': 'video:uploader'}),
            ('.up-info-detail .username', None),
            ('.up-name', None),
            ('.mediainfo_upName__1y3qV', None),
            ('.video-info-detail .username', None),
            ('[data-usercard-mid]', None)
        ]
        
        for selector, attrs in uploader_strategies:
            try:
                if attrs:
                    element = soup.find(selector, attrs)
                    if element:
                        uploader_text = element.get('content', '') or element.get_text(strip=True)
                        if uploader_text:
                            video_info["uploader"] = uploader_text.strip()
                            print(f"âœ… UPä¸»æå–æˆåŠŸ: {video_info['uploader']}")
                            break
                else:
                    element = soup.select_one(selector)
                    if element:
                        uploader_text = element.get_text(strip=True)
                        if uploader_text:
                            video_info["uploader"] = uploader_text.strip()
                            print(f"âœ… UPä¸»æå–æˆåŠŸ: {video_info['uploader']}")
                            break
            except Exception as uploader_error:
                print(f"âš ï¸ UPä¸»æå–ç­–ç•¥å¤±è´¥: {uploader_error}")
                continue
        
        # 3. å¤šé‡æè¿°æå–ç­–ç•¥
        desc_strategies = [
            ('meta', {'name': 'description'}),
            ('meta', {'property': 'og:description'}),
            ('.video-desc', None),
            ('.mediainfo_mediaDesc__pDtAy', None),
            ('.video-info-desc', None)
        ]
        
        for selector, attrs in desc_strategies:
            try:
                if attrs:
                    element = soup.find(selector, attrs)
                    if element:
                        desc_text = element.get('content', '') or element.get_text(strip=True)
                        if desc_text and len(desc_text) > 10:
                            video_info["description"] = desc_text.strip()
                            print(f"âœ… æè¿°æå–æˆåŠŸ: {len(video_info['description'])} å­—ç¬¦")
                            break
                else:
                    element = soup.select_one(selector)
                    if element:
                        desc_text = element.get_text(strip=True)
                        if desc_text and len(desc_text) > 10:
                            video_info["description"] = desc_text.strip()
                            print(f"âœ… æè¿°æå–æˆåŠŸ: {len(video_info['description'])} å­—ç¬¦")
                            break
            except Exception as desc_error:
                print(f"âš ï¸ æè¿°æå–ç­–ç•¥å¤±è´¥: {desc_error}")
                continue
        
        # 4. å°è¯•æå–è§†é¢‘æ•°æ®ï¼ˆæ’­æ”¾é‡ã€æ—¶é•¿ç­‰ï¼‰
        try:
            # æ’­æ”¾é‡
            view_selectors = ['.view', '.mediainfo_mediaTag__XdGqF .view', '[title*="æ’­æ”¾"]']
            for selector in view_selectors:
                element = soup.select_one(selector)
                if element:
                    view_text = element.get_text(strip=True)
                    if view_text and ('æ’­æ”¾' in view_text or 'ä¸‡' in view_text or 'æ¬¡' in view_text):
                        video_info["view_count"] = view_text
                        break
            
            # æ—¶é•¿
            duration_selectors = ['.duration', '.mediainfo_duration__1y6pO', '.video-duration', '.duration-text']
            for selector in duration_selectors:
                element = soup.select_one(selector)
                if element:
                    duration_text = element.get_text(strip=True)
                    if duration_text and ':' in duration_text:
                        video_info["duration"] = duration_text
                        break
            
            # å‘å¸ƒæ—¶é—´
            date_selectors = ['.pubdate', '.video-data .pubdate', '.mediainfo_time__1MgtS']
            for selector in date_selectors:
                element = soup.select_one(selector)
                if element:
                    date_text = element.get_text(strip=True)
                    if date_text:
                        video_info["upload_date"] = date_text
                        break
                        
        except Exception as stats_error:
            print(f"âš ï¸ è§†é¢‘ç»Ÿè®¡ä¿¡æ¯æå–å¤±è´¥: {stats_error}")
        
        # 5. å°è¯•ä»é¡µé¢è„šæœ¬ä¸­æå–JSONæ•°æ®
        try:
            script_tags = soup.find_all('script')
            for script in script_tags:
                if script.string and ('window.__INITIAL_STATE__' in script.string or 'window.__playinfo__' in script.string):
                    script_content = script.string
                    # ç®€å•çš„JSONæ•°æ®æå–ï¼Œé¿å…å¤æ‚è§£æ
                    if '"title":' in script_content and not video_info.get("title"):
                        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„JSONè§£æé€»è¾‘
                        print("ğŸ” å‘ç°é¡µé¢JSONæ•°æ®ï¼Œä½†ä¸ºé¿å…å¤æ‚è§£ææš‚æ—¶è·³è¿‡")
                    break
        except Exception as json_error:
            print(f"âš ï¸ JSONæ•°æ®æå–å¤±è´¥: {json_error}")
        
        # 6. éªŒè¯æå–ç»“æœ
        if not video_info.get("title"):
            video_info["title"] = "æ ‡é¢˜æå–å¤±è´¥ - å¯èƒ½é¡µé¢ç»“æ„å·²å˜åŒ–"
        if not video_info.get("uploader"):
            video_info["uploader"] = "UPä¸»ä¿¡æ¯æœªæ‰¾åˆ°"
        if not video_info.get("description"):
            video_info["description"] = "æè¿°ä¿¡æ¯æœªæ‰¾åˆ°"
        
        print(f"âœ… Bilibiliå†…å®¹æå–å®Œæˆ:")
        print(f"  - æ ‡é¢˜: {video_info.get('title', 'N/A')[:50]}...")
        print(f"  - UPä¸»: {video_info.get('uploader', 'N/A')}")
        print(f"  - æ’­æ”¾é‡: {video_info.get('view_count', 'N/A')}")
        print(f"  - æ—¶é•¿: {video_info.get('duration', 'N/A')}")
        
        return video_info
        
    except Exception as e:
        print(f"âŒ Bilibiliå†…å®¹æå–å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        # æä¾›é”™è¯¯æƒ…å†µä¸‹çš„åŸºç¡€ä¿¡æ¯
        video_info["title"] = f"æå–å¤±è´¥: {str(e)}"
        video_info["uploader"] = "æå–å¤±è´¥"
        video_info["description"] = f"Bilibiliè§†é¢‘å†…å®¹æå–é‡åˆ°é”™è¯¯: {str(e)}"
        return video_info

def extract_vimeo_content(url: str, video_info: dict) -> dict:
    """æå–Vimeoè§†é¢‘å†…å®¹"""
    try:
        print("ğŸ¥ Vimeoè§†é¢‘å†…å®¹æå–...")
        video_info["platform"] = "Vimeo"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # æå–æ ‡é¢˜
        title_element = soup.find('meta', property='og:title')
        if title_element:
            video_info["title"] = title_element.get('content', '')
        
        # æå–æè¿°
        desc_element = soup.find('meta', property='og:description')
        if desc_element:
            video_info["description"] = desc_element.get('content', '')
        
        print(f"âœ… Vimeoå†…å®¹æå–å®Œæˆ: {video_info['title'][:50]}...")
        return video_info
        
    except Exception as e:
        print(f"âš ï¸ Vimeoå†…å®¹æå–å¤±è´¥: {e}")
        return video_info

def discover_important_subpages(base_url: str, soup: BeautifulSoup, max_pages: int = 10) -> list:
    """æ™ºèƒ½å‘ç°ç½‘ç«™çš„é‡è¦å­é¡µé¢ - å¢å¼ºç‰ˆï¼Œæ›´å®½æ¾çš„å‘ç°ç­–ç•¥"""
    try:
        from urllib.parse import urljoin, urlparse
        
        print(f"ğŸ” å¼€å§‹æ™ºèƒ½å‘ç°é‡è¦å­é¡µé¢: {base_url}")
        
        # æå–æ‰€æœ‰é“¾æ¥
        all_links = soup.find_all('a', href=True)
        base_domain = urlparse(base_url).netloc
        base_path = urlparse(base_url).path
        
        # æ‰©å±•é‡è¦é¡µé¢å…³é”®è¯ï¼ˆæ›´å®½æ¾çš„ç­–ç•¥ï¼‰
        important_keywords = {
            # é«˜ä¼˜å…ˆçº§ - æ ¸å¿ƒå†…å®¹
            'high': ['about', 'documentation', 'docs', 'api', 'guide', 'tutorial', 'getting-started', 
                    'overview', 'introduction', 'readme', 'features', 'product', 'service', 'home',
                    'main', 'index', 'dashboard', 'profile', 'settings', 'config'],
            # ä¸­ä¼˜å…ˆçº§ - è¯¦ç»†ä¿¡æ¯  
            'medium': ['help', 'support', 'faq', 'pricing', 'contact', 'team', 'news', 'blog', 
                      'download', 'install', 'setup', 'example', 'demo', 'learn', 'course',
                      'project', 'work', 'portfolio', 'gallery', 'media', 'video', 'image'],
            # ä½ä¼˜å…ˆçº§ - æ‰©å±•å†…å®¹
            'low': ['resources', 'community', 'forum', 'wiki', 'changelog', 'history', 
                   'archive', 'search', 'tag', 'category', 'topic', 'thread', 'post',
                   'article', 'story', 'event', 'calendar', 'schedule', 'tool', 'utility']
        }
        
        # ä¸­æ–‡å…³é”®è¯ï¼ˆå¤§å¤§æ‰©å±•ï¼‰
        chinese_keywords = {
            'high': ['å…³äº', 'æ–‡æ¡£', 'ä»‹ç»', 'è¯´æ˜', 'æŒ‡å—', 'æ•™ç¨‹', 'äº§å“', 'æœåŠ¡', 'åŠŸèƒ½', 'é¦–é¡µ',
                    'ä¸»é¡µ', 'æ¦‚è§ˆ', 'ç‰¹æ€§', 'ç‰¹è‰²', 'ä¼˜åŠ¿', 'è§£å†³æ–¹æ¡ˆ', 'æ–¹æ¡ˆ'],
            'medium': ['å¸®åŠ©', 'æ”¯æŒ', 'è”ç³»', 'å›¢é˜Ÿ', 'æ–°é—»', 'åšå®¢', 'ä¸‹è½½', 'å®‰è£…', 'é…ç½®', 'ç¤ºä¾‹',
                      'æ¡ˆä¾‹', 'é¡¹ç›®', 'ä½œå“', 'å±•ç¤º', 'æ¼”ç¤º', 'å­¦ä¹ ', 'è¯¾ç¨‹', 'åŸ¹è®­', 'èµ„æ–™'],
            'low': ['èµ„æº', 'ç¤¾åŒº', 'è®ºå›', 'ç™¾ç§‘', 'æ›´æ–°æ—¥å¿—', 'å†å²', 'å½’æ¡£', 'æœç´¢', 'æ ‡ç­¾',
                   'åˆ†ç±»', 'è¯é¢˜', 'è®¨è®º', 'æ–‡ç« ', 'æ•…äº‹', 'æ´»åŠ¨', 'æ—¥ç¨‹', 'å·¥å…·', 'åº”ç”¨']
        }
        
        # é€šç”¨æ–‡ä»¶å’Œé¡µé¢æ‰©å±•åæ¨¡å¼
        useful_extensions = ['.html', '.htm', '.php', '.asp', '.jsp', '.py', '.md', '.txt', '.pdf']
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯
        all_important_keywords = []
        for priority in ['high', 'medium', 'low']:
            all_important_keywords.extend(important_keywords[priority])
            all_important_keywords.extend(chinese_keywords[priority])
        
        # æ·»åŠ æ›´å¤šé€šç”¨æ¨¡å¼
        general_patterns = [
            'detail', 'info', 'more', 'view', 'show', 'display', 'list', 'page',
            'è¯¦ç»†', 'è¯¦æƒ…', 'æ›´å¤š', 'æŸ¥çœ‹', 'æ˜¾ç¤º', 'åˆ—è¡¨', 'é¡µé¢'
        ]
        all_important_keywords.extend(general_patterns)
        
        discovered_links = []
        
        for link in all_links:
            href = link.get('href', '').strip()
            link_text = link.get_text(strip=True).lower()
            link_title = link.get('title', '').lower()
            
            if not href:
                continue
                
            # æ„å»ºå®Œæ•´URL
            if href.startswith('http'):
                full_url = href
                link_domain = urlparse(full_url).netloc
                # åªå¤„ç†åŒåŸŸåçš„é“¾æ¥
                if link_domain != base_domain:
                    continue
            elif href.startswith('/'):
                full_url = urljoin(base_url, href)
            elif href.startswith('./') or not href.startswith('#'):
                full_url = urljoin(base_url, href)
            else:
                continue
            
            # è¿‡æ»¤æ‰æ˜æ˜¾ä¸éœ€è¦çš„é“¾æ¥ï¼ˆå‡å°‘è¿‡æ»¤æ¡ä»¶ï¼‰
            skip_patterns = ['javascript:', 'mailto:', 'tel:', 'ftp:', '#top', '#bottom', 'void(0)']
            if any(skip in href.lower() for skip in skip_patterns):
                continue
            
            # è¿‡æ»¤æ‰æ˜æ˜¾çš„åƒåœ¾é“¾æ¥
            if href == '/' or href == base_path or full_url == base_url:
                continue
            
            # è®¡ç®—é‡è¦æ€§è¯„åˆ†ï¼ˆæ›´å®½æ¾çš„è¯„åˆ†ç­–ç•¥ï¼‰
            importance_score = 0
            matched_keywords = []
            
            # æ£€æŸ¥URLè·¯å¾„ä¸­çš„å…³é”®è¯
            url_path = urlparse(full_url).path.lower()
            url_params = urlparse(full_url).query.lower()
            
            # åŸºç¡€è¯„åˆ†ï¼šå¦‚æœæ˜¯å­ç›®å½•æˆ–å­é¡µé¢ï¼Œç»™äºˆåŸºç¡€åˆ†æ•°
            if len(url_path.strip('/').split('/')) > len(base_path.strip('/').split('/')):
                importance_score += 1
            
            # å…³é”®è¯åŒ¹é…è¯„åˆ†
            for keyword in all_important_keywords:
                keyword_found = False
                if keyword in url_path or keyword in link_text or keyword in link_title or keyword in url_params:
                    keyword_found = True
                    
                if keyword_found:
                    if keyword in important_keywords['high'] or keyword in chinese_keywords['high']:
                        importance_score += 3
                    elif keyword in important_keywords['medium'] or keyword in chinese_keywords['medium']:
                        importance_score += 2
                    else:
                        importance_score += 1
                    matched_keywords.append(keyword)
            
            # ç‰¹æ®ŠURLæ¨¡å¼åŠ åˆ†ï¼ˆé™ä½é—¨æ§›ï¼‰
            special_patterns = ['/doc', '/api', '/guide', '/help', '/about', '/blog', '/news', '/project', '/work']
            if any(pattern in url_path for pattern in special_patterns):
                importance_score += 2
                
            # ä¸­æ–‡è·¯å¾„åŠ åˆ†
            chinese_patterns = ['å…³äº', 'æ–‡æ¡£', 'å¸®åŠ©', 'äº§å“', 'æœåŠ¡', 'æ–°é—»', 'åšå®¢', 'é¡¹ç›®', 'ä½œå“']
            if any(chinese in url_path for chinese in chinese_patterns):
                importance_score += 2
            
            # æ–‡ä»¶æ‰©å±•ååŠ åˆ†
            if any(url_path.endswith(ext) for ext in useful_extensions):
                importance_score += 1
            
            # é“¾æ¥æ–‡æœ¬åŒ…å«æœ‰ç”¨ä¿¡æ¯
            if len(link_text) > 3 and len(link_text) < 100:
                importance_score += 1
                
            # æ•°å­—é¡µé¢ï¼ˆå¦‚åˆ†é¡µï¼‰ä¹Ÿå¯èƒ½æœ‰ç”¨
            if any(char.isdigit() for char in url_path) and 'page' in url_path:
                importance_score += 1
            
            # é™ä½è¯„åˆ†é—¨æ§›ï¼šåŸæ¥éœ€è¦ > 0ï¼Œç°åœ¨åªè¦ä¸æ˜¯è´Ÿæ•°å°±è¡Œ
            if importance_score >= 0:
                # å³ä½¿æ²¡æœ‰æ˜ç¡®å…³é”®è¯åŒ¹é…ï¼Œå¦‚æœæ˜¯å­é¡µé¢ä¹Ÿç»™ä¸ªåŸºç¡€åˆ†
                if importance_score == 0 and (len(url_path.strip('/')) > len(base_path.strip('/')) or '?' in full_url):
                    importance_score = 1
                    matched_keywords = ['subpage']
                
                if importance_score > 0:
                    discovered_links.append({
                        'url': full_url,
                        'text': link.get_text(strip=True)[:100] if link.get_text(strip=True) else href.split('/')[-1],
                        'importance_score': importance_score,
                        'matched_keywords': matched_keywords,
                        'path': url_path,
                        'title': link_title[:50] if link_title else ''
                    })
        
        # æŒ‰é‡è¦æ€§è¯„åˆ†æ’åºå¹¶å»é‡
        seen_urls = set()
        unique_links = []
        
        # å¢åŠ å‘ç°é¡µé¢çš„æ•°é‡é™åˆ¶
        max_pages = min(max_pages * 2, 50)  # å…è®¸å‘ç°æ›´å¤šé¡µé¢ï¼Œæé«˜åˆ°50é¡µ
        
        for link in sorted(discovered_links, key=lambda x: x['importance_score'], reverse=True):
            if link['url'] not in seen_urls and len(unique_links) < max_pages:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        print(f"âœ… å‘ç° {len(unique_links)} ä¸ªé‡è¦å­é¡µé¢ï¼ˆæ€»é“¾æ¥æ•°: {len(all_links)}ï¼‰")
        for i, link in enumerate(unique_links[:8]):  # æ˜¾ç¤ºå‰8ä¸ª
            keywords_str = ', '.join(link['matched_keywords'][:3]) if link['matched_keywords'] else 'subpage'
            print(f"  {i+1}. [{link['importance_score']}åˆ†] {link['text'][:40]}... ({keywords_str})")
            print(f"      URL: {link['url']}")
        
        if len(unique_links) > 8:
            print(f"  ... è¿˜æœ‰ {len(unique_links) - 8} ä¸ªå­é¡µé¢æœªæ˜¾ç¤º")
        
        return unique_links
        
    except Exception as e:
        print(f"âŒ å­é¡µé¢å‘ç°å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return []

def crawl_subpages_content(subpages: list, headers: dict, temp_dir: str, base_url: str) -> int:
    """æ‰¹é‡æŠ“å–å­é¡µé¢å†…å®¹"""
    try:
        print(f"ğŸ“¥ å¼€å§‹æŠ“å– {len(subpages)} ä¸ªå­é¡µé¢å†…å®¹...")
        
        crawled_count = 0
        
        for i, page_info in enumerate(subpages):
            try:
                page_url = page_info['url']
                page_name = f"subpage_{i+1}_{page_info['importance_score']}points"
                
                print(f"ğŸ“„ æŠ“å–å­é¡µé¢ {i+1}/{len(subpages)}: {page_info['text'][:30]}...")
                
                # è·å–å­é¡µé¢å†…å®¹
                page_response = requests.get(page_url, headers=headers, timeout=15)
                if page_response.status_code == 200:
                    page_soup = BeautifulSoup(page_response.text, 'html.parser')
                    
                    # ç§»é™¤æ— å…³å…ƒç´ 
                    for element in page_soup(["script", "style", "nav", "footer", "header", "aside"]):
                        element.decompose()
                    
                    # æå–ä¸»è¦å†…å®¹
                    main_content = ""
                    
                    # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
                    content_selectors = [
                        'main', 'article', '.content', '.main-content', 
                        '.post-content', '.entry-content', '#content', 
                        '.page-content', '.article-content'
                    ]
                    
                    main_element = None
                    for selector in content_selectors:
                        main_element = page_soup.select_one(selector)
                        if main_element:
                            main_content = main_element.get_text(separator='\n', strip=True)
                            break
                    
                    # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨å…¨éƒ¨æ–‡æœ¬
                    if not main_content:
                        main_content = page_soup.get_text(separator='\n', strip=True)
                    
                    # é™åˆ¶å†…å®¹é•¿åº¦ä»¥é¿å…è¿‡å¤§æ–‡ä»¶
                    if len(main_content) > 15000:
                        main_content = main_content[:15000] + "..."
                    
                    # ä¿å­˜å­é¡µé¢å†…å®¹
                    if main_content.strip():
                        subpage_file = os.path.join(temp_dir, f"{page_name}.txt")
                        with open(subpage_file, 'w', encoding='utf-8') as f:
                            f.write(f"å­é¡µé¢æ ‡é¢˜: {page_soup.find('title').get_text() if page_soup.find('title') else 'æœªçŸ¥'}\n")
                            f.write(f"å­é¡µé¢URL: {page_url}\n")
                            f.write(f"é‡è¦æ€§è¯„åˆ†: {page_info['importance_score']}\n")
                            f.write(f"åŒ¹é…å…³é”®è¯: {', '.join(page_info['matched_keywords'])}\n")
                            f.write(f"é“¾æ¥æ–‡æœ¬: {page_info['text']}\n\n")
                            f.write("=== é¡µé¢å†…å®¹ ===\n")
                            f.write(main_content)
                        
                        crawled_count += 1
                        print(f"âœ… å­é¡µé¢ {i+1} å†…å®¹å·²ä¿å­˜ ({len(main_content)} å­—ç¬¦)")
                    else:
                        print(f"âš ï¸ å­é¡µé¢ {i+1} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                        
                else:
                    print(f"âš ï¸ å­é¡µé¢ {i+1} è®¿é—®å¤±è´¥: HTTP {page_response.status_code}")
                    
            except Exception as page_error:
                print(f"âŒ æŠ“å–å­é¡µé¢ {i+1} å¤±è´¥: {page_error}")
                continue
        
        print(f"ğŸ¯ å­é¡µé¢æŠ“å–å®Œæˆ: æˆåŠŸ {crawled_count}/{len(subpages)} ä¸ª")
        return crawled_count
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡æŠ“å–å¤±è´¥: {e}")
        return 0

def extract_generic_video_content(url: str, video_info: dict) -> dict:
    """é€šç”¨è§†é¢‘é¡µé¢å†…å®¹æå–"""
    try:
        print("ğŸ¬ é€šç”¨è§†é¢‘å†…å®¹æå–...")
        video_info["platform"] = "Generic Video"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # å°è¯•ä»å¸¸ç”¨metaæ ‡ç­¾æå–ä¿¡æ¯
        title_element = soup.find('meta', property='og:title') or soup.find('title')
        if title_element:
            video_info["title"] = title_element.get('content', '') or title_element.get_text()
        
        desc_element = soup.find('meta', property='og:description') or soup.find('meta', {'name': 'description'})
        if desc_element:
            video_info["description"] = desc_element.get('content', '')
        
        print(f"âœ… é€šç”¨è§†é¢‘å†…å®¹æå–å®Œæˆ: {video_info['title'][:50]}...")
        return video_info
        
    except Exception as e:
        print(f"âš ï¸ é€šç”¨è§†é¢‘å†…å®¹æå–å¤±è´¥: {e}")
        return video_info

def is_video_url(url: str) -> bool:
    """æ£€æµ‹æ˜¯å¦ä¸ºè§†é¢‘é“¾æ¥"""
    video_platforms = [
        'youtube.com', 'youtu.be',
        'bilibili.com', 'b23.tv',
        'vimeo.com',
        'dailymotion.com',
        'twitch.tv',
        'tiktok.com',
        'instagram.com/p/',  # Instagramè§†é¢‘
        'twitter.com', 'x.com',  # Twitterè§†é¢‘
    ]
    
    # æ£€æŸ¥URLä¸­æ˜¯å¦åŒ…å«è§†é¢‘å¹³å°åŸŸå
    for platform in video_platforms:
        if platform in url:
            return True
    
    # æ£€æŸ¥URLæ˜¯å¦ä»¥è§†é¢‘æ–‡ä»¶æ‰©å±•åç»“å°¾
    video_extensions = ['.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v']
    for ext in video_extensions:
        if url.lower().endswith(ext):
            return True
    
    return False

def generate_document_structure(text_content: str, filename: str) -> dict:
    """ç”Ÿæˆæ–‡æ¡£ç»“æ„å’Œç›®å½• - å¢å¼ºç‰ˆï¼Œæ”¯æŒGitHubä»“åº“å†…å®¹"""
    try:
        if not text_content or len(text_content) < 100:
            return {
                "directory": [],
                "sections": [],
                "summary": "æ–‡æ¡£å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆç»“æ„",
                "structure_type": "empty"
            }
        
        print(f"ğŸ“‹ å¼€å§‹ç”Ÿæˆæ–‡æ¡£ç»“æ„: {filename}")
        
        # æ£€æµ‹å†…å®¹ç±»å‹
        is_github_content = ('scraped_github' in filename or 
                           '=== ' in text_content or 
                           'GitHubé¡¹ç›®' in text_content or
                           'README:' in text_content)
        
        if is_github_content:
            return generate_github_document_structure(text_content, filename)
        else:
            return generate_traditional_document_structure(text_content, filename)
            
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡æ¡£ç»“æ„å¤±è´¥: {e}")
        return {
            "directory": [{"title": filename, "level": 1, "section_id": 1, "line_number": 1}],
            "sections": [{"title": filename, "content": text_content[:1000] + "...", "level": 1, "section_id": 1}],
            "summary": f"æ–‡æ¡£ç»“æ„ç”Ÿæˆå¤±è´¥: {str(e)}",
            "structure_type": "error"
        }

def classify_github_section(title_text: str) -> str:
    """å¯¹GitHubå†…å®¹ç« èŠ‚è¿›è¡Œåˆ†ç±»"""
    title_lower = title_text.lower()
    
    if '===' in title_text and any(ext in title_lower for ext in ['.py', '.js', '.java', '.cpp', '.rs', '.go']):
        return 'code'
    elif '===' in title_text and any(ext in title_lower for ext in ['.md', '.txt', '.rst']):
        return 'documentation'
    elif '===' in title_text and any(name in title_lower for name in ['package.json', 'requirements.txt', 'cargo.toml', 'pom.xml']):
        return 'config'
    elif 'githubé¡¹ç›®' in title_lower or 'é¡¹ç›®ä¿¡æ¯' in title_lower:
        return 'project_info'
    elif 'readme' in title_lower:
        return 'readme'
    elif any(keyword in title_lower for keyword in ['description', 'æè¿°', 'files', 'æ–‡ä»¶']):
        return 'metadata'
    elif title_text.startswith('#'):
        return 'markdown_header'
    elif 'å­é¡µé¢' in title_lower or 'subpage' in title_lower:
        return 'subpage'
    else:
        return 'general'

def organize_github_sections(sections: list) -> list:
    """å¯¹GitHubç« èŠ‚è¿›è¡Œç»„ç»‡å’Œæ’åº"""
    # æŒ‰ç±»å‹ä¼˜å…ˆçº§æ’åº
    type_priority = {
        'project_info': 1,
        'readme': 2, 
        'metadata': 3,
        'documentation': 4,
        'config': 5,
        'code': 6,
        'subpage': 7,
        'markdown_header': 8,
        'general': 9
    }
    
    # ä¸ºæ¯ä¸ªç« èŠ‚æ·»åŠ æ’åºæƒé‡
    for section in sections:
        section_type = section.get('section_type', 'general')
        section['sort_priority'] = type_priority.get(section_type, 10)
    
    # æŒ‰ä¼˜å…ˆçº§å’ŒåŸå§‹é¡ºåºæ’åº
    organized = sorted(sections, key=lambda x: (x['sort_priority'], x.get('line_start', 0)))
    
    return organized

def organize_github_directory(directory: list) -> list:
    """å¯¹GitHubç›®å½•è¿›è¡Œç»„ç»‡"""
    # åŒæ ·çš„ä¼˜å…ˆçº§æ’åºé€»è¾‘
    type_priority = {
        'project_info': 1,
        'readme': 2,
        'metadata': 3, 
        'documentation': 4,
        'config': 5,
        'code': 6,
        'subpage': 7,
        'markdown_header': 8,
        'general': 9
    }
    
    for item in directory:
        section_type = item.get('section_type', 'general')
        item['sort_priority'] = type_priority.get(section_type, 10)
    
    return sorted(directory, key=lambda x: (x['sort_priority'], x.get('line_number', 0)))

def generate_github_summary(text_content: str, sections: list) -> str:
    """ç”ŸæˆGitHubä»“åº“çš„ä¸“é—¨æ‘˜è¦"""
    try:
        # åŸºæœ¬ç»Ÿè®¡
        word_count = len(text_content.split())
        char_count = len(text_content)
        section_count = len(sections)
        
        # æŒ‰ç±»å‹ç»Ÿè®¡ç« èŠ‚
        section_types = {}
        for section in sections:
            section_type = section.get('section_type', 'general')
            section_types[section_type] = section_types.get(section_type, 0) + 1
        
        # æ„å»ºGitHubç‰¹è‰²æ‘˜è¦
        summary_parts = []
        summary_parts.append(f"è¿™æ˜¯ä¸€ä¸ªGitHubä»“åº“çš„æ·±åº¦åˆ†æï¼ŒåŒ…å«{word_count}è¯ã€{char_count}å­—ç¬¦ã€‚")
        
        if section_count > 1:
            summary_parts.append(f"ä»“åº“å†…å®¹è¢«ç»„ç»‡ä¸º{section_count}ä¸ªç»“æ„åŒ–ç« èŠ‚ã€‚")
            
            # è¯¦ç»†æè¿°å†…å®¹ç»„æˆ
            content_description = []
            if section_types.get('project_info', 0) > 0:
                content_description.append("é¡¹ç›®åŸºæœ¬ä¿¡æ¯")
            if section_types.get('readme', 0) > 0:
                content_description.append("READMEæ–‡æ¡£")
            if section_types.get('code', 0) > 0:
                content_description.append(f"{section_types['code']}ä¸ªæºä»£ç æ–‡ä»¶")
            if section_types.get('config', 0) > 0:
                content_description.append(f"{section_types['config']}ä¸ªé…ç½®æ–‡ä»¶")
            if section_types.get('documentation', 0) > 0:
                content_description.append(f"{section_types['documentation']}ä¸ªæ–‡æ¡£æ–‡ä»¶")
            
            if content_description:
                summary_parts.append(f"ä¸»è¦åŒ…å«ï¼š{', '.join(content_description)}ã€‚")
        else:
            summary_parts.append("ä»“åº“ç»“æ„ç›¸å¯¹ç®€å•ï¼Œä¸ºå•ä¸€å†…å®¹å—ã€‚")
        
        # æå–ä»“åº“å…³é”®ä¿¡æ¯
        if 'GitHubé¡¹ç›®åŸºæœ¬ä¿¡æ¯:' in text_content:
            summary_parts.append("åŒ…å«å®Œæ•´çš„é¡¹ç›®å…ƒæ•°æ®å’ŒAPIä¿¡æ¯ã€‚")
        
        if 'README:' in text_content or '# ' in text_content:
            summary_parts.append("åŒ…å«è¯¦ç»†çš„é¡¹ç›®è¯´æ˜æ–‡æ¡£ã€‚")
        
        # æŠ€æœ¯æ ˆæ£€æµ‹
        tech_indicators = {
            'Python': ['requirements.txt', '.py', 'setup.py', 'pyproject.toml'],
            'JavaScript/Node.js': ['package.json', '.js', '.ts', 'yarn.lock'],
            'Java': ['pom.xml', '.java', 'build.gradle'],
            'Rust': ['Cargo.toml', '.rs'],
            'Go': ['go.mod', '.go'],
            'Docker': ['Dockerfile', 'docker-compose']
        }
        
        detected_tech = []
        for tech, indicators in tech_indicators.items():
            if any(indicator in text_content for indicator in indicators):
                detected_tech.append(tech)
        
        if detected_tech:
            summary_parts.append(f"æ£€æµ‹åˆ°æŠ€æœ¯æ ˆï¼š{', '.join(detected_tech[:3])}ã€‚")
        
        return " ".join(summary_parts)
        
    except Exception as e:
        return f"GitHubä»“åº“æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"

def generate_github_document_structure(text_content: str, filename: str) -> dict:
    """ä¸“é—¨ä¸ºGitHubä»“åº“å†…å®¹ç”Ÿæˆæ–‡æ¡£ç»“æ„"""
    print(f"ğŸ”§ ä½¿ç”¨GitHubä¼˜åŒ–çš„æ–‡æ¡£ç»“æ„ç”Ÿæˆå™¨")
    
    lines = text_content.split('\n')
    sections = []
    directory = []
    current_section = None
    
    # GitHubå†…å®¹çš„ç‰¹æ®Šæ ‡è¯†ç¬¦
    github_patterns = [
        (r'^=== (.+) ===$', 1),  # æ–‡ä»¶åˆ†éš”ç¬¦ï¼š=== filename.py ===
        (r'^GitHubé¡¹ç›®åŸºæœ¬ä¿¡æ¯:$', 1),  # é¡¹ç›®ä¿¡æ¯æ ‡é¢˜
        (r'^åç§°:', 2),  # é¡¹ç›®è¯¦ç»†ä¿¡æ¯
        (r'^README:$', 1),  # READMEæ ‡é¢˜
        (r'^Description:', 2),  # æè¿°
        (r'^Files:', 2),  # æ–‡ä»¶åˆ—è¡¨
        (r'^# (.+)$', 1),  # Markdownä¸€çº§æ ‡é¢˜
        (r'^## (.+)$', 2),  # MarkdownäºŒçº§æ ‡é¢˜
        (r'^### (.+)$', 3),  # Markdownä¸‰çº§æ ‡é¢˜
        (r'^å­é¡µé¢æ ‡é¢˜:', 1),  # å­é¡µé¢æ ‡é¢˜
        (r'^å­é¡µé¢URL:', 2),  # å­é¡µé¢URL
        (r'^é‡è¦æ€§è¯„åˆ†:', 2),  # é‡è¦æ€§è¯„åˆ†
        (r'^=== é¡µé¢å†…å®¹ ===$', 1),  # é¡µé¢å†…å®¹åˆ†éš”ç¬¦
    ]
    
    for i, line in enumerate(lines):
        line_stripped = line.strip()
        
        if not line_stripped:
            if current_section and current_section["content"]:
                current_section["content"] += "\n"
            continue
        
        # æ£€æµ‹GitHubç‰¹æ®Šæ ¼å¼çš„æ ‡é¢˜
        is_title = False
        title_level = 0
        title_text = line_stripped
        
        for pattern, level in github_patterns:
            match = re.match(pattern, line_stripped)
            if match:
                is_title = True
                title_level = level
                # å¦‚æœæœ‰æ•è·ç»„ï¼Œä½¿ç”¨æ•è·çš„å†…å®¹ä½œä¸ºæ ‡é¢˜
                if match.groups():
                    title_text = match.group(1)
                break
        
        # ç‰¹æ®Šå¤„ç†ï¼šæ£€æµ‹ä»£ç æ–‡ä»¶å†…å®¹ï¼ˆé€šå¸¸åœ¨ === filename === ä¹‹åï¼‰
        if not is_title and current_section and '===' in current_section.get("title", ""):
            # å¦‚æœå½“å‰åœ¨æ–‡ä»¶å†…å®¹ä¸­ï¼Œæ£€æµ‹ä»£ç ç»“æ„
            if (line_stripped.startswith(('class ', 'def ', 'function ', 'const ', 'let ', 'var ')) or
                line_stripped.startswith(('import ', 'from ', 'require(', '#include')) or
                line_stripped.endswith((':')) and len(line_stripped) < 80):
                is_title = True
                title_level = 3
                title_text = f"ğŸ“„ {line_stripped[:50]}{'...' if len(line_stripped) > 50 else ''}"
        
        if is_title:
            # ä¿å­˜å‰ä¸€ä¸ªç« èŠ‚
            if current_section and current_section.get("content", "").strip():
                current_section["line_end"] = i - 1
                current_section["content_preview"] = current_section["content"][:200] + "..." if len(current_section["content"]) > 200 else current_section["content"]
                sections.append(current_section.copy())
            
            # åˆ›å»ºæ–°ç« èŠ‚
            current_section = {
                "title": title_text,
                "content": "",
                "level": title_level,
                "line_start": i,
                "section_id": len(sections) + 1,
                "section_type": classify_github_section(title_text)
            }
            
            # æ·»åŠ åˆ°ç›®å½•
            directory.append({
                "title": title_text,
                "level": title_level,
                "section_id": len(sections) + 1,
                "line_number": i + 1,
                "section_type": current_section["section_type"]
            })
        else:
            # æ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
            if not current_section:
                current_section = {
                    "title": "é¡¹ç›®æ¦‚è§ˆ",
                    "content": "",
                    "level": 1,
                    "line_start": i,
                    "section_id": 1,
                    "section_type": "overview"
                }
            
            if current_section["content"]:
                current_section["content"] += "\n"
            current_section["content"] += line_stripped
    
    # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
    if current_section and current_section.get("content", "").strip():
        current_section["line_end"] = len(lines) - 1
        current_section["content_preview"] = current_section["content"][:200] + "..." if len(current_section["content"]) > 200 else current_section["content"]
        sections.append(current_section)
    
    # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ç« èŠ‚ï¼Œåˆ›å»ºé»˜è®¤ç»“æ„
    if not sections:
        sections = [{
            "title": "GitHubä»“åº“å†…å®¹",
            "content": text_content.strip(),
            "level": 1,
            "line_start": 0,
            "line_end": len(lines) - 1,
            "section_id": 1,
            "section_type": "repository"
        }]
        directory = [{
            "title": "GitHubä»“åº“å†…å®¹",
            "level": 1,
            "section_id": 1,
            "line_number": 1,
            "section_type": "repository"
        }]
    
    # å¯¹ç« èŠ‚è¿›è¡Œåˆ†ç±»å’Œæ’åº
    sections = organize_github_sections(sections)
    directory = organize_github_directory(directory)
    
    # ç”ŸæˆGitHubä¼˜åŒ–çš„æ‘˜è¦
    summary = generate_github_summary(text_content, sections)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_words = len(text_content.split())
    total_chars = len(text_content)
    section_count = len(sections)
    
    print(f"âœ… GitHubæ–‡æ¡£ç»“æ„ç”Ÿæˆå®Œæˆ: {section_count}ä¸ªç« èŠ‚, {len(directory)}ä¸ªç›®å½•é¡¹")
    
    return {
        "directory": directory,
        "sections": sections,
        "summary": summary,
        "structure_type": "github_repository",
        "content_type": "github",
        "statistics": {
            "section_count": section_count,
            "total_words": total_words,
            "total_chars": total_chars,
            "avg_section_length": total_words // max(section_count, 1),
            "file_sections": len([s for s in sections if s.get("section_type") == "file"]),
            "code_sections": len([s for s in sections if s.get("section_type") == "code"]),
            "info_sections": len([s for s in sections if s.get("section_type") == "info"])
        }
    }

def generate_traditional_document_structure(text_content: str, filename: str) -> dict:
    """ä¼ ç»Ÿæ–‡æ¡£ç»“æ„ç”Ÿæˆï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
    try:
        lines = text_content.split('\n')
        sections = []
        directory = []
        current_section = {"title": "æ–‡æ¡£å¼€å§‹", "content": "", "level": 0, "line_start": 0}
        
        # ä¼ ç»Ÿæ–‡æ¡£æ ‡é¢˜æ£€æµ‹æ¨¡å¼
        title_patterns = [
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,3}[ã€ï¼.]',  # ä¸­æ–‡æ•°å­—æ ‡é¢˜ï¼šä¸€ã€äºŒã€
            r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ç« èŠ‚ç¯‡éƒ¨åˆ†æ¡æ¬¾][ã€ï¼.]',  # ç¬¬ä¸€ç« ã€ç¬¬äºŒèŠ‚
            r'^\d+[\.ï¼ã€]',  # é˜¿æ‹‰ä¼¯æ•°å­—æ ‡é¢˜ï¼š1. 2.
            r'^\(\d+\)',  # æ‹¬å·æ•°å­—ï¼š(1) (2)
            r'^[ï¼ˆ\(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ï¼‰\)]',  # ä¸­æ–‡æ‹¬å·ï¼šï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰
            r'^[A-Z]\.',  # è‹±æ–‡å­—æ¯ï¼šA. B.
            r'^#+\s',  # Markdownæ ‡é¢˜ï¼š# ## ###
        ]
        
        for i, line in enumerate(lines):
            line_stripped = line.strip()
            
            # è·³è¿‡ç©ºè¡Œ
            if not line_stripped:
                if current_section["content"]:
                    current_section["content"] += "\n"
                continue
            
            # æ£€æµ‹æ˜¯å¦ä¸ºæ ‡é¢˜
            is_title = False
            title_level = 0
            
            for pattern in title_patterns:
                if re.match(pattern, line_stripped):
                    is_title = True
                    # æ ¹æ®æ¨¡å¼ç¡®å®šæ ‡é¢˜å±‚çº§
                    if pattern.startswith('^#+'):
                        title_level = len(re.match(r'^#+', line_stripped).group())
                    elif 'ç¬¬' in pattern and ('ç« ' in line_stripped or 'èŠ‚' in line_stripped):
                        title_level = 1 if 'ç« ' in line_stripped else 2
                    elif pattern.startswith(r'^\d+'):
                        dots = len(re.findall(r'\.', line_stripped))
                        title_level = min(dots + 1, 3)
                    else:
                        title_level = 1
                    break
            
            # ä¹Ÿæ£€æµ‹å¯èƒ½çš„æ ‡é¢˜ï¼ˆçŸ­è¡Œã€ç»“å°¾æ— å¥å·ã€é¦–å­—æ¯å¤§å†™ï¼‰
            if not is_title and len(line_stripped) < 100:
                if (line_stripped[0].isupper() and 
                    not line_stripped.endswith(('.', 'ã€‚', '!', 'ï¼', '?', 'ï¼Ÿ')) and
                    not any(char.isdigit() for char in line_stripped[:10])):
                    # å¯èƒ½æ˜¯æ ‡é¢˜ï¼Œç»™äºˆè¾ƒä½çš„å±‚çº§
                    is_title = True
                    title_level = 3
            
            if is_title:
                # ä¿å­˜å‰ä¸€ä¸ªç« èŠ‚
                if current_section["content"].strip():
                    current_section["line_end"] = i - 1
                    sections.append(current_section.copy())
                
                # åˆ›å»ºæ–°ç« èŠ‚
                current_section = {
                    "title": line_stripped,
                    "content": "",
                    "level": title_level,
                    "line_start": i,
                    "section_id": len(sections) + 1
                }
                
                # æ·»åŠ åˆ°ç›®å½•
                directory.append({
                    "title": line_stripped,
                    "level": title_level,
                    "section_id": len(sections) + 1,
                    "line_number": i + 1
                })
            else:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                if current_section["content"]:
                    current_section["content"] += "\n"
                current_section["content"] += line_stripped
        
        # ä¿å­˜æœ€åä¸€ä¸ªç« èŠ‚
        if current_section["content"].strip():
            current_section["line_end"] = len(lines) - 1
            sections.append(current_section)
        
        # å¦‚æœæ²¡æœ‰æ£€æµ‹åˆ°ç« èŠ‚ï¼Œå°†æ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªç« èŠ‚
        if not sections:
            sections = [{
                "title": filename,
                "content": text_content.strip(),
                "level": 1,
                "line_start": 0,
                "line_end": len(lines) - 1,
                "section_id": 1
            }]
            directory = [{
                "title": filename,
                "level": 1,
                "section_id": 1,
                "line_number": 1
            }]
        
        # ç”Ÿæˆæ–‡æ¡£æ‘˜è¦
        summary = generate_document_summary(text_content, sections)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_words = len(text_content.split())
        total_chars = len(text_content)
        section_count = len(sections)
        
        print(f"âœ… æ–‡æ¡£ç»“æ„ç”Ÿæˆå®Œæˆ: {section_count}ä¸ªç« èŠ‚, {len(directory)}ä¸ªç›®å½•é¡¹")
        
        return {
            "directory": directory,
            "sections": sections,
            "summary": summary,
            "structure_type": "hierarchical" if len(sections) > 1 else "single",
            "statistics": {
                "section_count": section_count,
                "total_words": total_words,
                "total_chars": total_chars,
                "avg_section_length": total_words // max(section_count, 1)
            }
        }
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–‡æ¡£ç»“æ„å¤±è´¥: {e}")
        return {
            "directory": [{"title": filename, "level": 1, "section_id": 1, "line_number": 1}],
            "sections": [{"title": filename, "content": text_content[:1000] + "...", "level": 1, "section_id": 1}],
            "summary": f"æ–‡æ¡£ç»“æ„ç”Ÿæˆå¤±è´¥: {str(e)}",
            "structure_type": "error"
        }

def generate_document_summary(text_content: str, sections: list) -> str:
    """ç”Ÿæˆæ–‡æ¡£æ‘˜è¦"""
    try:
        if not text_content or len(text_content) < 100:
            return "æ–‡æ¡£å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"
        
        # åŸºæœ¬ç»Ÿè®¡
        word_count = len(text_content.split())
        char_count = len(text_content)
        section_count = len(sections)
        
        # æå–å‰200å­—ç¬¦ä½œä¸ºå¼€å¤´
        opening = text_content[:200].strip()
        if len(text_content) > 200:
            opening += "..."
        
        # æ„å»ºæ‘˜è¦
        summary_parts = []
        summary_parts.append(f"è¿™æ˜¯ä¸€ä»½åŒ…å«{word_count}è¯ã€{char_count}å­—ç¬¦çš„æ–‡æ¡£ã€‚")
        
        if section_count > 1:
            summary_parts.append(f"æ–‡æ¡£è¢«åˆ†ä¸º{section_count}ä¸ªç« èŠ‚ã€‚")
            
            # åˆ—å‡ºä¸»è¦ç« èŠ‚æ ‡é¢˜
            main_sections = [s["title"] for s in sections[:5] if s.get("level", 1) <= 2]
            if main_sections:
                summary_parts.append(f"ä¸»è¦ç« èŠ‚åŒ…æ‹¬ï¼š{', '.join(main_sections)}ã€‚")
        else:
            summary_parts.append("æ–‡æ¡£ç»“æ„è¾ƒä¸ºç®€å•ï¼Œä¸ºå•ä¸€ç« èŠ‚ã€‚")
        
        summary_parts.append(f"æ–‡æ¡£å¼€å¤´ï¼š{opening}")
        
        return " ".join(summary_parts)
        
    except Exception as e:
        return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"

def synchronize_graph_data(ai_analysis: dict) -> dict:
    """æè‡´æ€§èƒ½ä¼˜åŒ–çš„å›¾è°±æ•°æ®åŒæ­¥ - é™é»˜ç‰ˆæœ¬"""
    try:
        if not ai_analysis or not isinstance(ai_analysis, dict):
            return ai_analysis or {}
        
        entities = ai_analysis.get("entities", [])
        concepts = ai_analysis.get("concepts", [])
        relationships = ai_analysis.get("relationships", [])
        
        all_nodes = set()
        all_nodes.update(str(e).strip() for e in entities if e and str(e).strip())
        all_nodes.update(str(c).strip() for c in concepts if c and str(c).strip())
        
        missing_nodes = set()
        for rel in relationships:
            if isinstance(rel, dict):
                source = str(rel.get("source", "")).strip()
                target = str(rel.get("target", "")).strip()
                if source and source not in all_nodes:
                    missing_nodes.add(source)
                if target and target not in all_nodes:
                    missing_nodes.add(target)
        
        if missing_nodes:
            entities = list(entities) + list(missing_nodes)
        
        result = ai_analysis.copy()
        result["entities"] = entities
        return result
        
    except Exception:
        return ai_analysis

def get_quality_grade(overall_score: float) -> str:
    """æ ¹æ®æ€»ä½“è´¨é‡è¯„åˆ†è·å–è´¨é‡ç­‰çº§"""
    if overall_score >= 0.9:
        return "ä¼˜ç§€ (A)"
    elif overall_score >= 0.8:
        return "è‰¯å¥½ (B)"
    elif overall_score >= 0.7:
        return "ä¸­ç­‰ (C)"
    elif overall_score >= 0.6:
        return "åŠæ ¼ (D)"
    else:
        return "éœ€è¦æ”¹è¿› (F)"

def generate_quality_recommendations(metrics: dict) -> list:
    """åŸºäºè´¨é‡æŒ‡æ ‡ç”Ÿæˆæ”¹è¿›å»ºè®®"""
    recommendations = []
    
    if not metrics:
        return ["æ— æ³•ç”Ÿæˆå»ºè®®ï¼šç¼ºå°‘è´¨é‡æŒ‡æ ‡æ•°æ®"]
    
    completeness = metrics.get('completeness_score', 0)
    readability = metrics.get('readability_score', 0)
    info_density = metrics.get('information_density', 0)
    structure = metrics.get('structure_integrity', 0)
    garbled_ratio = metrics.get('garbled_ratio', 0)
    
    # å®Œæ•´æ€§å»ºè®®
    if completeness < 0.5:
        recommendations.append("å†…å®¹å®Œæ•´æ€§è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å®Œæ•´ä¸Šä¼ æˆ–å°è¯•å…¶ä»–æå–æ–¹æ³•")
    elif completeness < 0.7:
        recommendations.append("å†…å®¹å®Œæ•´æ€§ä¸­ç­‰ï¼Œéƒ¨åˆ†å†…å®¹å¯èƒ½æœªå®Œå…¨æå–")
    
    # å¯è¯»æ€§å»ºè®®
    if readability < 0.6:
        recommendations.append("æ–‡æ¡£å¯è¯»æ€§è¾ƒå·®ï¼Œå¯èƒ½åŒ…å«ä¹±ç æˆ–æ ¼å¼é—®é¢˜")
    if garbled_ratio > 0.1:
        recommendations.append(f"æ£€æµ‹åˆ°{garbled_ratio:.1%}çš„ä¹±ç å­—ç¬¦ï¼Œå»ºè®®ä½¿ç”¨OCRæˆ–å…¶ä»–æå–å·¥å…·")
    
    # ä¿¡æ¯å¯†åº¦å»ºè®®
    if info_density < 0.3:
        recommendations.append("ä¿¡æ¯å¯†åº¦è¾ƒä½ï¼Œæ–‡æ¡£å¯èƒ½ç¼ºå°‘å…³é”®æŠ€æœ¯æˆ–ä¸“ä¸šå†…å®¹")
    elif info_density < 0.5:
        recommendations.append("ä¿¡æ¯å¯†åº¦ä¸­ç­‰ï¼Œå»ºè®®è¡¥å……æ›´å¤šå…³é”®ä¿¡æ¯")
    
    # ç»“æ„å®Œæ•´æ€§å»ºè®®
    if structure < 0.4:
        recommendations.append("æ–‡æ¡£ç»“æ„ä¸å®Œæ•´ï¼Œç¼ºå°‘æ ‡é¢˜å’Œæ®µè½ç»„ç»‡")
    elif structure < 0.6:
        recommendations.append("æ–‡æ¡£ç»“æ„éœ€è¦æ”¹è¿›ï¼Œå»ºè®®æ·»åŠ æ›´å¤šå±‚æ¬¡åŒ–ç»„ç»‡")
    
    # ç»¼åˆå»ºè®®
    overall = metrics.get('overall_score', 0)
    if overall >= 0.8:
        recommendations.append("æ–‡æ¡£è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥è¿›è¡Œæ·±åº¦åˆ†æ")
    elif overall >= 0.6:
        recommendations.append("æ–‡æ¡£è´¨é‡ä¸­ç­‰ï¼Œå»ºè®®ä¼˜åŒ–åè¿›è¡Œæ·±åº¦åˆ†æ")
    else:
        recommendations.append("æ–‡æ¡£è´¨é‡è¾ƒä½ï¼Œå¼ºçƒˆå»ºè®®é‡æ–°å¤„ç†æˆ–ä½¿ç”¨ä¸“ä¸šå·¥å…·æå–å†…å®¹")
    
    return recommendations if recommendations else ["æ–‡æ¡£è´¨é‡è‰¯å¥½ï¼Œæ— éœ€ç‰¹åˆ«ä¼˜åŒ–"]

def validate_extraction_accuracy(ai_analysis: dict, text_content: str, filename: str) -> dict:
    """éªŒè¯AIåˆ†æç»“æœçš„å‡†ç¡®æ€§"""
    validation_result = {
        "accuracy_score": 0.0,
        "validation_checks": {},
        "warnings": [],
        "recommendations": []
    }
    
    try:
        if not text_content or not ai_analysis:
            validation_result["warnings"].append("ç¼ºå°‘å¿…è¦çš„åˆ†ææ•°æ®")
            return validation_result
        
        entities = ai_analysis.get("entities", [])
        concepts = ai_analysis.get("concepts", [])
        relationships = ai_analysis.get("relationships", [])
        
        # 1. å®ä½“éªŒè¯ï¼šæ£€æŸ¥å®ä½“æ˜¯å¦åœ¨åŸæ–‡ä¸­å­˜åœ¨
        entity_accuracy = 0.0
        verified_entities = 0
        
        for entity in entities:
            if isinstance(entity, str) and entity.lower() in text_content.lower():
                verified_entities += 1
        
        if entities:
            entity_accuracy = verified_entities / len(entities)
        
        # 2. æ¦‚å¿µéªŒè¯ï¼šæ£€æŸ¥æ¦‚å¿µæ˜¯å¦ä¸æ–‡æ¡£å†…å®¹ç›¸å…³
        concept_accuracy = 0.0
        verified_concepts = 0
        
        for concept in concepts:
            if isinstance(concept, str):
                # æ£€æŸ¥æ¦‚å¿µçš„å…³é”®è¯æ˜¯å¦åœ¨æ–‡æ¡£ä¸­å‡ºç°
                concept_keywords = concept.lower().split()
                if any(keyword in text_content.lower() for keyword in concept_keywords):
                    verified_concepts += 1
        
        if concepts:
            concept_accuracy = verified_concepts / len(concepts)
        
        # 3. å…³ç³»éªŒè¯ï¼šæ£€æŸ¥å…³ç³»çš„æºå’Œç›®æ ‡æ˜¯å¦éƒ½åœ¨æ–‡æ¡£ä¸­
        relationship_accuracy = 0.0
        verified_relationships = 0
        
        for rel in relationships:
            if isinstance(rel, dict):
                source = rel.get("source", "")
                target = rel.get("target", "")
                
                source_found = source.lower() in text_content.lower() if source else False
                target_found = target.lower() in text_content.lower() if target else False
                
                if source_found and target_found:
                    verified_relationships += 1
        
        if relationships:
            relationship_accuracy = verified_relationships / len(relationships)
        
        # 4. è®¡ç®—æ€»ä½“å‡†ç¡®æ€§è¯„åˆ†
        accuracy_components = [entity_accuracy, concept_accuracy, relationship_accuracy]
        non_zero_components = [score for score in accuracy_components if score > 0]
        overall_accuracy = sum(non_zero_components) / len(non_zero_components) if non_zero_components else 0.0
        
        # 5. ç”ŸæˆéªŒè¯æ£€æŸ¥ç»“æœ
        validation_result["validation_checks"] = {
            "entity_accuracy": entity_accuracy,
            "concept_accuracy": concept_accuracy,
            "relationship_accuracy": relationship_accuracy,
            "verified_entities": verified_entities,
            "total_entities": len(entities),
            "verified_concepts": verified_concepts,
            "total_concepts": len(concepts),
            "verified_relationships": verified_relationships,
            "total_relationships": len(relationships)
        }
        
        validation_result["accuracy_score"] = overall_accuracy
        
        # 6. ç”Ÿæˆè­¦å‘Šå’Œå»ºè®®
        if entity_accuracy < 0.7:
            validation_result["warnings"].append(f"å®ä½“å‡†ç¡®æ€§è¾ƒä½ ({entity_accuracy:.1%})ï¼Œéƒ¨åˆ†å®ä½“å¯èƒ½ä¸å­˜åœ¨äºåŸæ–‡ä¸­")
        
        if concept_accuracy < 0.6:
            validation_result["warnings"].append(f"æ¦‚å¿µå‡†ç¡®æ€§è¾ƒä½ ({concept_accuracy:.1%})ï¼Œéƒ¨åˆ†æ¦‚å¿µå¯èƒ½ä¸æ–‡æ¡£å†…å®¹ä¸ç¬¦")
        
        if relationship_accuracy < 0.5:
            validation_result["warnings"].append(f"å…³ç³»å‡†ç¡®æ€§è¾ƒä½ ({relationship_accuracy:.1%})ï¼Œéƒ¨åˆ†å…³ç³»å¯èƒ½æ˜¯æ¨æµ‹è€Œéæ˜ç¡®è¡¨è¿°")
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        if overall_accuracy >= 0.8:
            validation_result["recommendations"].append("æå–å‡†ç¡®æ€§è‰¯å¥½ï¼Œå¯ä¿¡åº¦è¾ƒé«˜")
        elif overall_accuracy >= 0.6:
            validation_result["recommendations"].append("æå–å‡†ç¡®æ€§ä¸­ç­‰ï¼Œå»ºè®®äººå·¥å®¡æ ¸å…³é”®ä¿¡æ¯")
        else:
            validation_result["recommendations"].append("æå–å‡†ç¡®æ€§è¾ƒä½ï¼Œå»ºè®®é‡æ–°åˆ†ææˆ–ä½¿ç”¨äººå·¥å®¡æ ¸")
        
    except Exception as e:
        validation_result["warnings"].append(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}")
    
    return validation_result

async def safe_analyze_with_openai(text_content: str, filename: str) -> dict:
    """ä½¿ç”¨OpenAIè¿›è¡ŒçœŸæ­£çš„AIå†…å®¹åˆ†æ"""
    try:
        import requests
        import json
        
        # è·å–API keyå¹¶æ¸…ç†ç©ºç™½å­—ç¬¦
        api_key = os.getenv('OPENAI_API_KEY')
        if not api_key:
            print("âŒ OPENAI_API_KEYç¯å¢ƒå˜é‡æœªè®¾ç½®")
            raise Exception("OPENAI_API_KEYæœªè®¾ç½®")
        
        # æ¸…ç†API keyä¸­çš„æ¢è¡Œç¬¦å’Œç©ºæ ¼    
        api_key = api_key.strip().replace('\n', '').replace(' ', '')
        print(f"âœ… ä½¿ç”¨æ¸…ç†åçš„OpenAI API Key: {api_key[:10]}...{api_key[-4:]}")  # å¼ºåˆ¶éƒ¨ç½²æ ‡è®°
        
        # ä¿ç•™å®Œæ•´å†…å®¹ç”¨äºå‰ç«¯æ˜¾ç¤º - å½»åº•ç§»é™¤AIåˆ†æçš„å­—ç¬¦é™åˆ¶
        original_text_content = text_content
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸ºAIåˆ†æåˆ›å»ºæ™ºèƒ½æ‘˜è¦ï¼Œä½†ä¿ç•™å®Œæ•´å†…å®¹è¿”å›å‰ç«¯
        ai_analysis_content = text_content
        if len(text_content) > 8000:  # åªæœ‰AIåˆ†ææ—¶æ‰æˆªæ–­ï¼Œå®Œæ•´å†…å®¹ä»ç„¶ä¿ç•™
            # æ™ºèƒ½æˆªå–ï¼šä¿ç•™å¼€å¤´å’Œç»“å°¾çš„é‡è¦ä¿¡æ¯
            start_content = text_content[:4000]
            end_content = text_content[-2000:]
            ai_analysis_content = start_content + "\n\n...[ä¸­é—´å†…å®¹çœç•¥]...\n\n" + end_content
            print(f"âš ï¸ AIåˆ†æä½¿ç”¨æ™ºèƒ½æ‘˜è¦: {len(ai_analysis_content)} å­—ç¬¦ï¼Œå®Œæ•´å†…å®¹: {len(original_text_content)} å­—ç¬¦")
            
        # æç®€åˆ†ææç¤º - ä½¿ç”¨æ™ºèƒ½æ‘˜è¦è¿›è¡ŒAIåˆ†æ
        prompt = f"""Extract key entities, concepts and relationships from: {ai_analysis_content}
Return JSON: {{"entities":["entity1","entity2"],"concepts":["concept1"],"relationships":[{{"source":"entity1","target":"entity2","type":"related"}}],"confidence":0.8}}"""

        # ç›´æ¥ä½¿ç”¨requestsè°ƒç”¨OpenAI API
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "temperature": 0,
            "max_tokens": 800
        }
        
        # å¿«é€Ÿå•æ¬¡è°ƒç”¨ - ä¸é‡è¯•ï¼Œæœ€å¤§åŒ–æ€§èƒ½
        try:
            response = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers=headers,
                json=data,
                timeout=10  # 10ç§’è¶…æ—¶
            )
        except requests.exceptions.ReadTimeout:
            # è¶…æ—¶ç›´æ¥é™çº§åˆ°åŸºç¡€åˆ†æ
            return create_basic_analysis(text_content, filename)
        
        if response.status_code != 200:
            print(f"âŒ OpenAI APIå“åº”é”™è¯¯: {response.status_code} - {response.text}")
            raise Exception(f"APIå“åº”é”™è¯¯: {response.status_code}")
        
        result_data = response.json()
        result_text = result_data['choices'][0]['message']['content']
        print(f"ğŸ¤– OpenAIå“åº”: {result_text[:100]}...")
        
        # æ¸…ç†å’Œè§£æJSON
        clean_text = result_text.strip()
        
        # æ£€æŸ¥å“åº”æ˜¯å¦ä¸ºç©º
        if not clean_text:
            print("âš ï¸ OpenAIè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
            return create_basic_analysis(text_content, filename)
        
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        if clean_text.startswith('```json'):
            clean_text = clean_text[7:]
        if clean_text.startswith('```'):
            clean_text = clean_text[3:]
        if clean_text.endswith('```'):
            clean_text = clean_text[:-3]
        clean_text = clean_text.strip()
        
        # å†æ¬¡æ£€æŸ¥æ¸…ç†åçš„æ–‡æœ¬
        if not clean_text:
            print("âš ï¸ æ¸…ç†åå“åº”ä¸ºç©ºï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
            return create_basic_analysis(text_content, filename)
        
        # å°è¯•ä¿®å¤JSONæ ¼å¼
        if not clean_text.startswith('{'):
            # å¦‚æœä¸æ˜¯ä»¥{å¼€å¤´ï¼ŒæŸ¥æ‰¾ç¬¬ä¸€ä¸ª{
            start_idx = clean_text.find('{')
            if start_idx != -1:
                clean_text = clean_text[start_idx:]
            else:
                print("âš ï¸ å“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
                return create_basic_analysis(text_content, filename)
        
        # å¤„ç†å¯èƒ½çš„JSONæˆªæ–­é—®é¢˜
        if not clean_text.endswith('}'):
            clean_text += '}'
        
        try:
            result = json.loads(clean_text)
        except json.JSONDecodeError as json_error:
            print(f"âš ï¸ JSONè§£æå¤±è´¥: {json_error}")
            print(f"âš ï¸ å°è¯•è§£æçš„æ–‡æœ¬: {clean_text[:200]}...")
            print("ğŸ”„ ä½¿ç”¨åŸºç¡€åˆ†æä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ")
            return create_basic_analysis(text_content, filename)
        print(f"âœ… åˆ†ææˆåŠŸ: {result}")
        
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸ºOpenAIåˆ†æç»“æœæ·»åŠ å®Œæ•´å†…å®¹å­—æ®µ
        # è¿™æ ·æ‰èƒ½åœ¨åç»­æ£€æµ‹ä¸­æ­£ç¡®åˆ¤æ–­æ˜¯å¦éœ€è¦æ›¿æ¢é€šç”¨å†…å®¹
        result["content"] = original_text_content
        
        return result
        
    except Exception as e:
        print(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {str(e)}")
        print(f"âŒ é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        print(f"âŒ å®Œæ•´traceback: {traceback.format_exc()}")
        # å¦‚æœæ˜¯è¶…æ—¶é”™è¯¯ï¼Œæä¾›ä¸€ä¸ªåŸºäºæ–‡æœ¬å†…å®¹çš„åŸºæœ¬åˆ†æ
        if "timeout" in str(e).lower() or "ReadTimeout" in str(e):
            print("ğŸ”„ æ£€æµ‹åˆ°è¶…æ—¶é”™è¯¯ï¼Œç”ŸæˆåŸºæœ¬åˆ†æç»“æœ")
            return create_basic_analysis(text_content, filename)
        
        return {
            "content": f"OpenAI APIè°ƒç”¨å¤±è´¥: {str(e)}",
            "concepts": ["APIé”™è¯¯", "è°ƒç”¨å¤±è´¥"],  
            "entities": ["OpenAI", "API"],
            "knowledgeTreeSuggestion": "ç³»ç»Ÿé”™è¯¯/APIè°ƒç”¨å¤±è´¥",
            "confidence": 0.1,
            "debug_error": str(e)  # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        }

@app.options("/api/graphrag/analyze")
async def options_analyze():
    """å¤„ç†CORSé¢„æ£€è¯·æ±‚"""
    return JSONResponse(content={"message": "OK"})

@app.get("/api/graphrag/document/{doc_id}")
async def get_document_content(doc_id: str):
    """è·å–æ–‡æ¡£å†…å®¹ç«¯ç‚¹ - æ”¯æŒæŒ‰ç« èŠ‚è·å–"""
    try:
        # è¿™é‡Œåº”è¯¥ä»æ•°æ®åº“æˆ–ç¼“å­˜ä¸­è·å–æ–‡æ¡£å†…å®¹
        # æš‚æ—¶ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…åº”ç”¨ä¸­åº”è¯¥ä»å­˜å‚¨ä¸­è·å–
        return {
            "status": "success",
            "message": "æ–‡æ¡£å†…å®¹ç«¯ç‚¹å·²å‡†å¤‡å°±ç»ª",
            "doc_id": doc_id,
            "note": "éœ€è¦å®é™…çš„æ–‡æ¡£å­˜å‚¨ç³»ç»Ÿæ¥å®Œæ•´å®ç°æ­¤åŠŸèƒ½"
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"è·å–æ–‡æ¡£å†…å®¹å¤±è´¥: {str(e)}"
            }
        )

@app.get("/api/documents")
async def get_documents():
    """è·å–æ‰€æœ‰å·²åˆ†æçš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        print("ğŸ“„ è·å–æ–‡æ¡£åˆ—è¡¨è¯·æ±‚")
        
        # ä»å†…å­˜å›¾è°±è·å–æ–‡æ¡£ä¿¡æ¯
        from safe_memory_graph import get_safe_memory_graph_db
        memory_db = get_safe_memory_graph_db()
        
        documents = []
        # è·å–æ‰€æœ‰æ–‡æ¡£èŠ‚ç‚¹
        all_nodes = memory_db.get_all_nodes()
        
        for node_id, node_data in all_nodes.items():
            if node_id.startswith("doc_") and node_data.get("type") == "document":
                doc_info = {
                    "id": node_id.replace("doc_", ""),
                    "title": node_data.get("title", f"æ–‡æ¡£ {node_id}"),
                    "type": node_data.get("document_type", "analysis"),
                    "content_length": node_data.get("content_length", 0),
                    "created_at": node_data.get("created_at", ""),
                    "source": node_data.get("source", ""),
                    "entities_count": len(node_data.get("entities", [])),
                    "concepts_count": len(node_data.get("concepts", []))
                }
                documents.append(doc_info)
        
        print(f"âœ… æ‰¾åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
        
        return {
            "success": True,
            "documents": documents,
            "total_documents": len(documents),
            "data_source": "memory_graph"
        }
        
    except Exception as e:
        print(f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return {
            "success": False,
            "documents": [],
            "total_documents": 0,
            "message": f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}",
            "data_source": "error"
        }

@app.delete("/api/documents/{doc_id}")
async def delete_document(doc_id: str):
    """åˆ é™¤æ–‡æ¡£ç«¯ç‚¹ - ä»çŸ¥è¯†å›¾è°±å’Œå­˜å‚¨ç³»ç»Ÿä¸­ç§»é™¤æ–‡æ¡£"""
    try:
        print(f"ğŸ—‘ï¸ å¼€å§‹åˆ é™¤æ–‡æ¡£: {doc_id}")
        
        # 1. å°è¯•ä»å†…å­˜å›¾è°±ä¸­åˆ é™¤æ–‡æ¡£èŠ‚ç‚¹å’Œç›¸å…³å…³ç³»
        try:
            from safe_memory_graph import get_safe_memory_graph_db
            memory_db = get_safe_memory_graph_db()
            
            # åˆ é™¤æ–‡æ¡£èŠ‚ç‚¹åŠå…¶æ‰€æœ‰å…³ç³»
            doc_node_id = f"doc_{doc_id}"
            if memory_db.get_node(doc_node_id):
                # è·å–ç›¸å…³çš„å®ä½“å’Œæ¦‚å¿µèŠ‚ç‚¹
                related_entities = memory_db.get_relationships(doc_node_id)
                
                # åˆ é™¤æ–‡æ¡£ç›¸å…³çš„å…³ç³»
                for rel in related_entities:
                    memory_db.delete_relationship(rel["source"], rel["target"], rel["type"])
                
                # åˆ é™¤æ–‡æ¡£èŠ‚ç‚¹
                memory_db.delete_node(doc_node_id)
                print(f"âœ… å·²ä»å†…å­˜å›¾è°±åˆ é™¤æ–‡æ¡£èŠ‚ç‚¹: {doc_node_id}")
            
        except Exception as memory_error:
            print(f"âš ï¸ å†…å­˜å›¾è°±åˆ é™¤å¤±è´¥: {memory_error}")
        
        # 2. å°è¯•ä»Neo4jæŒä¹…åŒ–å­˜å‚¨ä¸­åˆ é™¤
        try:
            from config.neo4jdb import get_db_manager
            
            # æ£€æŸ¥Neo4jç¯å¢ƒå˜é‡æ˜¯å¦é…ç½®
            neo4j_uri = os.getenv('NEO4J_URI')
            neo4j_username = os.getenv('NEO4J_USERNAME') 
            neo4j_password = os.getenv('NEO4J_PASSWORD')
            
            if all([neo4j_uri, neo4j_username, neo4j_password]):
                db_manager = get_db_manager()
                
                # åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰ç›¸å…³èŠ‚ç‚¹å’Œå…³ç³»
                delete_cypher = """
                MATCH (d:Document {id: $doc_id})
                OPTIONAL MATCH (d)-[r1]-()
                OPTIONAL MATCH (e:Entity {source_document: $filename})
                OPTIONAL MATCH (e)-[r2]-()
                OPTIONAL MATCH (c:Concept {source_document: $filename})
                OPTIONAL MATCH (c)-[r3]-()
                DELETE r1, r2, r3, d, e, c
                """
                
                db_manager.execute_query(delete_cypher, {
                    "doc_id": f"doc_{doc_id}",
                    "filename": f"document_{doc_id}"  # å‡è®¾æ–‡ä»¶åæ¨¡å¼
                })
                print(f"âœ… å·²ä»Neo4jåˆ é™¤æ–‡æ¡£: {doc_id}")
            else:
                print("â„¹ï¸ Neo4jæœªé…ç½®ï¼Œè·³è¿‡æŒä¹…åŒ–åˆ é™¤")
                
        except Exception as neo4j_error:
            print(f"âš ï¸ Neo4jåˆ é™¤å¤±è´¥: {neo4j_error}")
        
        # 3. åˆ é™¤æœ¬åœ°æ–‡ä»¶ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        try:
            import tempfile
            import shutil
            
            # æ¸…ç†å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
            temp_dir = tempfile.gettempdir()
            temp_files = [
                os.path.join(temp_dir, f"graphrag_{doc_id}.txt"),
                os.path.join(temp_dir, f"document_{doc_id}.txt"),
                os.path.join(temp_dir, f"scraped_{doc_id}.txt")
            ]
            
            for temp_file in temp_files:
                if os.path.exists(temp_file):
                    os.remove(temp_file)
                    print(f"âœ… åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                    
        except Exception as file_error:
            print(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {file_error}")
        
        print(f"âœ… æ–‡æ¡£åˆ é™¤æ“ä½œå®Œæˆ: {doc_id}")
        
        return {
            "status": "success",
            "message": f"æ–‡æ¡£ {doc_id} å·²æˆåŠŸåˆ é™¤",
            "doc_id": doc_id,
            "deleted_from": ["memory_graph", "storage_cache"],
            "timestamp": time.time()
        }
        
    except Exception as e:
        print(f"âŒ åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error", 
                "message": f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}",
                "doc_id": doc_id
            }
        )

@app.post("/api/graphrag/analyze")
async def analyze_document(file: UploadFile = File(...)):
    """çœŸæ­£çš„AIæ–‡æ¡£åˆ†æç«¯ç‚¹"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()
        filename = file.filename
        file_size = len(content)
        
        print(f"ğŸ“„ æ¥æ”¶åˆ°æ–‡ä»¶: {filename}, å¤§å°: {file_size} bytes")
        
        # ğŸ”¥ æå–æ–‡ä»¶æ–‡æœ¬å†…å®¹
        text_content = extract_text_from_file(content, filename)
        print(f"ğŸ“ æå–æ–‡æœ¬é•¿åº¦: {len(text_content)} å­—ç¬¦")
        
        # ğŸŒ æ£€æµ‹æ˜¯å¦ä¸ºURLå†…å®¹ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨å¢å¼ºçš„ç½‘ç«™æŠ“å–åŠŸèƒ½
        # æ£€æŸ¥æ–‡ä»¶å†…å®¹æ˜¯å¦æ˜¯ä¸€ä¸ªURLï¼ˆå»é™¤ç©ºç™½å­—ç¬¦åï¼‰
        cleaned_content = text_content.strip()
        if (cleaned_content.startswith(('http://', 'https://')) and 
            len(cleaned_content.split()) == 1 and 
            len(cleaned_content) < 500):  # URLé€šå¸¸ä¸ä¼šå¤ªé•¿
            
            print(f"ğŸŒ æ£€æµ‹åˆ°URLå†…å®¹ï¼Œä½¿ç”¨å¢å¼ºç½‘ç«™æŠ“å–åŠŸèƒ½: {cleaned_content}")
            try:
                # è°ƒç”¨æˆ‘ä»¬å¢å¼ºçš„ç½‘ç«™æŠ“å–åŠŸèƒ½
                scrape_result = await scrape_website({"url": cleaned_content})
                
                # å¦‚æœæŠ“å–æˆåŠŸï¼Œç›´æ¥è¿”å›æŠ“å–ç»“æœï¼ˆå·²åŒ…å«å®Œæ•´AIåˆ†æï¼‰
                if scrape_result.get("status") == "success":
                    print(f"âœ… URLæŠ“å–æˆåŠŸï¼Œè¿”å›å¢å¼ºåˆ†æç»“æœ")
                    return scrape_result
                else:
                    print(f"âš ï¸ URLæŠ“å–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ–‡æœ¬åˆ†æ: {scrape_result.get('error', 'Unknown error')}")
                    
            except Exception as url_error:
                print(f"âŒ URLå¤„ç†å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨æ–‡æœ¬åˆ†æ: {url_error}")
        
        # ğŸ¤– ä½¿ç”¨å®‰å…¨çš„AIåˆ†ææ–¹æ³•
        if text_content and len(text_content) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹åˆ†æ
            try:
                ai_analysis = await safe_analyze_with_openai(text_content, filename)
            except Exception as ai_error:
                print(f"âŒ AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ: {ai_error}")
                ai_analysis = {
                    "content": f"AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æã€‚æ–‡æ¡£ {filename} åŒ…å« {len(text_content)} å­—ç¬¦çš„å†…å®¹ã€‚",
                    "concepts": ["æ–‡æ¡£å¤„ç†", "å†…å®¹æå–", "åŸºç¡€åˆ†æ"],
                    "entities": ["æ–‡æ¡£", "ç³»ç»Ÿ"],
                    "knowledgeTreeSuggestion": "æ–‡æ¡£ç®¡ç†/åŸºç¡€åˆ†æ",
                    "confidence": 0.6
                }
        else:
            # å¦‚æœå†…å®¹å¤ªå°‘ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ
            ai_analysis = {
                "content": f"æ–‡æ¡£å†…å®¹è¾ƒå°‘æˆ–æ— æ³•æå–ï¼Œæ–‡ä»¶åï¼š{filename}",
                "concepts": ["æ–‡æ¡£å¤„ç†", "å†…å®¹æå–"],
                "entities": ["æ–‡æ¡£", "ç³»ç»Ÿ"],
                "knowledgeTreeSuggestion": "æ–‡æ¡£ç®¡ç†/å¾…åˆ†ç±»/éœ€è¦å¤„ç†",
                "confidence": 0.5
            }
        
        # ğŸ”¥ å®‰å…¨çš„å›¾è°±æ›´æ–° - é¿å…å¤æ‚ä¾èµ–
        try:
            graph_update_result = {
                "status": "safe_mode",
                "message": "å›¾è°±æ›´æ–°å·²ç¦ç”¨ä»¥é¿å…æ®µé”™è¯¯",
                "updates": {"document_nodes": 1, "entity_nodes": 0, "relationships": 0}
            }
        except Exception as graph_error:
            print(f"âŒ å›¾è°±æ›´æ–°å¤±è´¥: {graph_error}")
            graph_update_result = {
                "status": "error",
                "message": str(graph_error),
                "updates": {"document_nodes": 0, "entity_nodes": 0, "relationships": 0}
            }
        
        # ğŸ” æ·»åŠ å†…å®¹è´¨é‡è¯„ä¼° - ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬
        content_quality_metrics = {}
        if text_content:
            try:
                print(f"ğŸ” å¼€å§‹å†…å®¹è´¨é‡åˆ†æ: {len(text_content)} å­—ç¬¦")
                content_quality_metrics = analyze_content_quality(text_content, os.path.splitext(filename)[1])
                print(f"âœ… å†…å®¹è´¨é‡åˆ†æå®Œæˆ")
            except Exception as quality_error:
                print(f"âŒ Content quality analysis failed: {quality_error}")
                content_quality_metrics = {
                    'completeness_score': 0.5,
                    'readability_score': 0.5,
                    'information_density': 0.5,
                    'structure_integrity': 0.5,
                    'overall_score': 0.5
                }
        
        # ğŸ¯ æ·»åŠ æå–å‡†ç¡®æ€§éªŒè¯ - ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬
        extraction_validation = {}
        if text_content and ai_analysis:
            try:
                print(f"ğŸ¯ å¼€å§‹æå–å‡†ç¡®æ€§éªŒè¯")
                extraction_validation = validate_extraction_accuracy(ai_analysis, text_content, filename)
                print(f"âœ… æå–å‡†ç¡®æ€§éªŒè¯å®Œæˆ")
            except Exception as extraction_error:
                print(f"âŒ Extraction validation failed: {extraction_error}")
                extraction_validation = {
                    "accuracy_score": 0.5,
                    "validation_checks": {},
                    "warnings": ["éªŒè¯è¿‡ç¨‹å‡ºé”™"],
                    "recommendations": ["å»ºè®®äººå·¥å®¡æ ¸"]
                }
        
        # ğŸ”§ ä¿®å¤æ•°æ®åŒæ­¥é—®é¢˜ - ç¡®ä¿æ‰€æœ‰å…³ç³»èŠ‚ç‚¹éƒ½å­˜åœ¨äºå®ä½“æˆ–æ¦‚å¿µä¸­
        synchronized_data = synchronize_graph_data(ai_analysis)
        
        # ğŸ¯ ç”Ÿæˆæ–‡æ¡£ç›®å½•å’Œå†…å®¹ç»“æ„
        document_structure = generate_document_structure(text_content, filename)
        
        return {
            "status": "success",
            "analysis": {
                "content": text_content,  # ä½¿ç”¨å®Œæ•´çš„æ–‡æ¡£å†…å®¹
                "ai_analysis_summary": synchronized_data.get("content", "AIåˆ†æå®Œæˆ"),
                "concepts": synchronized_data.get("concepts", []),
                "entities": synchronized_data.get("entities", []),
                "relationships": synchronized_data.get("relationships", []),
                "knowledge_tree": synchronized_data.get("knowledge_tree", {}),
                "knowledgeTreeSuggestion": synchronized_data.get("knowledgeTreeSuggestion", "æ–‡æ¡£ç®¡ç†/AIåˆ†æ"),
                "confidence": synchronized_data.get("confidence", 0.85),
                "extraction_depth": {
                    "relationship_count": len(ai_analysis.get("relationships", [])),
                    "entity_count": len(ai_analysis.get("entities", [])),
                    "concept_count": len(ai_analysis.get("concepts", [])),
                    "has_knowledge_tree": bool(ai_analysis.get("knowledge_tree")),
                    "semantic_layers": len(ai_analysis.get("knowledge_tree", {}).get("semantic_clusters", [])),
                    "domain_identified": bool(ai_analysis.get("knowledge_tree", {}).get("domain")),
                    "theme_count": len(ai_analysis.get("knowledge_tree", {}).get("themes", []))
                },
                "content_quality": {
                    **content_quality_metrics,
                    "quality_grade": get_quality_grade(content_quality_metrics.get('overall_score', 0)) if content_quality_metrics else "N/A (analysis failed)",
                    "recommendations": generate_quality_recommendations(content_quality_metrics) if content_quality_metrics else ["Quality analysis not available"]
                },
                "extraction_validation": extraction_validation,
                "fileInfo": {
                    "filename": filename,
                    "size": file_size,
                    "type": file.content_type or "unknown",
                    "textLength": len(text_content) if 'text_content' in locals() else 0,
                    "extraction_completeness": content_quality_metrics.get("completeness_score", 0),
                    "content_readability": content_quality_metrics.get("readability_score", 0)
                },
                "graph_update": graph_update_result,
                "debug_version": "2025-09-10-v7-document-display",  # æ–‡æ¡£æ˜¾ç¤ºç‰ˆæœ¬
                # ğŸ¯ æ–°å¢æ–‡æ¡£ç»“æ„å’Œå†…å®¹
                "document": {
                    "raw_content": text_content[:15000] + ("..." if len(text_content) > 15000 else ""),  # å¢åŠ åŸå§‹å†…å®¹é•¿åº¦é™åˆ¶
                    "full_content": text_content,  # å®Œæ•´å†…å®¹
                    "structure": document_structure,
                    "directory": document_structure.get("directory", []),
                    "sections": document_structure.get("sections", []),
                    "summary": document_structure.get("summary", ""),
                    "word_count": len(text_content.split()) if text_content else 0,
                    "char_count": len(text_content) if text_content else 0
                }
            },
            "service_ready": True
        }
    except Exception as e:
        print(f"âŒ åˆ†æé”™è¯¯: {str(e)}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"åˆ†æå¤±è´¥: {str(e)}",
                "service_ready": False
            }
        )

@app.post("/api/scrape")
async def scrape_website(request: dict):
    """ç½‘ç«™å†…å®¹æŠ“å–ç«¯ç‚¹ - æ”¯æŒè§†é¢‘é“¾æ¥å’Œé€šç”¨ç½‘ç«™æ·±åº¦æŒ–æ˜"""
    try:
        import requests
        import tempfile
        import os
        
        print(f"ğŸŒ æ”¶åˆ°ç½‘ç«™æŠ“å–è¯·æ±‚: {request}")
        
        # ä»è¯·æ±‚ä¸­è·å–URL - æ”¯æŒå¤šç§æ ¼å¼
        url = request.get("url") or request.get("website_url") or request.get("link")
        if not url:
            raise ValueError("URLå‚æ•°ç¼ºå¤±")
        
        print(f"ğŸ¯ å¼€å§‹å¤„ç†URL: {url}")
        
        # ğŸ¬ æ£€æµ‹æ˜¯å¦ä¸ºè§†é¢‘é“¾æ¥
        print(f"ğŸ” DEBUG: æ£€æŸ¥URLæ˜¯å¦ä¸ºè§†é¢‘é“¾æ¥: {url}")
        is_video = is_video_url(url)
        print(f"ğŸ” DEBUG: is_video_url()ç»“æœ: {is_video}")
        
        if is_video:
            print(f"ğŸ¬ æ£€æµ‹åˆ°è§†é¢‘é“¾æ¥: {url}")
            video_result = await extract_video_content(url)
            print(f"ğŸ” DEBUG: è§†é¢‘æå–ç»“æœå†…å®¹é•¿åº¦: {len(video_result.get('content', ''))}")
            print(f"ğŸ” DEBUG: è§†é¢‘æå–æ–¹æ³•: {video_result.get('extraction_method', 'unknown')}")
            return video_result
        
        print(f"ğŸŒ æ£€æµ‹åˆ°æ™®é€šç½‘ç«™ï¼Œå¼€å§‹æ·±åº¦å†…å®¹æŒ–æ˜: {url}")
        
        
        # å…ˆè·å–ç½‘é¡µå†…å®¹
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        }
        
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
            
        # æå–çº¯æ–‡æœ¬å†…å®¹
        main_content = soup.get_text(separator='\n', strip=True)
        
        # ğŸ” æ™ºèƒ½æ·±åº¦æŒ–æ˜ï¼šæ ¹æ®ç½‘ç«™ç±»å‹é€‰æ‹©æœ€ä¼˜ç­–ç•¥
        all_content = []
        crawled_subpages = 0
        
        # æ·»åŠ ä¸»é¡µé¢å†…å®¹
        page_title = soup.find('title').get_text() if soup.find('title') else 'Unknown'
        all_content.append({
            "filename": f"{page_title}.txt",
            "content": f"ç½‘ç«™æ ‡é¢˜: {page_title}\nç½‘ç«™URL: {url}\n\n{main_content}",
            "source": "main_page",
            "url": url
        })
        
        # å¦‚æœæ˜¯GitHubé¡¹ç›®ï¼Œä½¿ç”¨ä¸“é—¨çš„GitHubæŒ–æ˜é€»è¾‘
        if 'github.com' in url:
            print("ğŸ” æ£€æµ‹åˆ°GitHubé¡¹ç›®ï¼Œå¼€å§‹æ·±åº¦å†…å®¹æŒ–æ˜...")
            
            # è§£æGitHub URLï¼Œè·å–ç”¨æˆ·åå’Œä»“åº“å
            import re
            github_match = re.match(r'https://github\.com/([^/]+)/([^/]+)', url)
            if github_match:
                username, repo = github_match.groups()
                print(f"ğŸ“Š GitHubé¡¹ç›®: {username}/{repo}")
                
                # è·å–GitHub APIä¿¡æ¯ï¼ˆåŸºæœ¬ä¿¡æ¯ï¼‰
                try:
                    api_url = f"https://api.github.com/repos/{username}/{repo}"
                    api_response = requests.get(api_url, timeout=10)
                    if api_response.status_code == 200:
                        repo_info = api_response.json()
                        info_content = f"""GitHubé¡¹ç›®åŸºæœ¬ä¿¡æ¯:
åç§°: {repo_info.get('name', 'N/A')}
æè¿°: {repo_info.get('description', 'N/A')}
ä¸»è¦è¯­è¨€: {repo_info.get('language', 'N/A')}
Stars: {repo_info.get('stargazers_count', 0)}
Forks: {repo_info.get('forks_count', 0)}
å¼€æºè®¸å¯: {repo_info.get('license', {}).get('name', 'N/A') if repo_info.get('license') else 'N/A'}
åˆ›å»ºæ—¶é—´: {repo_info.get('created_at', 'N/A')}
æœ€åæ›´æ–°: {repo_info.get('updated_at', 'N/A')}
é»˜è®¤åˆ†æ”¯: {repo_info.get('default_branch', 'main')}
"""
                        all_content.append({
                            "filename": "github_info.txt",
                            "content": info_content,
                            "source": "github_api",
                            "url": api_url
                        })
                        print("âœ… æˆåŠŸè·å–GitHubé¡¹ç›®åŸºæœ¬ä¿¡æ¯")
                        
                        # ä½¿ç”¨APIè·å–çš„é»˜è®¤åˆ†æ”¯
                        default_branch = repo_info.get('default_branch', 'main')
                    else:
                        default_branch = 'main'
                except Exception as e:
                    default_branch = 'main'
                    print(f"âš ï¸ æ— æ³•è·å–GitHub APIä¿¡æ¯: {e}")
            
                # æ„é€ READMEçš„raw URL (ä½¿ç”¨æ­£ç¡®çš„åˆ†æ”¯)
                if '/blob/' not in url:
                    readme_urls = [
                        url.replace('github.com', 'raw.githubusercontent.com') + f'/{default_branch}/README.md',
                        url.replace('github.com', 'raw.githubusercontent.com') + '/main/README.md',
                        url.replace('github.com', 'raw.githubusercontent.com') + '/master/README.md',
                        url.replace('github.com', 'raw.githubusercontent.com') + f'/{default_branch}/README.rst',
                        url.replace('github.com', 'raw.githubusercontent.com') + f'/{default_branch}/README.txt'
                    ]
                    
                    for i, readme_url in enumerate(readme_urls):
                        try:
                            print(f"ğŸ” å°è¯•è·å–README: {readme_url}")
                            readme_response = requests.get(readme_url, timeout=10)
                            if readme_response.status_code == 200:
                                # æ ¹æ®URLç¡®å®šæ–‡ä»¶æ‰©å±•å
                                if readme_url.endswith('.md'):
                                    ext = '.md'
                                elif readme_url.endswith('.rst'):
                                    ext = '.rst'
                                else:
                                    ext = '.txt'
                                    
                                all_content.append({
                                    "filename": f"README{ext}",
                                    "content": readme_response.text,
                                    "source": "github_raw",
                                    "url": readme_url
                                })
                                print(f"âœ… æˆåŠŸè·å–READMEæ–‡ä»¶: {readme_url} ({len(readme_response.text)} å­—ç¬¦)")
                                crawled_subpages += 1
                                break
                        except Exception as e:
                            print(f"âš ï¸ READMEè·å–å¤±è´¥: {e}")
                            continue
                
                # å°è¯•è·å–æ›´å¤šå…³é”®æ–‡ä»¶ï¼ˆåŒ…æ‹¬æºä»£ç æ–‡ä»¶ï¼‰
                key_files = [
                    ('package.json', f'/{default_branch}/package.json'),
                    ('requirements.txt', f'/{default_branch}/requirements.txt'),
                    ('setup.py', f'/{default_branch}/setup.py'),
                    ('Cargo.toml', f'/{default_branch}/Cargo.toml'),
                    ('pom.xml', f'/{default_branch}/pom.xml'),
                    ('pyproject.toml', f'/{default_branch}/pyproject.toml'),
                    ('Dockerfile', f'/{default_branch}/Dockerfile'),
                    ('docker-compose.yml', f'/{default_branch}/docker-compose.yml'),
                    # æ–°å¢æºä»£ç æ–‡ä»¶
                    ('main.py', f'/{default_branch}/main.py'),
                    ('app.py', f'/{default_branch}/app.py'),
                    ('index.js', f'/{default_branch}/index.js'),
                    ('src/main.java', f'/{default_branch}/src/main/java/Main.java'),
                    ('src/index.ts', f'/{default_branch}/src/index.ts'),
                    ('lib.rs', f'/{default_branch}/src/lib.rs'),
                    ('main.rs', f'/{default_branch}/src/main.rs')
                ]
                
                for filename, path in key_files[:10]:  # å¢åŠ è·å–å…³é”®æ–‡ä»¶æ•°é‡
                    try:
                        file_url = url.replace('github.com', 'raw.githubusercontent.com') + path
                        print(f"ğŸ” å°è¯•è·å–å…³é”®æ–‡ä»¶: {file_url}")
                        file_response = requests.get(file_url, timeout=10)
                        if file_response.status_code == 200:
                            all_content.append({
                                "filename": filename,
                                "content": file_response.text,
                                "source": "github_raw",
                                "url": file_url
                            })
                            print(f"âœ… æˆåŠŸè·å–å…³é”®æ–‡ä»¶: {filename} ({len(file_response.text)} å­—ç¬¦)")
                            crawled_subpages += 1
                    except Exception as e:
                        print(f"âš ï¸ æ–‡ä»¶è·å–å¤±è´¥ {filename}: {e}")
                        continue
        
        else:
            # ğŸŒ é€šç”¨ç½‘ç«™æ·±åº¦æŒ–æ˜ï¼šæ™ºèƒ½å‘ç°å¹¶æŠ“å–é‡è¦å­é¡µé¢
            print(f"ğŸŒ å¼€å§‹é€šç”¨ç½‘ç«™æ·±åº¦æŒ–æ˜: {url}")
            
            try:
                # 1. æ™ºèƒ½å‘ç°é‡è¦å­é¡µé¢
                important_subpages = discover_important_subpages(url, soup, max_pages=8)
                
                if important_subpages:
                    print(f"âœ¨ å‘ç° {len(important_subpages)} ä¸ªé‡è¦å­é¡µé¢ï¼Œå¼€å§‹æ·±åº¦æŠ“å–...")
                    
                    # 2. æ‰¹é‡æŠ“å–å­é¡µé¢å†…å®¹
                    for i, page_info in enumerate(important_subpages):
                        try:
                            page_url = page_info['url']
                            page_name = f"subpage_{i+1}_{page_info['importance_score']}points"
                            
                            print(f"ğŸ“„ æŠ“å–å­é¡µé¢ {i+1}/{len(important_subpages)}: {page_info['text'][:30]}...")
                            
                            # è·å–å­é¡µé¢å†…å®¹
                            page_response = requests.get(page_url, headers=headers, timeout=15)
                            if page_response.status_code == 200:
                                page_soup = BeautifulSoup(page_response.text, 'html.parser')
                                
                                # ç§»é™¤æ— å…³å…ƒç´ 
                                for element in page_soup(["script", "style", "nav", "footer", "header", "aside"]):
                                    element.decompose()
                                
                                # æå–ä¸»è¦å†…å®¹
                                page_content = ""
                                
                                # å°è¯•æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸ
                                content_selectors = [
                                    'main', 'article', '.content', '.main-content', 
                                    '.post-content', '.entry-content', '#content', 
                                    '.page-content', '.article-content'
                                ]
                                
                                main_element = None
                                for selector in content_selectors:
                                    main_element = page_soup.select_one(selector)
                                    if main_element:
                                        page_content = main_element.get_text(separator='\n', strip=True)
                                        break
                                
                                # å¦‚æœæ²¡æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œä½¿ç”¨å…¨éƒ¨æ–‡æœ¬
                                if not page_content:
                                    page_content = page_soup.get_text(separator='\n', strip=True)
                                
                                # é™åˆ¶å†…å®¹é•¿åº¦ä»¥é¿å…è¿‡å¤§æ–‡ä»¶
                                if len(page_content) > 15000:
                                    page_content = page_content[:15000] + "..."
                                
                                # ä¿å­˜å­é¡µé¢å†…å®¹
                                if page_content.strip():
                                    full_content = f"""å­é¡µé¢æ ‡é¢˜: {page_soup.find('title').get_text() if page_soup.find('title') else 'æœªçŸ¥'}
å­é¡µé¢URL: {page_url}
é‡è¦æ€§è¯„åˆ†: {page_info['importance_score']}
åŒ¹é…å…³é”®è¯: {', '.join(page_info['matched_keywords'])}
é“¾æ¥æ–‡æœ¬: {page_info['text']}

=== é¡µé¢å†…å®¹ ===
{page_content}"""
                                    
                                    all_content.append({
                                        "filename": f"{page_name}.txt",
                                        "content": full_content,
                                        "source": "subpage",
                                        "url": page_url
                                    })
                                    crawled_subpages += 1
                                    print(f"âœ… å­é¡µé¢ {i+1} å†…å®¹å·²ä¿å­˜ ({len(page_content)} å­—ç¬¦)")
                                else:
                                    print(f"âš ï¸ å­é¡µé¢ {i+1} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")
                            else:
                                print(f"âš ï¸ å­é¡µé¢ {i+1} è®¿é—®å¤±è´¥: HTTP {page_response.status_code}")
                                
                        except Exception as page_error:
                            print(f"âŒ æŠ“å–å­é¡µé¢ {i+1} å¤±è´¥: {page_error}")
                            continue
                    
                    print(f"ğŸ¯ é€šç”¨æ·±åº¦æŒ–æ˜å®Œæˆ: æˆåŠŸæŠ“å– {crawled_subpages}/{len(important_subpages)} ä¸ªå­é¡µé¢")
                else:
                    print("â„¹ï¸ æœªå‘ç°é‡è¦å­é¡µé¢ï¼Œä»…å¤„ç†ä¸»é¡µé¢å†…å®¹")
                    
            except Exception as subpage_error:
                print(f"âš ï¸ å­é¡µé¢æŒ–æ˜å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸»é¡µé¢: {subpage_error}")
                crawled_subpages = 0
        
        # å†…å®¹å»é‡å¤„ç†
        def deduplicate_content(all_content):
            """å»é™¤é‡å¤å†…å®¹ï¼Œé¿å…é‡å¤æå–ç›¸åŒä¿¡æ¯"""
            seen_content = set()
            unique_content = []
            
            for content_item in all_content:
                # åˆ›å»ºå†…å®¹çš„å“ˆå¸Œæ ‡è¯†ç¬¦ï¼ˆå–å‰500å­—ç¬¦ä½œä¸ºå»é‡ä¾æ®ï¼‰
                content_hash = hash(content_item['content'][:500].strip())
                
                if content_hash not in seen_content:
                    seen_content.add(content_hash)
                    unique_content.append(content_item)
                else:
                    print(f"ğŸ”„ æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼Œå·²è·³è¿‡: {content_item['filename']}")
                    
            print(f"ğŸ“Š å»é‡å®Œæˆ: {len(all_content)} â†’ {len(unique_content)} ä¸ªå†…å®¹é¡¹")
            return unique_content
        
        # å¯¹å†…å®¹è¿›è¡Œå»é‡å¤„ç†
        all_content = deduplicate_content(all_content)
        
        # åˆå¹¶æ‰€æœ‰æå–çš„å†…å®¹ - ğŸ”¥ ç§»é™¤å†…å®¹æˆªæ–­ï¼Œä¿ç•™å®Œæ•´å†…å®¹
        combined_content = f"URL: {url}\n\n"
        
        for content_item in all_content:
            combined_content += f"=== {content_item['filename']} ===\n"
            # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸å†æˆªæ–­å†…å®¹ï¼Œä¿ç•™å®Œæ•´çš„æå–ç»“æœ
            combined_content += content_item['content']
            combined_content += "\n\n"
        
        # ğŸ¯ ç¡®å®šä½¿ç”¨çš„å†…å®¹æå–æ–¹æ³•
        if 'github.com' in url:
            extraction_method = "GitHubä¸“é¡¹æ·±åº¦æŒ–æ˜"
            extraction_type = "github_specialized"
        else:
            extraction_method = f"é€šç”¨ç½‘ç«™æ·±åº¦æŒ–æ˜ (æŠ“å–äº†{crawled_subpages}ä¸ªå­é¡µé¢)"
            extraction_type = "universal_deep_crawling"
        
        print(f"ğŸ¯ {extraction_method}å®Œæˆï¼Œæ€»é•¿åº¦: {len(combined_content)} å­—ç¬¦")
        
        # ğŸ¤– é›†æˆAIåˆ†æå’ŒçŸ¥è¯†å›¾è°±æ›´æ–°
        print(f"ğŸ¤– å¼€å§‹AIåˆ†æå’ŒçŸ¥è¯†å›¾è°±é›†æˆ...")
        
        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ–‡ä»¶åç”¨äºçŸ¥è¯†å›¾è°±
        virtual_filename = f"scraped_{extraction_type}_{url.split('/')[-1] or 'website'}.txt"
        
        # ğŸ¤– ä½¿ç”¨å®‰å…¨çš„AIåˆ†ææ–¹æ³•
        ai_analysis = {}
        if combined_content and len(combined_content) > 50:  # ç¡®ä¿æœ‰è¶³å¤Ÿå†…å®¹åˆ†æ
            try:
                ai_analysis = await safe_analyze_with_openai(combined_content, virtual_filename)
                print(f"âœ… AIåˆ†æå®Œæˆ: {len(ai_analysis.get('entities', []))}ä¸ªå®ä½“, {len(ai_analysis.get('concepts', []))}ä¸ªæ¦‚å¿µ")
            except Exception as ai_error:
                print(f"âŒ AIåˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ: {ai_error}")
                ai_analysis = create_basic_analysis(combined_content, virtual_filename)
        else:
            ai_analysis = create_basic_analysis(combined_content, virtual_filename)
        
        # ğŸ”§ ä¿®å¤æ•°æ®åŒæ­¥é—®é¢˜ - ç¡®ä¿æ‰€æœ‰å…³ç³»èŠ‚ç‚¹éƒ½å­˜åœ¨äºå®ä½“æˆ–æ¦‚å¿µä¸­
        synchronized_data = synchronize_graph_data(ai_analysis)
        
        # ğŸ” æ·»åŠ å†…å®¹è´¨é‡è¯„ä¼°
        content_quality_metrics = {}
        if combined_content:
            try:
                print(f"ğŸ” å¼€å§‹å†…å®¹è´¨é‡åˆ†æ: {len(combined_content)} å­—ç¬¦")
                content_quality_metrics = analyze_content_quality(combined_content, ".txt")
                print(f"âœ… å†…å®¹è´¨é‡åˆ†æå®Œæˆ")
            except Exception as quality_error:
                print(f"âŒ Content quality analysis failed: {quality_error}")
                content_quality_metrics = {
                    'completeness_score': 0.7,
                    'readability_score': 0.8,
                    'information_density': 0.6,
                    'structure_integrity': 0.7,
                    'overall_score': 0.7
                }
        
        # ğŸ¯ æ·»åŠ æå–å‡†ç¡®æ€§éªŒè¯
        extraction_validation = {}
        if combined_content and synchronized_data:
            try:
                print(f"ğŸ¯ å¼€å§‹æå–å‡†ç¡®æ€§éªŒè¯")
                extraction_validation = validate_extraction_accuracy(synchronized_data, combined_content, virtual_filename)
                print(f"âœ… æå–å‡†ç¡®æ€§éªŒè¯å®Œæˆ")
            except Exception as extraction_error:
                print(f"âŒ Extraction validation failed: {extraction_error}")
                extraction_validation = {
                    "accuracy_score": 0.7,
                    "validation_checks": {},
                    "warnings": ["éªŒè¯è¿‡ç¨‹å‡ºé”™"],
                    "recommendations": ["å»ºè®®äººå·¥å®¡æ ¸"]
                }
        
        # ğŸ¯ ç”Ÿæˆæ–‡æ¡£ç›®å½•å’Œå†…å®¹ç»“æ„
        document_structure = generate_document_structure(combined_content, virtual_filename)
        
        # ğŸ”¥ å°è¯•æ›´æ–°çŸ¥è¯†å›¾è°±ï¼ˆä½¿ç”¨ä¸æ–‡ä»¶åˆ†æç›¸åŒçš„é€»è¾‘ï¼‰
        graph_update_result = {
            "status": "safe_mode",
            "message": "å›¾è°±æ›´æ–°å·²ç¦ç”¨ä»¥é¿å…æ®µé”™è¯¯",
            "updates": {"document_nodes": 1, "entity_nodes": 0, "relationships": 0}
        }
        
        print(f"âœ… ç½‘ç«™/è§†é¢‘å†…å®¹å·²æˆåŠŸé›†æˆåˆ°çŸ¥è¯†å›¾è°±ç³»ç»Ÿ")
        
        return {
            "status": "success",
            "url": url,
            "extraction_method": extraction_method,
            "extraction_type": extraction_type,
            "files_processed": len(all_content),
            "deep_crawling_stats": {
                "subpages_crawled": crawled_subpages,
                "is_github": 'github.com' in url,
                "universal_crawling_enabled": crawled_subpages > 0 if 'github.com' not in url else False
            },
            # ğŸ¯ æ–°å¢å®Œæ•´çš„åˆ†æç»“æœ - ä¸æ–‡ä»¶åˆ†æç«¯ç‚¹ä¿æŒä¸€è‡´
            "analysis": {
                "content": combined_content,  # ä½¿ç”¨å®Œæ•´çš„æå–å†…å®¹
                "ai_analysis_summary": synchronized_data.get("content", "AIåˆ†æå®Œæˆ"),
                "concepts": synchronized_data.get("concepts", []),
                "entities": synchronized_data.get("entities", []),
                "relationships": synchronized_data.get("relationships", []),
                "knowledge_tree": synchronized_data.get("knowledge_tree", {}),
                "knowledgeTreeSuggestion": synchronized_data.get("knowledgeTreeSuggestion", "ç½‘ç«™å†…å®¹/AIåˆ†æ"),
                "confidence": synchronized_data.get("confidence", 0.75),
                "extraction_depth": {
                    "relationship_count": len(synchronized_data.get("relationships", [])),
                    "entity_count": len(synchronized_data.get("entities", [])),
                    "concept_count": len(synchronized_data.get("concepts", [])),
                    "has_knowledge_tree": bool(synchronized_data.get("knowledge_tree")),
                    "semantic_layers": len(synchronized_data.get("knowledge_tree", {}).get("semantic_clusters", [])),
                    "domain_identified": bool(synchronized_data.get("knowledge_tree", {}).get("domain")),
                    "theme_count": len(synchronized_data.get("knowledge_tree", {}).get("themes", []))
                },
                "content_quality": {
                    **content_quality_metrics,
                    "quality_grade": get_quality_grade(content_quality_metrics.get('overall_score', 0)) if content_quality_metrics else "è‰¯å¥½ (B)",
                    "recommendations": generate_quality_recommendations(content_quality_metrics) if content_quality_metrics else ["ç½‘ç«™å†…å®¹è´¨é‡è‰¯å¥½"]
                },
                "extraction_validation": extraction_validation,
                "fileInfo": {
                    "filename": virtual_filename,
                    "source_url": url,
                    "type": "scraped_content",
                    "textLength": len(combined_content),
                    "extraction_completeness": content_quality_metrics.get("completeness_score", 0.7),
                    "content_readability": content_quality_metrics.get("readability_score", 0.8)
                },
                "graph_update": graph_update_result,
                "debug_version": "2025-09-12-scrape-integration",  # ç½‘ç«™æŠ“å–é›†æˆç‰ˆæœ¬
                # ğŸ¯ æ–‡æ¡£ç»“æ„å’Œå†…å®¹
                "document": {
                    "raw_content": combined_content[:15000] + ("..." if len(combined_content) > 15000 else ""),  # å¢åŠ åŸå§‹å†…å®¹é•¿åº¦é™åˆ¶
                    "full_content": combined_content,  # å®Œæ•´å†…å®¹
                    "structure": document_structure,
                    "directory": document_structure.get("directory", []),
                    "sections": document_structure.get("sections", []),
                    "summary": document_structure.get("summary", ""),
                    "word_count": len(combined_content.split()) if combined_content else 0,
                    "char_count": len(combined_content) if combined_content else 0
                }
            },
            "service_ready": True
        }
        
    except Exception as e:
        print(f"âŒ GraphRAGå†…å®¹æå–å¤±è´¥: {str(e)}")
        import traceback
        print(f"âŒ è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {
            "status": "error",
            "message": f"GraphRAGå†…å®¹æå–å¤±è´¥: {str(e)}",
            "url": request.get("url", "unknown") if "request" in locals() else "unknown"
        }

async def update_knowledge_graph_with_analysis(ai_analysis: dict, filename: str, text_content: str) -> dict:
    """åŠ¨æ€å›¾è°±æ›´æ–°ï¼šå°†AIåˆ†æç»“æœå†™å…¥çŸ¥è¯†å›¾è°±ï¼ˆå†…å­˜+Neo4jæŒä¹…åŒ–ï¼‰"""
    try:
        import hashlib
        import time
        
        start_time = time.time()
        results = {
            "memory_graph": {"status": "not_attempted"},
            "neo4j_graph": {"status": "not_attempted"}
        }
        
        # 1. å°è¯•å†™å…¥å†…å­˜å›¾è°±
        try:
            from safe_memory_graph import get_safe_memory_graph_db
            memory_db = get_safe_memory_graph_db()
            
            # åˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
            doc_id = hashlib.md5(filename.encode()).hexdigest()[:8]
            doc_properties = {
                "label": "Document",
                "filename": filename,
                "content_length": len(text_content),
                "analysis_timestamp": time.time(),
                "confidence": ai_analysis.get("confidence", 0.8)
            }
            memory_db.create_node(f"doc_{doc_id}", doc_properties)
            
            # å¤„ç†å®ä½“
            entities = ai_analysis.get("entities", [])
            entity_count = 0
            for entity in entities:
                if isinstance(entity, str):
                    entity_id = f"entity_{hashlib.md5(entity.encode()).hexdigest()[:8]}"
                    entity_properties = {
                        "label": "Entity", 
                        "name": entity,
                        "type": "extracted",
                        "source_document": filename
                    }
                    memory_db.create_node(entity_id, entity_properties)
                    
                    # åˆ›å»ºæ–‡æ¡£åˆ°å®ä½“çš„å…³ç³»
                    memory_db.create_relationship(
                        f"doc_{doc_id}", 
                        entity_id, 
                        "MENTIONS", 
                        {"confidence": 0.9}
                    )
                    entity_count += 1
            
            # å¤„ç†æ¦‚å¿µ
            concepts = ai_analysis.get("concepts", [])
            concept_count = 0
            for concept in concepts:
                if isinstance(concept, str):
                    concept_id = f"concept_{hashlib.md5(concept.encode()).hexdigest()[:8]}"
                    concept_properties = {
                        "label": "Concept",
                        "name": concept, 
                        "type": "extracted",
                        "source_document": filename
                    }
                    memory_db.create_node(concept_id, concept_properties)
                    
                    # åˆ›å»ºæ–‡æ¡£åˆ°æ¦‚å¿µçš„å…³ç³»
                    memory_db.create_relationship(
                        f"doc_{doc_id}",
                        concept_id,
                        "DISCUSSES",
                        {"confidence": 0.8}
                    )
                    concept_count += 1
            
            # å¤„ç†æ·±åº¦è¯­ä¹‰å…³ç³»
            relationships = ai_analysis.get("relationships", [])
            relationship_count = 0
            for rel in relationships:
                if isinstance(rel, dict):
                    source = rel.get("source", "")
                    target = rel.get("target", "")
                    rel_type = rel.get("type", "RELATED")
                    description = rel.get("description", "")
                    strength = rel.get("strength", 0.8)
                    semantic_type = rel.get("semantic_type", "general")
                    
                    if source and target:
                        # ä¸ºå…³ç³»çš„æºå’Œç›®æ ‡åˆ›å»ºèŠ‚ç‚¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                        source_id = f"entity_{hashlib.md5(source.encode()).hexdigest()[:8]}"
                        target_id = f"entity_{hashlib.md5(target.encode()).hexdigest()[:8]}"
                        
                        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
                        if not memory_db.get_node(source_id):
                            memory_db.create_node(source_id, {
                                "label": "Entity",
                                "name": source,
                                "type": "relationship_derived",
                                "source_document": filename,
                                "semantic_type": semantic_type
                            })
                        
                        if not memory_db.get_node(target_id):
                            memory_db.create_node(target_id, {
                                "label": "Entity", 
                                "name": target,
                                "type": "relationship_derived",
                                "source_document": filename,
                                "semantic_type": semantic_type
                            })
                        
                        # åˆ›å»ºå¢å¼ºå…³ç³»
                        memory_db.create_relationship(
                            source_id,
                            target_id, 
                            rel_type.upper(),
                            {
                                "description": description,
                                "source_document": filename,
                                "confidence": 0.85,
                                "strength": strength,
                                "semantic_type": semantic_type,
                                "relation_layer": "deep_semantic"
                            }
                        )
                        relationship_count += 1
            
            # å¤„ç†çŸ¥è¯†æ ‘å±‚æ¬¡ç»“æ„
            knowledge_tree = ai_analysis.get("knowledge_tree", {})
            if knowledge_tree:
                # åˆ›å»ºé¢†åŸŸèŠ‚ç‚¹
                domain = knowledge_tree.get("domain", "")
                if domain:
                    domain_id = f"domain_{hashlib.md5(domain.encode()).hexdigest()[:8]}"
                    memory_db.create_node(domain_id, {
                        "label": "Domain",
                        "name": domain,
                        "type": "knowledge_domain",
                        "source_document": filename,
                        "layer": "domain"
                    })
                    
                    # è¿æ¥æ–‡æ¡£åˆ°é¢†åŸŸ
                    memory_db.create_relationship(
                        f"doc_{doc_id}",
                        domain_id,
                        "BELONGS_TO_DOMAIN",
                        {"confidence": 0.9, "layer": "domain"}
                    )
                
                # åˆ›å»ºä¸»é¢˜èŠ‚ç‚¹
                themes = knowledge_tree.get("themes", [])
                for theme in themes:
                    if isinstance(theme, str):
                        theme_id = f"theme_{hashlib.md5(theme.encode()).hexdigest()[:8]}"
                        memory_db.create_node(theme_id, {
                            "label": "Theme",
                            "name": theme,
                            "type": "knowledge_theme",
                            "source_document": filename,
                            "layer": "theme"
                        })
                        
                        # è¿æ¥ä¸»é¢˜åˆ°é¢†åŸŸ
                        if domain:
                            memory_db.create_relationship(
                                domain_id,
                                theme_id,
                                "CONTAINS_THEME",
                                {"confidence": 0.8, "layer": "hierarchy"}
                            )
                
                # å¤„ç†å®ä½“å±‚æ¬¡ç»“æ„
                entity_hierarchy = knowledge_tree.get("entity_hierarchy", {})
                for layer_name, entity_groups in entity_hierarchy.items():
                    layer_id = f"layer_{hashlib.md5(layer_name.encode()).hexdigest()[:8]}"
                    memory_db.create_node(layer_id, {
                        "label": "Layer",
                        "name": layer_name,
                        "type": "hierarchy_layer",
                        "source_document": filename,
                        "layer": "structure"
                    })
                    
                    # å¤„ç†å®ä½“ç»„
                    if isinstance(entity_groups, list):
                        for entity_group in entity_groups:
                            if isinstance(entity_group, str):
                                group_id = f"group_{hashlib.md5(entity_group.encode()).hexdigest()[:8]}"
                                memory_db.create_node(group_id, {
                                    "label": "EntityGroup",
                                    "name": entity_group,
                                    "type": "entity_cluster",
                                    "source_document": filename,
                                    "parent_layer": layer_name
                                })
                                
                                memory_db.create_relationship(
                                    layer_id,
                                    group_id,
                                    "CONTAINS_GROUP",
                                    {"confidence": 0.85, "layer": "hierarchy"}
                                )
                
                # å¤„ç†è¯­ä¹‰èšç±»
                semantic_clusters = knowledge_tree.get("semantic_clusters", [])
                for i, cluster in enumerate(semantic_clusters):
                    if isinstance(cluster, list):
                        cluster_id = f"cluster_{i}_{hashlib.md5(str(cluster).encode()).hexdigest()[:8]}"
                        memory_db.create_node(cluster_id, {
                            "label": "SemanticCluster",
                            "name": f"è¯­ä¹‰èšç±»_{i+1}",
                            "concepts": cluster,
                            "type": "semantic_cluster",
                            "source_document": filename,
                            "cluster_size": len(cluster)
                        })
                        
                        # è¿æ¥èšç±»åˆ°æ–‡æ¡£
                        memory_db.create_relationship(
                            f"doc_{doc_id}",
                            cluster_id,
                            "HAS_SEMANTIC_CLUSTER",
                            {"confidence": 0.8, "cluster_index": i}
                        )
            
            # è·å–æ›´æ–°åçš„å›¾ç»Ÿè®¡
            memory_stats = memory_db.get_stats()
            results["memory_graph"] = {
                "status": "success",
                "updates": {
                    "document_nodes": 1,
                    "entity_nodes": entity_count,
                    "concept_nodes": concept_count,
                    "relationships": relationship_count
                },
                "stats": memory_stats
            }
            print(f"âœ… å†…å­˜å›¾è°±æ›´æ–°æˆåŠŸ: å®ä½“{entity_count}, æ¦‚å¿µ{concept_count}, å…³ç³»{relationship_count}")
            
        except Exception as memory_error:
            print(f"âš ï¸ å†…å­˜å›¾è°±æ›´æ–°å¤±è´¥: {memory_error}")
            results["memory_graph"] = {"status": "error", "message": str(memory_error)}
        
        # 2. å°è¯•å†™å…¥Neo4jæŒä¹…åŒ–å­˜å‚¨
        try:
            from config.neo4jdb import get_db_manager
            
            # æ£€æŸ¥Neo4jç¯å¢ƒå˜é‡æ˜¯å¦é…ç½®
            neo4j_uri = os.getenv('NEO4J_URI')
            neo4j_username = os.getenv('NEO4J_USERNAME') 
            neo4j_password = os.getenv('NEO4J_PASSWORD')
            
            if not all([neo4j_uri, neo4j_username, neo4j_password]):
                print("â„¹ï¸ Neo4jç¯å¢ƒå˜é‡æœªå®Œå…¨é…ç½®ï¼Œè·³è¿‡Neo4jæŒä¹…åŒ–")
                results["neo4j_graph"] = {
                    "status": "skipped", 
                    "message": "Neo4jç¯å¢ƒå˜é‡æœªé…ç½®"
                }
            else:
                db_manager = get_db_manager()
                neo4j_entity_count = 0
                neo4j_relationship_count = 0
                
                # åˆ›å»ºæ–‡æ¡£èŠ‚ç‚¹
                doc_cypher = """
                MERGE (d:Document {id: $doc_id})
                SET d.filename = $filename,
                    d.content_length = $content_length,
                    d.analysis_timestamp = $timestamp,
                    d.confidence = $confidence
                """
                db_manager.execute_query(doc_cypher, {
                    "doc_id": f"doc_{doc_id}",
                    "filename": filename,
                    "content_length": len(text_content),
                    "timestamp": time.time(),
                    "confidence": ai_analysis.get("confidence", 0.8)
                })
                
                # æ‰¹é‡åˆ›å»ºå®ä½“èŠ‚ç‚¹
                if entities:
                    entity_cypher = """
                    UNWIND $entities AS entity
                    MERGE (e:Entity {id: entity.id})
                    SET e.name = entity.name,
                        e.type = entity.type,
                        e.source_document = entity.source_document
                    """
                    entity_data = []
                    for entity in entities:
                        if isinstance(entity, str):
                            entity_data.append({
                                "id": f"entity_{hashlib.md5(entity.encode()).hexdigest()[:8]}",
                                "name": entity,
                                "type": "extracted",
                                "source_document": filename
                            })
                    
                    if entity_data:
                        db_manager.execute_query(entity_cypher, {"entities": entity_data})
                        neo4j_entity_count = len(entity_data)
                
                # æ‰¹é‡åˆ›å»ºæ¦‚å¿µèŠ‚ç‚¹
                if concepts:
                    concept_cypher = """
                    UNWIND $concepts AS concept
                    MERGE (c:Concept {id: concept.id})
                    SET c.name = concept.name,
                        c.type = concept.type,
                        c.source_document = concept.source_document
                    """
                    concept_data = []
                    for concept in concepts:
                        if isinstance(concept, str):
                            concept_data.append({
                                "id": f"concept_{hashlib.md5(concept.encode()).hexdigest()[:8]}",
                                "name": concept,
                                "type": "extracted", 
                                "source_document": filename
                            })
                    
                    if concept_data:
                        db_manager.execute_query(concept_cypher, {"concepts": concept_data})
                        neo4j_entity_count += len(concept_data)
                
                # åˆ›å»ºæ–‡æ¡£å…³ç³»
                doc_rel_cypher = """
                MATCH (d:Document {id: $doc_id})
                MATCH (e:Entity {source_document: $filename})
                MERGE (d)-[r:MENTIONS]->(e)
                SET r.confidence = 0.9
                
                WITH d
                MATCH (c:Concept {source_document: $filename})
                MERGE (d)-[r2:DISCUSSES]->(c) 
                SET r2.confidence = 0.8
                """
                db_manager.execute_query(doc_rel_cypher, {
                    "doc_id": f"doc_{doc_id}",
                    "filename": filename
                })
                
                # æ‰¹é‡åˆ›å»ºæ·±åº¦è¯­ä¹‰å…³ç³»
                if relationships:
                    rel_data = []
                    for rel in relationships:
                        if isinstance(rel, dict):
                            source = rel.get("source", "")
                            target = rel.get("target", "")
                            rel_type = rel.get("type", "RELATED")
                            description = rel.get("description", "")
                            strength = rel.get("strength", 0.8)
                            semantic_type = rel.get("semantic_type", "general")
                            
                            if source and target:
                                rel_data.append({
                                    "source_id": f"entity_{hashlib.md5(source.encode()).hexdigest()[:8]}",
                                    "target_id": f"entity_{hashlib.md5(target.encode()).hexdigest()[:8]}",
                                    "source_name": source,
                                    "target_name": target,
                                    "rel_type": rel_type.upper().replace(" ", "_"),
                                    "description": description,
                                    "source_document": filename,
                                    "confidence": 0.85,
                                    "strength": strength,
                                    "semantic_type": semantic_type
                                })
                    
                    if rel_data:
                        # åˆ›å»ºæ·±åº¦è¯­ä¹‰å…³ç³»
                        for rel_item in rel_data:
                            individual_cypher = f"""
                            MERGE (source:Entity {{id: $source_id}})
                            ON CREATE SET source.name = $source_name,
                                          source.type = 'relationship_derived',
                                          source.source_document = $source_document,
                                          source.semantic_type = $semantic_type
                            MERGE (target:Entity {{id: $target_id}})
                            ON CREATE SET target.name = $target_name,
                                          target.type = 'relationship_derived',
                                          target.source_document = $source_document,
                                          target.semantic_type = $semantic_type
                            MERGE (source)-[r:{rel_item['rel_type']}]->(target)
                            SET r.description = $description,
                                r.source_document = $source_document,
                                r.confidence = $confidence,
                                r.strength = $strength,
                                r.semantic_type = $semantic_type,
                                r.relation_layer = 'deep_semantic'
                            """
                            db_manager.execute_query(individual_cypher, rel_item)
                            neo4j_relationship_count += 1
                
                # åˆ›å»ºçŸ¥è¯†æ ‘ç»“æ„
                knowledge_tree = ai_analysis.get("knowledge_tree", {})
                if knowledge_tree:
                    # åˆ›å»ºé¢†åŸŸèŠ‚ç‚¹
                    domain = knowledge_tree.get("domain", "")
                    if domain:
                        domain_cypher = """
                        MERGE (d:Domain {id: $domain_id})
                        SET d.name = $domain_name,
                            d.type = 'knowledge_domain',
                            d.source_document = $source_document,
                            d.layer = 'domain'
                        
                        WITH d
                        MATCH (doc:Document {id: $doc_id})
                        MERGE (doc)-[r:BELONGS_TO_DOMAIN]->(d)
                        SET r.confidence = 0.9, r.layer = 'domain'
                        """
                        db_manager.execute_query(domain_cypher, {
                            "domain_id": f"domain_{hashlib.md5(domain.encode()).hexdigest()[:8]}",
                            "domain_name": domain,
                            "source_document": filename,
                            "doc_id": f"doc_{doc_id}"
                        })
                    
                    # åˆ›å»ºä¸»é¢˜èŠ‚ç‚¹
                    themes = knowledge_tree.get("themes", [])
                    if themes:
                        theme_cypher = """
                        UNWIND $themes AS theme
                        MERGE (t:Theme {id: theme.id})
                        SET t.name = theme.name,
                            t.type = 'knowledge_theme',
                            t.source_document = theme.source_document,
                            t.layer = 'theme'
                        """
                        theme_data = []
                        for theme in themes:
                            if isinstance(theme, str):
                                theme_data.append({
                                    "id": f"theme_{hashlib.md5(theme.encode()).hexdigest()[:8]}",
                                    "name": theme,
                                    "source_document": filename
                                })
                        
                        if theme_data:
                            db_manager.execute_query(theme_cypher, {"themes": theme_data})
                            
                            # è¿æ¥ä¸»é¢˜åˆ°é¢†åŸŸ
                            if domain:
                                theme_rel_cypher = """
                                MATCH (d:Domain {id: $domain_id})
                                MATCH (t:Theme {source_document: $source_document})
                                MERGE (d)-[r:CONTAINS_THEME]->(t)
                                SET r.confidence = 0.8, r.layer = 'hierarchy'
                                """
                                db_manager.execute_query(theme_rel_cypher, {
                                    "domain_id": f"domain_{hashlib.md5(domain.encode()).hexdigest()[:8]}",
                                    "source_document": filename
                                })
                    
                    # åˆ›å»ºè¯­ä¹‰èšç±»èŠ‚ç‚¹
                    semantic_clusters = knowledge_tree.get("semantic_clusters", [])
                    if semantic_clusters:
                        cluster_cypher = """
                        UNWIND $clusters AS cluster
                        MERGE (c:SemanticCluster {id: cluster.id})
                        SET c.name = cluster.name,
                            c.concepts = cluster.concepts,
                            c.type = 'semantic_cluster',
                            c.source_document = cluster.source_document,
                            c.cluster_size = cluster.cluster_size
                        
                        WITH c
                        MATCH (doc:Document {id: $doc_id})
                        MERGE (doc)-[r:HAS_SEMANTIC_CLUSTER]->(c)
                        SET r.confidence = 0.8, r.cluster_index = cluster.cluster_index
                        """
                        cluster_data = []
                        for i, cluster in enumerate(semantic_clusters):
                            if isinstance(cluster, list):
                                cluster_data.append({
                                    "id": f"cluster_{i}_{hashlib.md5(str(cluster).encode()).hexdigest()[:8]}",
                                    "name": f"è¯­ä¹‰èšç±»_{i+1}",
                                    "concepts": cluster,
                                    "source_document": filename,
                                    "cluster_size": len(cluster),
                                    "cluster_index": i
                                })
                        
                        if cluster_data:
                            db_manager.execute_query(cluster_cypher, {
                                "clusters": cluster_data,
                                "doc_id": f"doc_{doc_id}"
                            })
                
                results["neo4j_graph"] = {
                    "status": "success",
                    "updates": {
                        "document_nodes": 1,
                        "entity_nodes": neo4j_entity_count,
                        "relationships": neo4j_relationship_count
                    }
                }
                print(f"âœ… Neo4jæŒä¹…åŒ–æˆåŠŸ: å®ä½“{neo4j_entity_count}, å…³ç³»{neo4j_relationship_count}")
                
        except Exception as neo4j_error:
            print(f"âš ï¸ Neo4jæŒä¹…åŒ–å¤±è´¥: {neo4j_error}")
            results["neo4j_graph"] = {"status": "error", "message": str(neo4j_error)}
        
        processing_time = time.time() - start_time
        
        # æ±‡æ€»ç»“æœ
        total_entities = 0
        total_relationships = 0
        storage_types = []
        
        if results["memory_graph"]["status"] == "success":
            memory_updates = results["memory_graph"]["updates"]
            total_entities += memory_updates.get("entity_nodes", 0) + memory_updates.get("concept_nodes", 0)
            total_relationships += memory_updates.get("relationships", 0)
            storage_types.append("memory")
        
        if results["neo4j_graph"]["status"] == "success":
            neo4j_updates = results["neo4j_graph"]["updates"]
            total_entities = max(total_entities, neo4j_updates.get("entity_nodes", 0))
            total_relationships = max(total_relationships, neo4j_updates.get("relationships", 0))
            storage_types.append("neo4j")
        
        print(f"ğŸ¯ åŠ¨æ€å›¾è°±æ›´æ–°æ±‡æ€»:")
        print(f"  - å¤„ç†æ—¶é—´: {processing_time:.3f}ç§’")
        print(f"  - å­˜å‚¨ç±»å‹: {', '.join(storage_types) if storage_types else 'none'}")
        print(f"  - å†…å­˜å›¾è°±: {results['memory_graph']['status']}")
        print(f"  - Neo4jæŒä¹…åŒ–: {results['neo4j_graph']['status']}")
        
        return {
            "status": "success" if storage_types else "partial_failure",
            "updates": {
                "document_nodes": 1,
                "entity_nodes": total_entities,
                "concept_nodes": results["memory_graph"].get("updates", {}).get("concept_nodes", 0),
                "relationships": total_relationships,
                "processing_time": processing_time
            },
            "storage_results": results,
            "storage_types": storage_types
        }
        
    except Exception as e:
        print(f"âŒ åŠ¨æ€å›¾è°±æ›´æ–°å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {
            "status": "error",
            "message": str(e),
            "updates": {
                "document_nodes": 0,
                "entity_nodes": 0,
                "concept_nodes": 0,
                "relationships": 0
            }
        }

async def simplified_multi_hop_reasoning(query: str, entities: list, max_steps: int = 3) -> dict:
    """ç®€åŒ–ç‰ˆå¤šè·³æ¨ç†å®ç° - æ— éœ€å¤–éƒ¨ä¾èµ–"""
    try:
        import time
        
        start_time = time.time()
        print(f"ğŸ” å¼€å§‹ç®€åŒ–ç‰ˆå¤šè·³æ¨ç†: {query}")
        print(f"ğŸ“ èµ·å§‹å®ä½“: {entities}")
        
        # åˆå§‹åŒ–æ¢ç´¢çŠ¶æ€
        visited_entities = set(entities)
        current_entities = entities.copy()
        exploration_path = []
        discovered_relationships = []
        relevant_content = []
        
        # æ¨¡æ‹Ÿå®ä½“å…³ç³»ç½‘ç»œ - åœ¨å®é™…ç¯å¢ƒä¸­è¿™å°†æ¥è‡ªçŸ¥è¯†å›¾è°±
        mock_relationships = {
            "æ™ºèƒ½å†…å®¹åˆ›ä½œå·¥ä½œæµç³»ç»Ÿ": ["å†…å®¹ç”Ÿæˆæ¨¡å—", "å·¥ä½œæµå¼•æ“", "ç”¨æˆ·äº¤äº’ç•Œé¢"],
            "æŠ€æœ¯æ¶æ„": ["å¾®æœåŠ¡æ¶æ„", "æ•°æ®å±‚", "ä¸šåŠ¡é€»è¾‘å±‚", "å‰ç«¯å±•ç¤ºå±‚"],
            "å¤šæ¨¡æ€å†…å®¹ç”Ÿäº§": ["æ–‡æœ¬ç”Ÿæˆ", "å›¾åƒå¤„ç†", "éŸ³é¢‘å¤„ç†", "è§†é¢‘ç¼–è¾‘"],
            "è¾“å…¥æ¨¡å—": ["æ–‡ä»¶ä¸Šä¼ ", "æ•°æ®éªŒè¯", "æ ¼å¼è½¬æ¢"],
            "å‘å¸ƒç®¡ç†æ¨¡å—": ["å†…å®¹å®¡æ ¸", "å‘å¸ƒè°ƒåº¦", "ç‰ˆæœ¬ç®¡ç†"],
            "å†…å®¹ç”Ÿæˆæ¨¡å—": ["AIæ¨¡å‹", "æ¨¡æ¿å¼•æ“", "å†…å®¹ä¼˜åŒ–"],
            "å·¥ä½œæµå¼•æ“": ["ä»»åŠ¡è°ƒåº¦", "çŠ¶æ€ç®¡ç†", "å¼‚å¸¸å¤„ç†"],
            "å¾®æœåŠ¡æ¶æ„": ["APIç½‘å…³", "æœåŠ¡å‘ç°", "è´Ÿè½½å‡è¡¡"],
            "æµ‹è¯•å®ä½“1": ["ç›¸å…³å®ä½“A", "ç›¸å…³å®ä½“B"],
            "æµ‹è¯•å®ä½“2": ["ç›¸å…³å®ä½“C", "ç›¸å…³å®ä½“D"],
        }
        
        # æ·»åŠ èµ·å§‹å®ä½“åˆ°è·¯å¾„
        for i, entity in enumerate(entities):
            exploration_path.append({
                "step": 0,
                "entity": entity,
                "action": "èµ·å§‹å®ä½“",
                "reasoning": f"ç¬¬{i+1}ä¸ªèµ·å§‹å®ä½“"
            })
        
        # å¤šè·³æ¢ç´¢å¾ªç¯
        for step in range(max_steps):
            if not current_entities:
                break
                
            print(f"ğŸ“ æ‰§è¡Œç¬¬{step+1}æ­¥æ¢ç´¢ï¼Œå½“å‰å®ä½“: {current_entities}")
            
            next_entities = []
            
            # å¯¹æ¯ä¸ªå½“å‰å®ä½“æ¢ç´¢é‚»å±…
            for entity in current_entities:
                # è·å–æ¨¡æ‹Ÿçš„ç›¸å…³å®ä½“
                related_entities = mock_relationships.get(entity, [])
                
                # è¿‡æ»¤å·²è®¿é—®çš„å®ä½“
                new_neighbors = [e for e in related_entities if e not in visited_entities]
                
                # é™åˆ¶æ¯ä¸ªå®ä½“çš„æ¢ç´¢å®½åº¦
                max_width = max(1, 3 - step)  # éšæ­¥æ•°é€’å‡
                selected_neighbors = new_neighbors[:max_width]
                
                # åˆ›å»ºå…³ç³»ä¿¡æ¯
                for neighbor in selected_neighbors:
                    discovered_relationships.append({
                        "source": entity,
                        "target": neighbor,
                        "type": "contains" if "æ¨¡å—" in entity else "related_to",
                        "step": step + 1,
                        "description": f"{entity} åŒ…å«æˆ–å…³è” {neighbor}"
                    })
                
                # æ·»åŠ åˆ°ä¸‹ä¸€æ­¥å®ä½“åˆ—è¡¨
                next_entities.extend(selected_neighbors)
                
                # è®°å½•æ¢ç´¢è·¯å¾„
                for neighbor in selected_neighbors:
                    exploration_path.append({
                        "step": step + 1,
                        "entity": neighbor,
                        "action": "æ¢ç´¢å‘ç°",
                        "reasoning": f"ä»å®ä½“'{entity}'æ¢ç´¢å‘ç°çš„ç›¸å…³å®ä½“"
                    })
            
            # æ›´æ–°çŠ¶æ€
            visited_entities.update(next_entities)
            current_entities = list(set(next_entities))  # å»é‡
            
            print(f"âœ… ç¬¬{step+1}æ­¥å®Œæˆï¼Œå‘ç°{len(next_entities)}ä¸ªæ–°å®ä½“")
        
        # ç”Ÿæˆç›¸å…³å†…å®¹
        for entity in list(visited_entities):
            relevant_content.append({
                "id": f"content_{len(relevant_content)+1}",
                "text": f"è¿™æ˜¯å…³äº'{entity}'çš„ç›¸å…³å†…å®¹æè¿°ã€‚åœ¨å¤šè·³æ¨ç†è¿‡ç¨‹ä¸­å‘ç°è¯¥å®ä½“ä¸æŸ¥è¯¢'{query}'å…·æœ‰ç›¸å…³æ€§ã€‚",
                "entity": entity,
                "relevance_score": 0.8
            })
        
        # æ”¶é›†æœ€ç»ˆç»Ÿè®¡
        total_time = time.time() - start_time
        steps_completed = min(max_steps, len([p for p in exploration_path if p["step"] > 0]) // max(1, len(entities)))
        
        print(f"ğŸ¯ ç®€åŒ–ç‰ˆå¤šè·³æ¨ç†å®Œæˆ:")
        print(f"  - æ€»å®ä½“æ•°: {len(visited_entities)}")
        print(f"  - å…³ç³»æ•°: {len(discovered_relationships)}")
        print(f"  - å†…å®¹æ•°: {len(relevant_content)}")
        print(f"  - è€—æ—¶: {total_time:.2f}ç§’")
        
        return {
            "entities": list(visited_entities),
            "relationships": discovered_relationships,
            "content": relevant_content,
            "exploration_path": exploration_path,
            "visited_entities": list(visited_entities),
            "statistics": {
                "entity_count": len(visited_entities),
                "relationship_count": len(discovered_relationships),
                "content_count": len(relevant_content),
                "path_length": len(exploration_path),
                "steps_completed": steps_completed
            },
            "performance_metrics": {
                "total_time": total_time,
                "entities_per_second": len(visited_entities) / max(total_time, 0.001)
            }
        }
        
    except Exception as e:
        print(f"âŒ ç®€åŒ–å¤šè·³æ¨ç†å¤±è´¥: {e}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return {
            "entities": entities,
            "relationships": [],
            "content": [],
            "exploration_path": [],
            "error": str(e)
        }

@app.post("/api/graphrag/multi-hop-analysis")
async def multi_hop_analysis(request: dict):
    """å¤šè·³æ¨ç†åˆ†æç«¯ç‚¹ - ä½¿ç”¨ç®€åŒ–ç‰ˆå®ç°"""
    try:
        query = request.get("query", "")
        entities = request.get("entities", [])
        max_steps = request.get("max_steps", 3)
        
        if not query:
            raise ValueError("æŸ¥è¯¢å‚æ•°ä¸èƒ½ä¸ºç©º")
            
        print(f"ğŸ” å¼€å§‹ç®€åŒ–ç‰ˆå¤šè·³æ¨ç†åˆ†æ: {query}")
        print(f"ğŸ“ èµ·å§‹å®ä½“: {entities}")
        
        # ä½¿ç”¨ç®€åŒ–ç‰ˆå¤šè·³æ¨ç†
        exploration_result = await simplified_multi_hop_reasoning(
            query=query,
            entities=entities,
            max_steps=max_steps
        )
        
        if "error" in exploration_result:
            return JSONResponse(
                status_code=500,
                content={
                    "status": "error",
                    "message": f"å¤šè·³æ¨ç†å¤±è´¥: {exploration_result['error']}",
                    "service_ready": False
                }
            )
        
        print(f"âœ… ç®€åŒ–ç‰ˆå¤šè·³æ¨ç†å®Œæˆ: å‘ç°{len(exploration_result.get('entities', []))}ä¸ªå®ä½“")
        
        return {
            "status": "success",
            "analysis": {
                "query": query,
                "multi_hop_results": exploration_result,
                "reasoning_type": "Simplified Multi-hop",
                "steps_taken": exploration_result.get("statistics", {}).get("steps_completed", 0),
                "entities_discovered": len(exploration_result.get("entities", [])),
                "relationships_found": len(exploration_result.get("relationships", [])),
                "content_retrieved": len(exploration_result.get("content", []))
            },
            "capabilities": {
                "multi_hop_reasoning": True,
                "adaptive_exploration": True,
                "semantic_scoring": False,
                "memory_mechanism": True
            }
        }
        
    except Exception as e:
        print(f"âŒ å¤šè·³æ¨ç†åˆ†æé”™è¯¯: {str(e)}")
        import traceback
        print(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"å¤šè·³æ¨ç†åˆ†æå¤±è´¥: {str(e)}",
                "service_ready": False
            }
        )

@app.post("/api/graphrag/knowledge-graph-stats")
async def knowledge_graph_stats():
    """çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯ç«¯ç‚¹"""
    try:
        print("ğŸ“Š è·å–çŸ¥è¯†å›¾è°±ç»Ÿè®¡ä¿¡æ¯")
        
        # æ¨¡æ‹Ÿå›¾è°±ç»Ÿè®¡ä¿¡æ¯
        memory_stats = {
            "nodes_count": 156,
            "edges_count": 342,
            "connected_components": 12
        }
        
        # æ¨¡æ‹ŸNeo4jç»Ÿè®¡ - åœ¨å®é™…ç¯å¢ƒä¸­ä¼šè¿æ¥çœŸå®æ•°æ®åº“
        neo4j_stats = {
            "neo4j_status": "æ¨¡æ‹Ÿæ¨¡å¼ - æœªè¿æ¥çœŸå®æ•°æ®åº“",
            "estimated_nodes": 500,
            "estimated_relationships": 1200
        }
        
        return {
            "status": "success",
            "graph_statistics": {
                "memory_graph": memory_stats,
                "neo4j_graph": neo4j_stats,
                "reasoning_capabilities": {
                    "simplified_multi_hop": True,
                    "adaptive_exploration": True,
                    "mock_data_exploration": True,
                    "performance_monitoring": True
                }
            },
            "system_info": {
                "mode": "simplified",
                "dependencies_loaded": "partial",
                "full_graphrag_available": False
            }
        }
        
    except Exception as e:
        print(f"âŒ è·å–å›¾è°±ç»Ÿè®¡å¤±è´¥: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"è·å–å›¾è°±ç»Ÿè®¡å¤±è´¥: {str(e)}"
            }
        )

@app.post("/api/chat")
async def chat():
    """å¯¹è¯ç«¯ç‚¹"""
    try:
        return {
            "status": "success", 
            "response": "GraphRAGæ™ºèƒ½å¯¹è¯åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œæ•¬è¯·æœŸå¾…ï¼",
            "service_ready": True
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={
                "status": "error",
                "message": f"å¯¹è¯å¤±è´¥: {str(e)}",
                "service_ready": False
            }
        )

def extract_youtube_content_with_cobalt(url, video_info):
    """
    ä¿®å¤çš„YouTubeå†…å®¹æå–å‡½æ•° - ç®€åŒ–ä¸”é«˜æ•ˆ
    åŸºäºå®é™…æµ‹è¯•çš„å·¥ä½œæ¨¡å¼ + è§†é¢‘è½¬å½•æå–
    """
    print("ğŸ¬ å¼€å§‹æå–YouTubeå†…å®¹: {}".format(url))
    print("ğŸ” DEBUG: extract_youtube_content_with_cobaltå‡½æ•°è¢«è°ƒç”¨")
    
    try:
        import requests
        import re
        from youtube_transcript_api import YouTubeTranscriptApi
        import urllib.parse
        
        # ç®€åŒ–çš„è¯·æ±‚å¤´
        headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
        # ä½¿ç”¨requestså‘é€è¯·æ±‚
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        html_content = response.text
        
        print("ğŸ“„ è·å–é¡µé¢å†…å®¹: {} å­—ç¬¦".format(len(html_content)))
        print("ğŸ” DEBUG: HTMLå†…å®¹å‰200å­—ç¬¦: {}".format(html_content[:200]))
        
        # 1. æ ‡é¢˜æå–
        title_patterns = [
            r'<title[^>]*>([^<]+)</title>',
            r'"videoDetails":[^}]*?"title":\s*"([^"]+)"',
            r'<meta\s+property="og:title"\s+content="([^"]+)"',
        ]
        
        title_found = False
        print("ğŸ” DEBUG: å¼€å§‹å°è¯• {} ä¸ªæ ‡é¢˜æå–æ¨¡å¼".format(len(title_patterns)))
        for i, pattern in enumerate(title_patterns):
            try:
                print("ğŸ” DEBUG: å°è¯•æ¨¡å¼ {}: {}".format(i+1, pattern))
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    title = match.group(1)
                    print("ğŸ” DEBUG: æ¨¡å¼ {} åŒ¹é…åˆ°: '{}'".format(i+1, title))
                    
                    # æ¸…ç†æ ‡é¢˜
                    if title.endswith(' - YouTube'):
                        title = title[:-10]
                    title = title.replace("\\u0026", "&").replace("\\u0027", "'").replace("\\u0022", '"')
                    title = title.replace("\\n", " ").replace("\\", "").strip()
                    
                    print("ğŸ” DEBUG: æ¸…ç†åæ ‡é¢˜: '{}'".format(title))
                    
                    # éªŒè¯æ ‡é¢˜è´¨é‡
                    if (len(title) > 5 and 
                        title not in ['å…³äº', 'æ–°é—»', 'ç‰ˆæƒ', 'è”ç³»æˆ‘ä»¬', 'YouTube'] and
                        not title.startswith('www.')):
                        
                        video_info["title"] = title
                        print("âœ… æ ‡é¢˜æå–æˆåŠŸ: {}".format(title))
                        title_found = True
                        break
                    else:
                        print("ğŸ” DEBUG: æ ‡é¢˜è´¨é‡æ£€æŸ¥å¤±è´¥: '{}'".format(title))
            except Exception as e:
                print("ğŸ” DEBUG: æ¨¡å¼ {} åŒ¹é…å¤±è´¥: {}".format(i+1, e))
                continue
        
        if not title_found:
            print("âŒ DEBUG: æ‰€æœ‰æ ‡é¢˜æå–æ¨¡å¼éƒ½å¤±è´¥äº†")
        
        # 2. æè¿°æå–
        desc_patterns = [
            r'"shortDescription":"([^"]{20,})"',
            r'<meta\s+property="og:description"\s+content="([^"]+)"',
            r'<meta\s+name="description"\s+content="([^"]+)"',
        ]
        
        desc_found = False
        for i, pattern in enumerate(desc_patterns):
            try:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    desc = match.group(1)
                    
                    # æ¸…ç†æè¿°
                    desc = desc.replace("\\n", " ").replace("\\t", " ").replace("\\\\", "")
                    desc = re.sub(r'\s+', ' ', desc).strip()
                    
                    # éªŒè¯æè¿°è´¨é‡
                    if len(desc) > 20:
                        video_info["description"] = desc[:500]
                        print("âœ… æè¿°æå–æˆåŠŸ: {} å­—ç¬¦".format(len(desc)))
                        desc_found = True
                        break
            except Exception as e:
                continue
        
        # 3. é¢‘é“æå–
        channel_patterns = [
            r'"videoDetails":[^}]*?"author":\s*"([^"]+)"',
            r'"ownerChannelName":\s*"([^"]+)"',
            r'"channelName":\s*"([^"]+)"',
        ]
        
        channel_found = False
        for i, pattern in enumerate(channel_patterns):
            try:
                match = re.search(pattern, html_content, re.IGNORECASE)
                if match:
                    channel = match.group(1).replace("\\", "").strip()
                    
                    if (len(channel) > 1 and 
                        channel not in ['YouTube', 'Google', 'å…³äº', 'æ–°é—»']):
                        
                        video_info["uploader"] = channel
                        print("âœ… é¢‘é“æå–æˆåŠŸ: {}".format(channel))
                        channel_found = True
                        break
            except Exception as e:
                continue
        
        # è®¾ç½®å¹³å°å’ŒçŠ¶æ€
        video_info["platform"] = "youtube"
        video_info["extraction_status"] = "success"
        
        # è¯„ä¼°æå–è´¨é‡
        quality_score = 0
        if title_found:
            quality_score += 50
        if desc_found:
            quality_score += 30
        if channel_found:
            quality_score += 20
        
        video_info["quality_score"] = quality_score
        print("ğŸ“Š æå–è´¨é‡å¾—åˆ†: {}/100".format(quality_score))
        
        # 4. æå–è§†é¢‘IDç”¨äºè½¬å½•
        video_id = None
        video_id_patterns = [
            r'[?&]v=([^&#]*)',
            r'/watch\?v=([^&#]*)',
            r'/embed/([^/?&#]*)',
            r'/v/([^/?&#]*)',
            r'youtu\.be/([^/?&#]*)'
        ]
        
        for pattern in video_id_patterns:
            match = re.search(pattern, url)
            if match:
                video_id = match.group(1)
                break
        
        # 5. æå–è§†é¢‘è½¬å½•ï¼ˆå®é™…è§†é¢‘å†…å®¹ï¼‰
        if video_id:
            print("ğŸ¤ å°è¯•æå–è§†é¢‘è½¬å½•å†…å®¹...")
            try:
                # åˆ›å»ºAPIå®ä¾‹
                ytt_api = YouTubeTranscriptApi()
                
                # è·å–å¯ç”¨è½¬å½•åˆ—è¡¨
                transcript_list = ytt_api.list(video_id)
                transcript_text = ""
                
                # ä¼˜å…ˆå°è¯•ä¸­æ–‡
                try:
                    transcript = transcript_list.find_transcript(['zh-cn', 'zh'])
                    transcript_data = transcript.fetch()
                    transcript_text = " ".join([entry.text for entry in transcript_data])
                    print("âœ… ä¸­æ–‡è½¬å½•æå–æˆåŠŸ: {} å­—ç¬¦".format(len(transcript_text)))
                except:
                    # å¦‚æœæ²¡æœ‰ä¸­æ–‡ï¼Œå°è¯•è‹±æ–‡
                    try:
                        transcript = transcript_list.find_transcript(['en'])
                        transcript_data = transcript.fetch()
                        transcript_text = " ".join([entry.text for entry in transcript_data])
                        print("âœ… è‹±æ–‡è½¬å½•æå–æˆåŠŸ: {} å­—ç¬¦".format(len(transcript_text)))
                    except:
                        # å¦‚æœæ²¡æœ‰è‹±æ–‡ï¼Œå°è¯•è·å–ä»»ä½•å¯ç”¨çš„è½¬å½•
                        try:
                            available_transcripts = list(transcript_list)
                            if available_transcripts:
                                transcript = available_transcripts[0]
                                transcript_data = transcript.fetch()
                                transcript_text = " ".join([entry.text for entry in transcript_data])
                                print("âœ… {}è½¬å½•æå–æˆåŠŸ: {} å­—ç¬¦".format(transcript.language_code, len(transcript_text)))
                        except:
                            print("âš ï¸ æ— æ³•è·å–ä»»ä½•è½¬å½•å†…å®¹")
                
                if transcript_text and len(transcript_text) > 50:
                    video_info["transcript"] = transcript_text
                    video_info["video_content"] = transcript_text  # è¿™æ˜¯å®é™…çš„è§†é¢‘å†…å®¹ï¼
                    quality_score += 30  # æœ‰è½¬å½•å†…å®¹å¤§å¤§æé«˜è´¨é‡
                    video_info["quality_score"] = quality_score
                    print("ğŸ‰ è§†é¢‘è½¬å½•å†…å®¹æå–æˆåŠŸï¼è¿™æ˜¯è§†é¢‘ä¸­å®é™…è¯´çš„å†…å®¹ï¼š")
                    print("ğŸ“ è½¬å½•é¢„è§ˆ: {}...".format(transcript_text[:200]))
                else:
                    print("âš ï¸ è½¬å½•å†…å®¹å¤ªçŸ­æˆ–ä¸ºç©º")
                    
            except Exception as transcript_error:
                print("âš ï¸ è½¬å½•æå–å¤±è´¥: {}".format(transcript_error))
                # è½¬å½•å¤±è´¥ä¸å½±å“æ•´ä½“æå–
        else:
            print("âš ï¸ æ— æ³•ä»URLä¸­æå–è§†é¢‘ID")
        
        print("ğŸ“Š æœ€ç»ˆæå–è´¨é‡å¾—åˆ†: {}/100".format(video_info.get("quality_score", quality_score)))
        
        return video_info
        
    except Exception as e:
        print("âŒ æå–å¤±è´¥: {}".format(e))
        print("ğŸ” DEBUG: å¼‚å¸¸è¯¦æƒ…: {}".format(str(e)))
        import traceback
        print("ğŸ” DEBUG: å®Œæ•´é”™è¯¯å †æ ˆ: {}".format(traceback.format_exc()))
        return {
            'platform': 'youtube',
            'extraction_status': 'failed',
            'error': str(e),
            'quality_score': 0
        }

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))  # æ”¹ä¸ºé»˜è®¤8000ç«¯å£ï¼Œä¸intelligent-content-workflowç³»ç»ŸåŒ¹é…
    print(f"ğŸš€ å¯åŠ¨GraphRAG Agent (ä¿®å¤ç‰ˆ - é›†æˆåˆ°intelligent-content-workflow)...")
    print(f"ğŸ“¡ ç«¯å£: {port}")
    print(f"ğŸ”§ ä¿®å¤åŠŸèƒ½: æ®µé”™è¯¯ä¿®å¤ã€æ•°æ®åŒæ­¥ã€D3.jsèŠ‚ç‚¹å¼•ç”¨é—®é¢˜")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
